{"items": [{"tags": ["python", "workflow", "airflow"], "owner": {"account_id": 1640670, "reputation": 3145, "user_id": 1513174, "user_type": "registered", "accept_rate": 57, "profile_image": "https://www.gravatar.com/avatar/69bdc15f304c4ec333ac9250397ae146?s=256&d=identicon&r=PG", "display_name": "costrouc", "link": "https://stackoverflow.com/users/1513174/costrouc"}, "is_answered": true, "view_count": 120683, "answer_count": 17, "score": 198, "last_activity_date": 1679501652, "creation_date": 1483763572, "last_edit_date": 1499790133, "question_id": 41517798, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/41517798/proper-way-to-create-dynamic-workflows-in-airflow", "title": "Proper way to create dynamic workflows in Airflow", "body": "<h2>Problem</h2>\n\n<p>Is there any way in Airflow to create a workflow such that the number of tasks B.* is unknown until completion of Task A? I have looked at subdags but it looks like it can only work with a static set of tasks that have to be determined at Dag creation.</p>\n\n<p>Would dag triggers work? And if so could you please provide an example.</p>\n\n<p>I have an issue where it is impossible to know the number of task B's that will be needed to calculate Task C until Task A has been completed. Each Task B.* will take several hours to compute and cannot be combined.</p>\n\n<pre><code>              |---&gt; Task B.1 --|\n              |---&gt; Task B.2 --|\n Task A ------|---&gt; Task B.3 --|-----&gt; Task C\n              |       ....     |\n              |---&gt; Task B.N --|\n</code></pre>\n\n<h2>Idea #1</h2>\n\n<p>I don't like this solution because I have to create a blocking ExternalTaskSensor and all the Task B.* will take between 2-24 hours to complete. So I do not consider this a viable solution. Surely there is an easier way? Or was Airflow not designed for this?</p>\n\n<pre><code>Dag 1\nTask A -&gt; TriggerDagRunOperator(Dag 2) -&gt; ExternalTaskSensor(Dag 2, Task Dummy B) -&gt; Task C\n\nDag 2 (Dynamically created DAG though python_callable in TriggerDagrunOperator)\n               |-- Task B.1 --|\n               |-- Task B.2 --|\nTask Dummy A --|-- Task B.3 --|-----&gt; Task Dummy B\n               |     ....     |\n               |-- Task B.N --|\n</code></pre>\n\n<h2>Edit 1:</h2>\n\n<p><strong>As of now this question still does not have a great answer</strong>. I have been contacted by several people looking for a solution. </p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 10317169, "reputation": 1333, "user_id": 7611145, "user_type": "registered", "accept_rate": 0, "profile_image": "https://www.gravatar.com/avatar/0451a0dc3a315c9d57661a248e6f7cb5?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "bronzels", "link": "https://stackoverflow.com/users/7611145/bronzels"}, "is_answered": true, "view_count": 80791, "answer_count": 9, "score": 104, "last_activity_date": 1678522376, "creation_date": 1492170156, "last_edit_date": 1553006186, "question_id": 43410836, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/43410836/how-to-remove-default-example-dags-in-airflow", "title": "How to remove default example dags in airflow", "body": "<p>I am a new user of Airbnb's open source workflow/datapipeline software <a href=\"https://github.com/apache/incubator-airflow\" rel=\"noreferrer\">airflow</a>. There are dozens of default example dags after the web UI is started.  I tried many ways to remove these dags, but I've failed to do so. </p>\n\n<ul>\n<li><code>load_examples = False</code> is set in airflow.cfg.</li>\n<li>Folder lib/python2.7/site-packages/airflow/example_dags is removed.</li>\n</ul>\n\n<p>States of those example dags are changed to gray after I removed the dags folder, but the items still occupy the web UI screen. And a new dag folder is specified in airflow.cfg as <code>dags_folder = /mnt/dag/1</code>. I checked this dag folder, nothing is there. It's really weird to me why it is so difficult to remove these examples.</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 421162, "reputation": 1665, "user_id": 3285394, "user_type": "registered", "accept_rate": 38, "profile_image": "https://i.sstatic.net/odFFY.jpg?s=256", "display_name": "subba", "link": "https://stackoverflow.com/users/3285394/subba"}, "is_answered": true, "view_count": 102003, "accepted_answer_id": 49683543, "answer_count": 21, "score": 99, "last_activity_date": 1725527069, "creation_date": 1479376765, "question_id": 40651783, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/40651783/airflow-how-to-delete-a-dag", "title": "Airflow: how to delete a DAG?", "body": "<p>I have started the Airflow webserver and scheduled some dags. I can see the dags on web GUI.</p>\n\n<p>How can I delete a particular DAG from being run and shown in web GUI? Is there an Airflow CLI command to do that?</p>\n\n<p>I looked around but could not find an answer for a simple way of deleting a DAG once it has been loaded and scheduled.</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 1113994, "reputation": 2923, "user_id": 1103817, "user_type": "registered", "accept_rate": 59, "profile_image": "https://www.gravatar.com/avatar/4f7ef2f36726859f4c28c945526d37bf?s=256&d=identicon&r=PG", "display_name": "Roger", "link": "https://stackoverflow.com/users/1103817/roger"}, "is_answered": true, "view_count": 180368, "accepted_answer_id": 37739468, "answer_count": 7, "score": 89, "last_activity_date": 1674224436, "creation_date": 1461105399, "last_edit_date": 1674224436, "question_id": 36730714, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/36730714/execution-date-in-airflow-need-to-access-as-a-variable", "title": "execution_date in airflow: need to access as a variable", "body": "<p>I am really a newbie in this forum. But I have been playing with airflow, for sometime, for our company.  Sorry if this question sounds really dumb.</p>\n<p>I am writing a pipeline using bunch of BashOperators.\nBasically, for each Task, I want to simply call a REST api using 'curl'</p>\n<p>This is what my pipeline looks like(very simplified version):</p>\n<pre class=\"lang-py prettyprint-override\"><code>from airflow import DAG\nfrom airflow.operators import BashOperator, PythonOperator\nfrom dateutil import tz\nimport datetime\n\ndatetime_obj = datetime.datetime\n                                  \ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime.datetime.combine(datetime_obj.today() - datetime.timedelta(1), datetime_obj.min.time()),\n    'email': ['xxxx@xxx.xxx'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 2,\n    'retry_delay': datetime.timedelta(minutes=5),\n}\n\n\ncurrent_datetime = datetime_obj.now(tz=tz.tzlocal())\n\ndag = DAG(\n    'test_run', default_args=default_args, schedule_interval=datetime.timedelta(minutes=60))\n\ncurl_cmd='curl -XPOST &quot;'+hostname+':8000/run?st='+current_datetime +'&quot;'\n\n\nt1 = BashOperator(\n    task_id='rest-api-1',\n    bash_command=curl_cmd,\n    dag=dag)\n</code></pre>\n<p>If you notice I am doing <code>current_datetime= datetime_obj.now(tz=tz.tzlocal())</code>\nInstead what I want here is <em><strong>'execution_date'</strong></em></p>\n<p>How do I use <strong>'execution_date'</strong> directly and assign it to a variable in my python file?</p>\n<p>I have having this general issue of accessing args.\nAny help will be genuinely appreciated.</p>\n<p>Thanks</p>\n"}, {"tags": ["pandas", "airflow", "apache-beam"], "owner": {"account_id": 11848085, "reputation": 1000, "user_id": 8670118, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3bf8f4c1712cbb04de4aab54c3b38fca?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "LouisB", "link": "https://stackoverflow.com/users/8670118/louisb"}, "is_answered": true, "view_count": 47347, "closed_date": 1710427630, "accepted_answer_id": 51513902, "answer_count": 4, "score": 87, "last_activity_date": 1618896865, "creation_date": 1525857580, "question_id": 50249759, "link": "https://stackoverflow.com/questions/50249759/apache-airflow-or-apache-beam-for-data-processing-and-job-scheduling", "closed_reason": "Not suitable for this site", "title": "Apache Airflow or Apache Beam for data processing and job scheduling", "body": "<p>I'm trying to give useful information but I am far from being a data engineer.</p>\n\n<p>I am currently using the python library pandas to execute a long series of transformation to my data which has a lot of inputs (currently CSV and excel files). The outputs are several excel files. I would like to be able to execute scheduled monitored batch jobs with parallel computation (I mean not as sequential as what I'm doing with pandas), once a month.</p>\n\n<p>I don't really know Beam or Airflow, I quickly read through the docs and it seems that both can achieve that. Which one should I use ? </p>\n"}, {"tags": ["python", "conditional-statements", "airflow"], "owner": {"account_id": 8367824, "reputation": 6183, "user_id": 6283849, "user_type": "registered", "accept_rate": 92, "profile_image": "https://i.sstatic.net/e5TQ0.png?s=256", "display_name": "Alexis.Rolland", "link": "https://stackoverflow.com/users/6283849/alexis-rolland"}, "is_answered": true, "view_count": 124084, "accepted_answer_id": 43680277, "answer_count": 3, "score": 82, "last_activity_date": 1670342560, "creation_date": 1493376572, "last_edit_date": 1515778601, "question_id": 43678408, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/43678408/how-to-create-a-conditional-task-in-airflow", "title": "How to create a conditional task in Airflow", "body": "<p>I would like to create a conditional task in Airflow as described in the schema below. The expected scenario is the following:</p>\n\n<ul>\n<li>Task 1 executes</li>\n<li>If Task 1 succeed, then execute Task 2a</li>\n<li>Else If Task 1 fails, then execute Task 2b</li>\n<li>Finally execute Task 3</li>\n</ul>\n\n<p><a href=\"https://i.sstatic.net/VS1Wm.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/VS1Wm.png\" alt=\"Conditional Task\"></a>\nAll tasks above are SSHExecuteOperator.\nI'm guessing I should be using the ShortCircuitOperator and / or XCom to manage the condition but I am not clear on how to implement that. Could you please describe the solution?</p>\n"}, {"tags": ["python", "scheduled-tasks", "airflow"], "owner": {"account_id": 4656831, "reputation": 16354, "user_id": 3772221, "user_type": "registered", "accept_rate": 97, "profile_image": "https://i.sstatic.net/xfx4l.png?s=256", "display_name": "m0meni", "link": "https://stackoverflow.com/users/3772221/m0meni"}, "is_answered": true, "view_count": 63469, "accepted_answer_id": 43122799, "answer_count": 3, "score": 79, "last_activity_date": 1587622473, "creation_date": 1470251576, "last_edit_date": 1537528895, "question_id": 38751872, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/38751872/how-to-prevent-airflow-from-backfilling-dag-runs", "title": "How to prevent airflow from backfilling dag runs?", "body": "<p>Say you have an airflow DAG that doesn't make sense to backfill, meaning that, after it's run once, running it subsequent times quickly would be completely pointless.</p>\n\n<p>For example, if you're loading data from some source that is only updated hourly into your database, backfilling, which occurs in rapid succession, would just be importing the same data again and again.</p>\n\n<p>This is especially annoying when you instantiate a new hourly task, and it runs <code>N</code> amount of times for each hour it missed, doing redundant work, before it starts running on the interval you specified.</p>\n\n<p>The only solution I can think of is something that they specifically advised against in <a href=\"https://airflow.apache.org/faq.html\" rel=\"noreferrer\">FAQ of the docs</a></p>\n\n<blockquote>\n  <p>We recommend against using dynamic values as start_date, especially <code>datetime.now()</code> as it can be quite confusing.</p>\n</blockquote>\n\n<p>Is there any way to disable backfilling for a DAG, or should I do the above?</p>\n"}, {"tags": ["python", "hadoop", "airflow"], "owner": {"account_id": 9009591, "reputation": 1987, "user_id": 6714806, "user_type": "registered", "accept_rate": 44, "profile_image": "https://www.gravatar.com/avatar/884eff772223810433146e34fec2d94b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Chetan J", "link": "https://stackoverflow.com/users/6714806/chetan-j"}, "is_answered": true, "view_count": 132404, "answer_count": 6, "score": 77, "last_activity_date": 1658147494, "creation_date": 1493202808, "last_edit_date": 1635599026, "question_id": 43631693, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/43631693/how-to-stop-kill-airflow-tasks-from-the-ui", "title": "How to stop/kill Airflow tasks from the UI", "body": "<p>How can I stop/kill a running task on Airflow UI? I am using <code>LocalExecutor</code>.\nEven if I use <code>CeleryExecutor</code>, how do can I kill/stop the running task?</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 19532729, "reputation": 874, "user_id": 14292128, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GiarSMT5UC3GP2F-zWczql_w01WmG7BOU1Rq0bnZw=k-s256", "display_name": "Manikanta Komuravelli", "link": "https://stackoverflow.com/users/14292128/manikanta-komuravelli"}, "is_answered": true, "view_count": 116215, "accepted_answer_id": 66161165, "answer_count": 7, "score": 73, "last_activity_date": 1683236266, "creation_date": 1613068599, "last_edit_date": 1646556180, "question_id": 66160780, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66160780/first-time-login-to-apache-airflow-asks-for-username-and-password-what-is-the-u", "title": "First time login to Apache Airflow asks for username and password, what is the username and password?", "body": "<p>I've just installed Apache Airflow, and I'm launching the webserver for the first time, and it asks me for username and password, I haven't set any username or password.</p>\n<p>Can you let me know what is the default username and password for airflow?</p>\n<p><a href=\"https://i.sstatic.net/bxcdr.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/bxcdr.png\" alt=\"enter image description here\" /></a></p>\n"}, {"tags": ["python", "google-cloud-platform", "airflow", "google-cloud-composer"], "owner": {"account_id": 1468834, "reputation": 90751, "user_id": 1380918, "user_type": "registered", "accept_rate": 98, "profile_image": "https://www.gravatar.com/avatar/6e7c96a05fdd3ab2a61d07e78cc0f59c?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "hexacyanide", "link": "https://stackoverflow.com/users/1380918/hexacyanide"}, "is_answered": true, "view_count": 96198, "accepted_answer_id": 56370721, "answer_count": 3, "score": 71, "last_activity_date": 1696450515, "creation_date": 1559181628, "last_edit_date": 1597294452, "question_id": 56370720, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/56370720/how-to-control-the-parallelism-or-concurrency-of-an-airflow-installation", "title": "How to control the parallelism or concurrency of an Airflow installation?", "body": "<p>In some of my Apache Airflow installations, DAGs or tasks that are scheduled to run do not run even when the scheduler doesn't appear to be fully loaded. How can I increase the number of DAGs or tasks that can run concurrently?</p>\n<p>Similarly, if my installation is under high load and I want to limit how quickly my Airflow workers pull queued tasks (such as to reduce resource consumption), what can I adjust to reduce the average load?</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 1664638, "reputation": 803, "user_id": 1532319, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/19e1b112cce5cb741747fcbef4beec77?s=256&d=identicon&r=PG", "display_name": "thatkaiguy", "link": "https://stackoverflow.com/users/1532319/thatkaiguy"}, "is_answered": true, "view_count": 81653, "accepted_answer_id": 41620340, "answer_count": 3, "score": 70, "last_activity_date": 1625676345, "creation_date": 1476839309, "question_id": 40120474, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/40120474/writing-to-airflow-logs", "title": "Writing to Airflow Logs", "body": "<p>One way to write to the logs in Airflow is to return a string from a PythonOperator like on line 44 <a href=\"https://github.com/apache/incubator-airflow/blob/ff45d8f2218a8da9328161aa66d004c3db3b367e/airflow/example_dags/example_python_operator.py\" rel=\"noreferrer\">here</a>.</p>\n\n<p>Are there other ways that allow me to write to the airflow log files? I've found that print statements are not saved to the logs.</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 55683, "reputation": 10296, "user_id": 166836, "user_type": "registered", "accept_rate": 78, "profile_image": "https://www.gravatar.com/avatar/7dc9b173972bd369209e43dd2c997bab?s=256&d=identicon&r=PG", "display_name": "sdot257", "link": "https://stackoverflow.com/users/166836/sdot257"}, "is_answered": true, "view_count": 202244, "accepted_answer_id": 46061162, "answer_count": 6, "score": 68, "last_activity_date": 1673970264, "creation_date": 1504627165, "last_edit_date": 1606152755, "question_id": 46059161, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/46059161/airflow-how-to-pass-xcom-variable-into-python-function", "title": "Airflow - How to pass xcom variable into Python function", "body": "<p>I need to reference a variable that's returned by a <code>BashOperator</code>. In my <code>task_archive_s3_file</code>, I need to get the filename from <code>get_s3_file</code>. The task simply prints <code>{{ ti.xcom_pull(task_ids=submit_file_to_spark) }}</code> as a string instead of the value.</p>\n<p>If I use the <code>bash_command</code>, the value prints correctly.</p>\n<pre class=\"lang-py prettyprint-override\"><code>get_s3_file = PythonOperator(\n    task_id='get_s3_file',\n    python_callable=obj.func_get_s3_file,\n    trigger_rule=TriggerRule.ALL_SUCCESS,\n    dag=dag)\n\nsubmit_file_to_spark = BashOperator(\n    task_id='submit_file_to_spark',\n    bash_command=&quot;echo 'hello world'&quot;,\n    trigger_rule=&quot;all_done&quot;,\n    xcom_push=True,\n    dag=dag)\n\ntask_archive_s3_file = PythonOperator(\n    task_id='archive_s3_file',\n#    bash_command=&quot;echo {{ ti.xcom_pull(task_ids='submit_file_to_spark') }}&quot;,\n    python_callable=obj.func_archive_s3_file,\n    params={'s3_path_filename': &quot;{{ ti.xcom_pull(task_ids=submit_file_to_spark) }}&quot; },\n    dag=dag)\n\nget_s3_file &gt;&gt; submit_file_to_spark &gt;&gt; task_archive_s3_file\n</code></pre>\n"}, {"tags": ["airflow", "airflow-scheduler"], "owner": {"account_id": 485591, "reputation": 1291, "user_id": 901983, "user_type": "registered", "accept_rate": 87, "profile_image": "https://www.gravatar.com/avatar/dccc45f0273f9d7e3a382d21432e6167?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "l0n3r4n83r", "link": "https://stackoverflow.com/users/901983/l0n3r4n83r"}, "is_answered": true, "view_count": 103904, "answer_count": 15, "score": 65, "last_activity_date": 1662723021, "creation_date": 1519784931, "last_edit_date": 1519949461, "question_id": 49021055, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/49021055/airflow-1-9-0-is-queuing-but-not-launching-tasks", "title": "Airflow 1.9.0 is queuing but not launching tasks", "body": "<p>Airflow is randomly not running queued tasks some tasks dont even get queued status. I keep seeing below in the scheduler logs</p>\n\n<pre><code> [2018-02-28 02:24:58,780] {jobs.py:1077} INFO - No tasks to consider for execution.\n</code></pre>\n\n<p>I do see tasks in database that either have no status or queued status but they never get started.</p>\n\n<p>The airflow setup is running <a href=\"https://github.com/puckel/docker-airflow\" rel=\"noreferrer\">https://github.com/puckel/docker-airflow</a> on ECS with Redis. There are 4 scheduler threads and 4 Celery worker tasks. For the tasks that are not running are showing in queued state (grey icon) when hovering over the task icon operator is null and task details says: </p>\n\n<pre><code>    All dependencies are met but the task instance is not running. In most cases this just means that the task will probably be scheduled soon unless:- The scheduler is down or under heavy load\n</code></pre>\n\n<p>Metrics on scheduler do not show heavy load. The dag is very simple with 2 independent tasks only dependent on last run. There are also tasks in the same dag that are stuck with no status (white icon). </p>\n\n<p><em>Interesting thing to notice is when I restart the scheduler tasks change to running state.</em></p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 47543, "reputation": 37176, "user_id": 140877, "user_type": "registered", "accept_rate": 79, "profile_image": "https://www.gravatar.com/avatar/317698d41135bf29fb65324a437ecfa7?s=256&d=identicon&r=PG", "display_name": "ryudice", "link": "https://stackoverflow.com/users/140877/ryudice"}, "is_answered": true, "view_count": 92268, "answer_count": 5, "score": 65, "last_activity_date": 1688472494, "creation_date": 1493111457, "last_edit_date": 1553290827, "question_id": 43606311, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/43606311/refreshing-dags-without-web-server-restart-apache-airflow", "title": "Refreshing dags without web server restart apache airflow", "body": "<p>Is there any way to reload the jobs without having to restart the server?</p>\n"}, {"tags": ["python-3.x", "airflow"], "owner": {"account_id": 9421132, "reputation": 743, "user_id": 7007498, "user_type": "registered", "profile_image": "https://graph.facebook.com/10154100201432462/picture?type=large", "display_name": "Nikhil Reddy", "link": "https://stackoverflow.com/users/7007498/nikhil-reddy"}, "is_answered": true, "view_count": 71260, "accepted_answer_id": 40774361, "answer_count": 8, "score": 64, "last_activity_date": 1606018090, "creation_date": 1476272485, "last_edit_date": 1476780049, "question_id": 39997714, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/39997714/airflow-s3-connection-using-ui", "title": "Airflow s3 connection using UI", "body": "<p>I've been trying to use Airflow to schedule a DAG.\nOne of the DAG includes a task which loads data from s3 bucket.</p>\n\n<p>For the purpose above I need to setup s3 connection. But UI provided by airflow isn't that intutive (<a href=\"http://pythonhosted.org/airflow/configuration.html?highlight=connection#connections\" rel=\"noreferrer\">http://pythonhosted.org/airflow/configuration.html?highlight=connection#connections</a>). Any one succeeded setting up the s3 connection if so are there any best practices you folks follow?</p>\n\n<p>Thanks.</p>\n"}, {"tags": ["airflow", "aws-step-functions"], "owner": {"account_id": 7923158, "reputation": 1859, "user_id": 6058499, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/811354acb0f0e20fe7ab600624973ead?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "sanjayr", "link": "https://stackoverflow.com/users/6058499/sanjayr"}, "is_answered": true, "view_count": 54547, "accepted_answer_id": 64239918, "answer_count": 3, "score": 64, "last_activity_date": 1675221884, "creation_date": 1600804708, "question_id": 64016869, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64016869/airflow-versus-aws-step-functions-for-workflow", "title": "Airflow versus AWS Step Functions for workflow", "body": "<p>I am working on a project that grabs a set of input data from AWS S3, pre-processes and divvies it up, spins up 10K batch containers to process the divvied data in parallel on AWS Batch, post-aggregates the data, and pushes it to S3.</p>\n<p>I already have software patterns from other projects for Airflow + Batch, but have not dealt with the scaling factors of 10k parallel tasks. Airflow is nice since I can look at which tasks failed and retry a task after debugging. But dealing with that many tasks on one Airflow EC2 instance seems like a barrier. Other option would be to have one task that kicks off the 10k containers and monitors it from there.</p>\n<p>I have no experience with Step Functions, but have heard it's AWS's Airflow. There looks to be plenty of patterns online for Step Functions + Batch. Does Step Functions seem like a good path to check out for my use case? Do you get the same insights on failing jobs / ability to retry tasks as you do with Airflow?</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 369948, "reputation": 5124, "user_id": 717600, "user_type": "registered", "accept_rate": 32, "profile_image": "https://i.sstatic.net/yYVqR.jpg?s=256", "display_name": "Md Sirajus Salayhin", "link": "https://stackoverflow.com/users/717600/md-sirajus-salayhin"}, "is_answered": true, "view_count": 29057, "accepted_answer_id": 52210596, "answer_count": 8, "score": 63, "last_activity_date": 1608223714, "creation_date": 1536234556, "last_edit_date": 1608223714, "question_id": 52203441, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/52203441/error-while-install-airflow-by-default-one-of-airflows-dependencies-installs-a", "title": "Error while install airflow: By default one of Airflow&#39;s dependencies installs a GPL", "body": "<p>Getting the following error after running <code>pip install airflow[postgres]</code> command:</p>\n<pre><code>&gt; raise RuntimeError(&quot;By default one of Airflow's dependencies installs\n&gt; a GPL &quot;\n&gt; \n&gt; RuntimeError: By default one of Airflow's dependencies  installs a GPL\n&gt; dependency (unidecode). To avoid this dependency set\n&gt; SLUGIFY_USES_TEXT_UNIDECODE=yes in your environment when you install\n&gt; or upgrade Airflow. To force installing the GPL version set\n&gt; AIRFLOW_GPL_UNIDECODE\n</code></pre>\n<p>I am trying to install in Debian 9</p>\n"}, {"tags": ["python", "hadoop", "airflow"], "owner": {"account_id": 9009591, "reputation": 1987, "user_id": 6714806, "user_type": "registered", "accept_rate": 44, "profile_image": "https://www.gravatar.com/avatar/884eff772223810433146e34fec2d94b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Chetan J", "link": "https://stackoverflow.com/users/6714806/chetan-j"}, "is_answered": true, "view_count": 60362, "accepted_answer_id": 43330451, "answer_count": 3, "score": 62, "last_activity_date": 1691075467, "creation_date": 1491545287, "last_edit_date": 1588353066, "question_id": 43270820, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/43270820/how-to-restart-a-failed-task-on-airflow", "title": "How to restart a failed task on Airflow", "body": "<p>I am using a <strong>LocalExecutor</strong> and my dag has <strong>3 tasks</strong> where task(C) is dependant on task(A). Task(B) and task(A) can run in parallel something like below</p>\n\n<p>A-->C</p>\n\n<p>B</p>\n\n<p><strong>So task(A) has failed</strong> and but <strong>task(B) ran fine</strong>. Task(C) is yet to run as task(A) has failed.</p>\n\n<p>My question is <strong>how do i re run Task(A) alone so Task(C) runs</strong> once Task(A) completes and Airflow UI marks them as success.</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 1310222, "reputation": 794, "user_id": 1258360, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/771a61380d8a87358f83562d2a49f23b?s=256&d=identicon&r=PG", "display_name": "Mask", "link": "https://stackoverflow.com/users/1258360/mask"}, "is_answered": true, "view_count": 81809, "accepted_answer_id": 43115743, "answer_count": 4, "score": 60, "last_activity_date": 1671471057, "creation_date": 1490859788, "question_id": 43111506, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/43111506/how-do-i-force-a-task-on-airflow-to-fail", "title": "How do I force a task on airflow to fail?", "body": "<p>I have a python callable <code>process_csv_entries</code> that processes csv file entries. I want my task to complete successfully only if all entries were processed successfully. Task should fail otherwise</p>\n\n<pre><code>def process_csv_entries(csv_file):\n    # Boolean \n    file_completely_parsed = &lt;call_to_module_to_parse_csv&gt;\n    return not file_completely_parsed\n\nCSV_FILE=&lt;Sets path to csv file&gt;\nt1 = PythonOperator(dag=dag,\n                      task_id='parse_csv_completely',\n                      python_operator=process_csv_entries,\n                      op_args=[CSV_FILE])\n</code></pre>\n\n<p>t1 seems to complete successfully irrespective of returned value.\nHow do I force PythonOperator task to fail?</p>\n"}, {"tags": ["python-2.7", "airflow"], "owner": {"account_id": 8861824, "reputation": 1401, "user_id": 6617888, "user_type": "registered", "accept_rate": 10, "profile_image": "https://www.gravatar.com/avatar/96fafbbfa706ea5dc48e473674e89687?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "MJK", "link": "https://stackoverflow.com/users/6617888/mjk"}, "is_answered": true, "view_count": 179738, "accepted_answer_id": 44022992, "answer_count": 13, "score": 60, "last_activity_date": 1676032111, "creation_date": 1471850377, "last_edit_date": 1582794656, "question_id": 39073443, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/39073443/how-do-i-restart-airflow-webserver", "title": "How do I restart airflow webserver?", "body": "<p>I am using airflow for my data pipeline project. I have configured my project in airflow and start the airflow server as a backend process using following command</p>\n\n<pre><code>airflow webserver -p 8080 -D True\n</code></pre>\n\n<p>Server running successfully in backend. Now I want to enable authentication in airflow and done configuration changes in airflow.cfg, but authentication functionality is not reflected in server. when I stop and start airflow server in my local machine it works.<br><br>\nSo How can I restart my daemon airflow webserver process in my server??</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 3294534, "reputation": 1399, "user_id": 2772093, "user_type": "registered", "accept_rate": 53, "profile_image": "https://www.gravatar.com/avatar/5087603249c814d3cce60e5757fd40f4?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "sidd607", "link": "https://stackoverflow.com/users/2772093/sidd607"}, "is_answered": true, "view_count": 67991, "accepted_answer_id": 43179385, "answer_count": 3, "score": 60, "last_activity_date": 1571081253, "creation_date": 1467713336, "question_id": 38200666, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/38200666/airflow-parallelism", "title": "Airflow parallelism", "body": "<p>the Local Executor spawns new processes while scheduling tasks. Is there a limit to the number of processes it creates. I needed to change it. I need to know what is the difference between scheduler's \"max_threads\" and \n\"parallelism\" in airflow.cfg ?</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 1778069, "reputation": 583, "user_id": 1621434, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bc7c8d2a669b092860cc20aacae68c3e?s=256&d=identicon&r=PG", "display_name": "Frank Liu", "link": "https://stackoverflow.com/users/1621434/frank-liu"}, "is_answered": true, "view_count": 140374, "accepted_answer_id": 53686174, "answer_count": 3, "score": 58, "last_activity_date": 1709066853, "creation_date": 1544158850, "last_edit_date": 1544297001, "question_id": 53663534, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/53663534/for-apache-airflow-how-can-i-pass-the-parameters-when-manually-trigger-dag-via", "title": "For Apache Airflow, How can I pass the parameters when manually trigger DAG via CLI?", "body": "<p>I use Airflow to manage ETL tasks execution and schedule. A DAG has been created and it works fine. But is it possible to pass parameters when manually trigger the dag via cli.</p>\n\n<p>For example:\nMy DAG runs every day at 01:30, and processes data for yesterday(time range from 01:30 yesterday to 01:30 today). There might be some issues with the data source. I need to re-process those data (manually specify the time range).</p>\n\n<p>So can I create such an airflow DAG, when it's scheduled, that the default time range is from 01:30 yesterday to 01:30 today. Then if anything wrong with the data source, I need to manually trigger the DAG and manually pass the time range as parameters.</p>\n\n<p>As I know <code>airflow test</code> has <code>-tp</code> that can pass params to the task. But this is only for testing a specific task. and <code>airflow trigger_dag</code> doesn't have <code>-tp</code> option. So is there any way to tigger_dag and pass parameters to the DAG, and then the Operator can read these parameters?</p>\n\n<p>Thanks!</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 1161261, "reputation": 571, "user_id": 7233050, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/760887750cc01e9b3edf9b2763fc0baa?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "jompa", "link": "https://stackoverflow.com/users/7233050/jompa"}, "is_answered": true, "view_count": 57137, "answer_count": 9, "score": 57, "last_activity_date": 1630573688, "creation_date": 1492795752, "question_id": 43548744, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/43548744/removing-airflow-task-logs", "title": "Removing Airflow task logs", "body": "<p>I'm running 5 DAG's which have generated a total of about 6GB of log data in the <code>base_log_folder</code> over a months period. I just added a <code>remote_base_log_folder</code> but it seems it does not exclude logging to the <code>base_log_folder</code>.</p>\n\n<p>Is there anyway to automatically remove old log files, rotate them or force airflow to not log on disk (base_log_folder) only in remote storage?</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 8197299, "reputation": 4078, "user_id": 6169688, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-xbiq_0X1UQc/AAAAAAAAAAI/AAAAAAAAABQ/XyC6SkmIKn4/photo.jpg?sz=256", "display_name": "DennisLi", "link": "https://stackoverflow.com/users/6169688/dennisli"}, "is_answered": true, "view_count": 127515, "answer_count": 15, "score": 56, "last_activity_date": 1668144579, "creation_date": 1566886369, "last_edit_date": 1623587804, "question_id": 57668584, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57668584/airflow-scheduler-does-not-appear-to-be-running-after-execute-a-task", "title": "Airflow scheduler does not appear to be running after execute a task", "body": "<p>When there is a task running, Airflow will pop a notice saying the scheduler does not appear to be running and it kept showing until the task finished:</p>\n<pre><code>The scheduler does not appear to be running. Last heartbeat was received 5 minutes ago.\n\nThe DAGs list may not update, and new tasks will not be scheduled.\n</code></pre>\n<p>Actually, the scheduler process is running, as I have checked the process. After the task finished, the notice will disappear and everything back to normal.</p>\n<p>My task is kind of heavy, may running for couple hours.</p>\n"}, {"tags": ["airflow", "airflow-scheduler"], "owner": {"account_id": 3040372, "reputation": 663, "user_id": 2577568, "user_type": "registered", "accept_rate": 67, "profile_image": "https://graph.facebook.com/100000147230886/picture?type=large", "display_name": "Sreenath Kamath", "link": "https://stackoverflow.com/users/2577568/sreenath-kamath"}, "is_answered": true, "view_count": 30273, "accepted_answer_id": 48382000, "answer_count": 3, "score": 55, "last_activity_date": 1650863416, "creation_date": 1516614800, "last_edit_date": 1635598968, "question_id": 48378712, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/48378712/efficient-way-to-deploy-dag-files-on-airflow", "title": "Efficient way to deploy dag files on airflow", "body": "<p>Are there any best practices that are followed for deploying new dags to airflow? </p>\n\n<p>I saw a couple of comments on the google forum stating that the dags are saved inside a GIT repository and the same is synced periodically to the local location in the airflow cluster.<br>    Regarding this approach, I had a  couple of questions\n<li>\n  Do we maintain separate dag files for separate environments? (testing. production)\n  <li>How to handle rollback of an ETL to an older version in case the new version has a bug?</p>\n\n<p>Any help here is highly appreciated. Let me know in case you need any further details?</p>\n"}, {"tags": ["python", "windows", "flask", "flask-admin", "airflow"], "owner": {"account_id": 3105950, "reputation": 1038, "user_id": 2628744, "user_type": "registered", "accept_rate": 44, "profile_image": "https://www.gravatar.com/avatar/26130e4b3a2773a24da8efa28ad501e9?s=256&d=identicon&r=PG", "display_name": "Rafael", "link": "https://stackoverflow.com/users/2628744/rafael"}, "is_answered": true, "view_count": 111450, "accepted_answer_id": 46996196, "answer_count": 6, "score": 52, "last_activity_date": 1592653286, "creation_date": 1441290649, "last_edit_date": 1446519188, "question_id": 32378494, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/32378494/how-to-run-airflow-on-windows", "title": "How to run Airflow on Windows", "body": "<p>The usual instructions for running Airflow do not apply on a Windows environment:</p>\n\n<pre><code># airflow needs a home, ~/airflow is the default,\n# but you can lay foundation somewhere else if you prefer\n# (optional)\nexport AIRFLOW_HOME=~/airflow\n\n# install from pypi using pip\npip install airflow\n\n# initialize the database\nairflow initdb\n\n# start the web server, default port is 8080\nairflow webserver -p 8080\n</code></pre>\n\n<p>The Airflow utility is not available in the command line and I can't find it elsewhere to be manually added.\nHow can Airflow run on Windows?</p>\n"}, {"tags": ["python", "airflow", "workflow"], "owner": {"account_id": 4312454, "reputation": 661, "user_id": 3523198, "user_type": "registered", "accept_rate": 0, "profile_image": "https://www.gravatar.com/avatar/f83925c66b54c289cb56b93f388d5e86?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "santi", "link": "https://stackoverflow.com/users/3523198/santi"}, "is_answered": true, "view_count": 27939, "answer_count": 5, "score": 52, "last_activity_date": 1664366084, "creation_date": 1484018514, "last_edit_date": 1635599059, "question_id": 41560614, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/41560614/airflow-this-dag-isnt-available-in-the-webserver-dagbag-object", "title": "Airflow &quot;This DAG isnt available in the webserver DagBag object &quot;", "body": "<p>when I put a new DAG python script in the dags folder, I can view a new entry of DAG in the DAG UI but it was not enabled automatically. On top of that, it seems does not loaded properly as well. I can only click on the Refresh button few times on the right side of the list and toggle the on/off button on the left side of the list to be able to schedule the DAG. These are manual process as I need to trigger something even though the DAG Script was put inside the dag folder. </p>\n\n<p>Anyone can help me on this ? Did I missed something ? Or this is a correct behavior in airflow ?</p>\n\n<p>By the way, as mentioned in the post title, there is an indicator with this message \"This DAG isn't available in the webserver DagBag object. It shows up in this list because the scheduler marked it as active in the metdata database\" tagged with the DAG title before i trigger all this manual process.</p>\n"}, {"tags": ["bash", "airflow"], "owner": {"account_id": 2721234, "reputation": 951, "user_id": 2347491, "user_type": "registered", "accept_rate": 48, "profile_image": "https://i.sstatic.net/NL4Xd.png?s=256", "display_name": "Carleto", "link": "https://stackoverflow.com/users/2347491/carleto"}, "is_answered": true, "view_count": 61767, "accepted_answer_id": 38883535, "answer_count": 2, "score": 50, "last_activity_date": 1689521150, "creation_date": 1469468264, "last_edit_date": 1596280293, "question_id": 38574151, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/38574151/airflow-pass-parameters-to-dependent-task", "title": "Airflow pass parameters to dependent task", "body": "<p>What is the way to pass parameter into dependent tasks in Airflow? I have a lot of bashes files, and i'm trying to migrate this approach to airflow, but i don't know how to pass some properties between tasks.</p>\n<p>This is a real example:</p>\n<pre><code>#sqoop bash template\nsqoop_template = &quot;&quot;&quot;\n        sqoop job --exec {{params.job}} -- --target-dir {{params.dir}} --outdir /src/\n    &quot;&quot;&quot;\n\ns3_template = &quot;&quot;&quot;\n        s3-dist-cp --src= {{params.dir}} --dest={{params.s3}}\n    &quot;&quot;&quot;\n\n\n\n#Task of extraction in EMR\nt1 = BashOperator(\n        task_id='extract_account', \n        bash_command=sqoop_template, \n        params={'job': 'job', 'dir': 'hdfs:///account/' + time.now().strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;)},\n        dag=dag)\n#Task to upload in s3 backup.\nt2 = BashOperator(\n        task_id='s3_upload',\n        bash_command=s3_template,\n        params={}, #here i need the dir name created in t1\n        depends_on_past=True\n    )\n\nt2.set_upstream(t1)\n</code></pre>\n<p>In t2 i need to access the dir name created in t1.</p>\n<h1>Solution</h1>\n<pre><code>#Execute a valid job sqoop\ndef sqoop_import(table_name, job_name):\n    s3, hdfs = dirpath(table_name)\n    sqoop_job = job_default_config(job_name, hdfs)\n    #call(sqoop_job)\n    return {'hdfs_dir': hdfs, 's3_dir': s3}\n\ndef s3_upload(**context):\n    hdfs = context['task_instance'].xcom_pull(task_ids='sqoop_import')['hdfs_dir']\n    s3 = context['task_instance'].xcom_pull(task_ids='sqoop_import')['s3_dir']\n    s3_cpdist_job = [&quot;s3-dist-cp&quot;, &quot;--src=%s&quot; % (hdfs), &quot;--dest=%s&quot; % (s3)]\n    #call(s3_cpdist_job)\n    return {'s3_dir': s3} #context['task_instance'].xcom_pull(task_ids='sqoop_import')\n\ndef sns_notify(**context):\n    s3 = context['task_instance'].xcom_pull(task_ids='distcp_s3')['s3_dir']\n    client = boto3.client('sns')\n    arn = 'arn:aws:sns:us-east-1:744617668409:pipeline-notification-stg'\n    response = client.publish(TargetArn=arn, Message=s3)\n    return response\n\n    \n</code></pre>\n<p>That's not is the definitive solution, so improvements are welcome. Thanks.</p>\n"}, {"tags": ["python", "operators", "airflow", "bit-shift"], "owner": {"account_id": 65306, "reputation": 2989, "user_id": 192296, "user_type": "registered", "accept_rate": 80, "profile_image": "https://www.gravatar.com/avatar/a33995c3825c3d6363ff872ea5662f2d?s=256&d=identicon&r=PG", "display_name": "idazuwaika", "link": "https://stackoverflow.com/users/192296/idazuwaika"}, "is_answered": true, "view_count": 21460, "accepted_answer_id": 52389312, "answer_count": 1, "score": 49, "last_activity_date": 1579794661, "creation_date": 1537281645, "last_edit_date": 1579794661, "question_id": 52389105, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/52389105/how-operator-defines-task-dependencies-in-airflow", "title": "How &gt;&gt; operator defines task dependencies in Airflow?", "body": "<p>I was going through Apache Airflow tutorial <a href=\"https://github.com/hgrif/airflow-tutorial\" rel=\"noreferrer\">https://github.com/hgrif/airflow-tutorial</a> and encountered this section for defining task dependencies.</p>\n\n<pre><code>with DAG('airflow_tutorial_v01',\n     default_args=default_args,\n     schedule_interval='0 * * * *',\n     ) as dag:\n\nprint_hello = BashOperator(task_id='print_hello',\n                           bash_command='echo \"hello\"')\nsleep = BashOperator(task_id='sleep',\n                     bash_command='sleep 5')\nprint_world = PythonOperator(task_id='print_world',\n                             python_callable=print_world)\n\n\nprint_hello &gt;&gt; sleep &gt;&gt; print_world\n</code></pre>\n\n<p>The line that confuses me is </p>\n\n<pre><code>print_hello &gt;&gt; sleep &gt;&gt; print_world\n</code></pre>\n\n<p>What does >> mean in Python? I know bitwise operator, but can't relate to the code here.</p>\n"}, {"tags": ["docker", "amazon-ec2", "airflow"], "owner": {"account_id": 1428354, "reputation": 16762, "user_id": 1351335, "user_type": "registered", "accept_rate": 76, "profile_image": "https://www.gravatar.com/avatar/0c036d31fbe1c38535000926d111b9fd?s=256&d=identicon&r=PG", "display_name": "jdotjdot", "link": "https://stackoverflow.com/users/1351335/jdotjdot"}, "is_answered": true, "view_count": 27445, "answer_count": 7, "score": 48, "last_activity_date": 1610099999, "creation_date": 1487863581, "last_edit_date": 1488142289, "question_id": 42419834, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/42419834/airbnb-airflow-using-all-system-resources", "title": "Airbnb Airflow using all system resources", "body": "<p>We've set up Airbnb/Apache Airflow for our ETL using <code>LocalExecutor</code>, and as we've started building more complex DAGs, we've noticed that Airflow has starting using up incredible amounts of system resources. This is surprising to us because we mostly use Airflow to orchestrate tasks that happen on other servers, so Airflow DAGs spend most of their time waiting for them to complete--there's no actual execution that happens locally.</p>\n\n<p>The biggest issue is that Airflow seems to use up 100% of CPU at all times (on an AWS t2.medium), and uses over 2GB of memory with the default airflow.cfg settings.</p>\n\n<p>If relevant, we're running Airflow using docker-compose running the container twice; once as <code>scheduler</code> and once as <code>webserver</code>.</p>\n\n<p>What are we doing wrong here?  Is this normal?</p>\n\n<p><strong>EDIT:</strong>\nHere is the output from <code>htop</code>, ordered by % Memory used (since that seems to be the main issue now, I got CPU down):\n<a href=\"https://i.sstatic.net/Dq9dh.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Dq9dh.png\" alt=\"Htop\"></a>\n<a href=\"https://i.sstatic.net/AnCx6.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/AnCx6.png\" alt=\"Htop2\"></a></p>\n\n<p>I suppose in theory I could reduce the number of gunicorn workers (it's at the default of 4), but I'm not sure what all the <code>/usr/bin/dockerd</code> processes are. If Docker is complicating things I could remove it, but it's made deployment of changes really easy and I'd rather not remove it if possible.</p>\n"}, {"tags": ["python", "etl", "airflow"], "owner": {"account_id": 28822, "reputation": 1517, "user_id": 77298, "user_type": "registered", "accept_rate": 93, "profile_image": "https://www.gravatar.com/avatar/8dd2f3502a6bc64c373f226d7d3708d5?s=256&d=identicon&r=PG", "display_name": "Conor", "link": "https://stackoverflow.com/users/77298/conor"}, "is_answered": true, "view_count": 51756, "accepted_answer_id": 38028511, "answer_count": 3, "score": 47, "last_activity_date": 1618576029, "creation_date": 1466803408, "question_id": 38022323, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/38022323/how-to-set-dependencies-between-dags-in-airflow", "title": "How to set dependencies between DAGs in Airflow?", "body": "<p>I am using <a href=\"https://github.com/apache/incubator-airflow\" rel=\"noreferrer\">Airflow</a> to schedule batch jobs. I have one DAG (A) that runs every night and another DAG (B) that runs once per month. B depends on A having completed successfully. However B takes a long time to run and so I would like to keep it in a separate DAG to allow better SLA reporting. </p>\n\n<p>How can I make running DAG B dependent on a successful run of DAG A on the same day?</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 6565281, "reputation": 819, "user_id": 5075122, "user_type": "registered", "accept_rate": 67, "profile_image": "https://www.gravatar.com/avatar/43e1fb4d6e3b6177c22aa42ba9bddc5c?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "diego_c", "link": "https://stackoverflow.com/users/5075122/diego-c"}, "is_answered": true, "view_count": 48041, "accepted_answer_id": 41790921, "answer_count": 3, "score": 46, "last_activity_date": 1619676110, "creation_date": 1475695906, "last_edit_date": 1499973817, "question_id": 39882204, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/39882204/airflow-backfill-clarification", "title": "Airflow backfill clarification", "body": "<p>I'm just getting started with Airbnb's <a href=\"http://nerds.airbnb.com/airflow/\" rel=\"noreferrer\">airflow</a>, and I'm still not clear on how/when backfilling is done.  </p>\n\n<p>Specifically, there are 2 use-cases that confuse me:</p>\n\n<ol>\n<li><p>If I run <code>airflow scheduler</code> for a few minutes, stop it for a minute, then restart it again, my DAG seems to run extra tasks for the first 30 seconds or so, then it continues as normal (runs every 10 sec).  Are these extra tasks \"backfilled\" tasks that weren't able to complete in an earlier run?  If so, how would I tell airflow not to backfill those tasks? </p></li>\n<li><p>If I run <code>airflow scheduler</code> for a few minutes, then run <code>airflow clear MY_tutorial</code>, then restart <code>airflow scheduler</code>, it seems to run a TON of extra tasks.  Are these tasks also somehow \"backfilled\" tasks?  Or am I missing something.</p></li>\n</ol>\n\n<p>Currently, I have a very simple dag: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>default_args = {\n    'owner': 'me',\n    'depends_on_past': False,\n    'start_date': datetime(2016, 10, 4),\n    'email': ['airflow@airflow.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n    # 'queue': 'bash_queue',\n    # 'pool': 'backfill',\n    # 'priority_weight': 10,\n    # 'end_date': datetime(2016, 1, 1),\n}\n\ndag = DAG(\n    'MY_tutorial', default_args=default_args, schedule_interval=timedelta(seconds=10))\n\n# t1, t2 and t3 are examples of tasks created by instantiating operators\nt1 = BashOperator(\n    task_id='print_date',\n    bash_command='date',\n    dag=dag)\n\nt2 = BashOperator(\n    task_id='sleep',\n    bash_command='sleep 5',\n    retries=3,\n    dag=dag)\n\ntemplated_command = \"\"\"\n    {% for i in range(5) %}\n        echo \"{{ ds }}\"\n        echo \"{{ macros.ds_add(ds, 8)}}\"\n        echo \"{{ params.my_param }}\"\n    {% endfor %}\n\"\"\"\n\nt3 = BashOperator(\n    task_id='templated',\n    bash_command=templated_command,\n    params={'my_param': 'Parameter I passed in'},\n    dag=dag)\n\nsecond_template = \"\"\"\n    touch ~/airflow/logs/test\n    echo $(date) &gt;&gt; ~/airflow/logs/test\n\"\"\"\n\nt4 = BashOperator(\n    task_id='write_test',\n    bash_command=second_template,\n    dag=dag)\n\nt1.set_upstream(t4)\nt2.set_upstream(t1)\nt3.set_upstream(t1)\n</code></pre>\n\n<p>The only two things I've changed in my airflow config are</p>\n\n<ol>\n<li>I changed from using a sqlite db to using a postgres db</li>\n<li>I'm using a <code>CeleryExecutor</code> instead of a <code>SequentialExecutor</code></li>\n</ol>\n\n<p>Thanks so much for you help!</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 5196137, "reputation": 1234, "user_id": 4157370, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-pVrgWRz3fI4/AAAAAAAAAAI/AAAAAAAAAGU/2b6KGmm3EvE/photo.jpg?sz=256", "display_name": "Tin Ng", "link": "https://stackoverflow.com/users/4157370/tin-ng"}, "is_answered": true, "view_count": 55771, "accepted_answer_id": 49292022, "answer_count": 4, "score": 45, "last_activity_date": 1721651455, "creation_date": 1520845133, "last_edit_date": 1624617431, "question_id": 49231340, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/49231340/how-to-limit-airflow-to-run-only-one-instance-of-a-dag-run-at-a-time", "title": "How to limit Airflow to run only one instance of a DAG run at a time?", "body": "<p>I want the tasks in the DAG to all finish before the 1st task of the next run gets executed.</p>\n<p>I have max_active_runs = 1, but <a href=\"https://i.sstatic.net/sxe54.png\" rel=\"noreferrer\">this</a> still happens.</p>\n<pre><code>default_args = {\n    'depends_on_past': True,\n    'wait_for_downstream': True,\n    'max_active_runs': 1,\n    'start_date': datetime(2018, 03, 04),\n    'owner': 't.n',\n    'email': ['t.n@example.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=4)\n}\n\ndag = DAG('example', default_args=default_args, schedule_interval = schedule_interval)\n</code></pre>\n<p>(All of my tasks are dependent on the previous task. Airflow version is 1.8.0)</p>\n<p>Thank you</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 3203491, "reputation": 727, "user_id": 2705076, "user_type": "registered", "accept_rate": 85, "profile_image": "https://www.gravatar.com/avatar/b0867f2d87b97dfd4a95ce81bae7eeb1?s=256&d=identicon&r=PG", "display_name": "quaintm", "link": "https://stackoverflow.com/users/2705076/quaintm"}, "is_answered": true, "view_count": 29893, "accepted_answer_id": 42198617, "answer_count": 3, "score": 45, "last_activity_date": 1608331478, "creation_date": 1486676960, "question_id": 42147514, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/42147514/templatenotfound-error-when-running-simple-airflow-bashoperator", "title": "TemplateNotFound error when running simple Airflow BashOperator", "body": "<p>I'm trying to write our first Airflow DAG, and I'm getting the following error when I try to list the tasks using command <code>airflow list_tasks orderwarehouse</code>:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/airflow/models.py\", line 2038, in resolve_template_files\n    setattr(self, attr, env.loader.get_source(env, content)[0])\n  File \"/usr/local/lib/python2.7/site-packages/jinja2/loaders.py\", line 187, in get_source\n    raise TemplateNotFound(template)\nTemplateNotFound: ./home/deploy/airflow-server/task_scripts/orderwarehouse/load_warehouse_tables.sh\n</code></pre>\n\n<p>This DAG is not supposed to use a template. I'm only trying to run the shell script in the specified location per the instructions in <a href=\"https://airflow.incubator.apache.org/_modules/bash_operator.html#BashOperator\" rel=\"noreferrer\">the docs</a>. The shell script does exist in that location and is spelled correctly. My DAG looks like this:</p>\n\n<pre><code>from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2015, 6, 1),\n    'email': ['airflow@airflow.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n    # 'queue': 'bash_queue',\n    # 'pool': 'backfill',\n    # 'priority_weight': 10,\n    # 'end_date': datetime(2016, 1, 1),\n}\n\norderwarehouse = DAG('orderwarehouse', default_args=default_args)\n\nload_mysql = BashOperator(\n    task_id='load_warehouse_mysql',\n    bash_command='./home/deploy/airflow-server/task_scripts/orderwarehouse/load_warehouse_tables.sh',\n    dag=orderwarehouse)\n</code></pre>\n\n<p>Not sure why it thinks it needs to look for a Jinja template. Running out of ideas on this one, would appreciate if anyone can point me to where I'm going astray. Thanks.</p>\n"}, {"tags": ["python", "amazon-s3", "airflow"], "owner": {"account_id": 1884722, "reputation": 1643, "user_id": 1704177, "user_type": "registered", "accept_rate": 62, "profile_image": "https://www.gravatar.com/avatar/c5bd3183a547a60a7ab76dc25129830d?s=256&d=identicon&r=PG", "display_name": "JackStat", "link": "https://stackoverflow.com/users/1704177/jackstat"}, "is_answered": true, "view_count": 44698, "accepted_answer_id": 44796247, "answer_count": 8, "score": 45, "last_activity_date": 1668930185, "creation_date": 1498567766, "last_edit_date": 1499098635, "question_id": 44780736, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/44780736/setting-up-s3-for-logs-in-airflow", "title": "setting up s3 for logs in airflow", "body": "<p>I am using docker-compose to set up a scalable airflow cluster. I based my approach off of this Dockerfile <a href=\"https://hub.docker.com/r/puckel/docker-airflow/\" rel=\"noreferrer\">https://hub.docker.com/r/puckel/docker-airflow/</a></p>\n\n<p>My problem is getting the logs set up to write/read from s3. When a dag has completed I get an error like this</p>\n\n<pre><code>*** Log file isn't local.\n*** Fetching here: http://ea43d4d49f35:8793/log/xxxxxxx/2017-06-26T11:00:00\n*** Failed to fetch log file from worker.\n\n*** Reading remote logs...\nCould not read logs from s3://buckets/xxxxxxx/airflow/logs/xxxxxxx/2017-06-\n26T11:00:00\n</code></pre>\n\n<p>I set up a new section in the <code>airflow.cfg</code> file like this</p>\n\n<pre><code>[MyS3Conn]\naws_access_key_id = xxxxxxx\naws_secret_access_key = xxxxxxx\naws_default_region = xxxxxxx\n</code></pre>\n\n<p>And then specified the s3 path in the remote logs section in <code>airflow.cfg</code></p>\n\n<pre><code>remote_base_log_folder = s3://buckets/xxxx/airflow/logs\nremote_log_conn_id = MyS3Conn\n</code></pre>\n\n<p>Did I set this up properly and there is a bug? Is there a recipe for success here that I am missing?</p>\n\n<p>-- Update</p>\n\n<p>I tried exporting in URI and JSON formats and neither seemed to work. I then exported the aws_access_key_id and aws_secret_access_key and then airflow started picking it up. Now I get his error in the worker logs </p>\n\n<pre><code>6/30/2017 6:05:59 PMINFO:root:Using connection to: s3\n6/30/2017 6:06:00 PMERROR:root:Could not read logs from s3://buckets/xxxxxx/airflow/logs/xxxxx/2017-06-30T23:45:00\n6/30/2017 6:06:00 PMERROR:root:Could not write logs to s3://buckets/xxxxxx/airflow/logs/xxxxx/2017-06-30T23:45:00\n6/30/2017 6:06:00 PMLogging into: /usr/local/airflow/logs/xxxxx/2017-06-30T23:45:00\n</code></pre>\n\n<p>-- Update</p>\n\n<p>I found this link as well \n<a href=\"https://www.mail-archive.com/dev@airflow.incubator.apache.org/msg00462.html\" rel=\"noreferrer\">https://www.mail-archive.com/dev@airflow.incubator.apache.org/msg00462.html</a></p>\n\n<p>I then shelled into one of my worker machines (separate from the webserver and scheduler) and ran this bit of code in python</p>\n\n<pre><code>import airflow\ns3 = airflow.hooks.S3Hook('s3_conn')\ns3.load_string('test', airflow.conf.get('core', 'remote_base_log_folder'))\n</code></pre>\n\n<p>I receive this error.</p>\n\n<pre><code>boto.exception.S3ResponseError: S3ResponseError: 403 Forbidden\n</code></pre>\n\n<p>I tried exporting several different types of <code>AIRFLOW_CONN_</code> envs as explained here in the connections section <a href=\"https://airflow.incubator.apache.org/concepts.html\" rel=\"noreferrer\">https://airflow.incubator.apache.org/concepts.html</a> and by other answers to this question.</p>\n\n<pre><code>s3://&lt;AWS_ACCESS_KEY_ID&gt;:&lt;AWS_SECRET_ACCESS_KEY&gt;@S3\n\n{\"aws_account_id\":\"&lt;xxxxx&gt;\",\"role_arn\":\"arn:aws:iam::&lt;xxxx&gt;:role/&lt;xxxxx&gt;\"}\n\n{\"aws_access_key_id\":\"&lt;xxxxx&gt;\",\"aws_secret_access_key\":\"&lt;xxxxx&gt;\"}\n</code></pre>\n\n<p>I have also exported AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY with no success.</p>\n\n<p>These credentials are being stored in a database so once I add them in the UI they should be picked up by the workers but they are not able to write/read logs for some reason.</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 11046249, "reputation": 451, "user_id": 8112234, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/5c8a1876285c5e0bef11805bf1d00e40?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "J. Doe", "link": "https://stackoverflow.com/users/8112234/j-doe"}, "is_answered": true, "view_count": 50496, "answer_count": 1, "score": 45, "last_activity_date": 1642106120, "creation_date": 1496629279, "question_id": 44360853, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/44360853/airflow-run-task-regardless-of-upstream-success-fail", "title": "Airflow - run task regardless of upstream success/fail", "body": "<p>I have a DAG which fans out to multiple independent units in parallel. This runs in AWS, so we have tasks which scale our AutoScalingGroup up to the maximum number of workers when the DAG starts, and to the minimum when the DAG completes. The simplified version looks like this:</p>\n\n<pre><code>           | - - taskA - - |\n           |               |\nscaleOut - | - - taskB - - | - scaleIn\n           |               |\n           | - - taskC - - |\n</code></pre>\n\n<p>However, some of the tasks in the parallel set fail occasionally, and I can't get the scaleDown task to run when any of the A-C tasks fail.</p>\n\n<p>What's the best way to have a task execute at the end of the DAG, once all other tasks have completed (success or fail)? The depends_on_upstream setting sounded like what we needed, but didn't actually do anything based on testing.</p>\n"}, {"tags": ["python", "python-3.x", "airflow", "directed-acyclic-graphs"], "owner": {"account_id": 5530948, "reputation": 1176, "user_id": 4390925, "user_type": "registered", "accept_rate": 82, "profile_image": "https://www.gravatar.com/avatar/5054bc9752174a671034e22cad244113?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Rusty", "link": "https://stackoverflow.com/users/4390925/rusty"}, "is_answered": true, "view_count": 99052, "accepted_answer_id": 39005303, "answer_count": 15, "score": 44, "last_activity_date": 1680723489, "creation_date": 1471426694, "last_edit_date": 1570731104, "question_id": 38992997, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/38992997/dag-not-visible-in-web-ui", "title": "DAG not visible in Web-UI", "body": "<p>I am new to <code>Airflow</code>. I am following a tutorial and written following code.</p>\n\n<pre><code>from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\nfrom models.correctness_prediction import CorrectnessPrediction\n\ndefault_args = {\n    'owner': 'abc',\n    'depends_on_past': False,\n    'start_date': datetime.now(),\n    'email': ['abc@xyz.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndef correctness_prediction(arg):\n    CorrectnessPrediction.train()\n\ndag = DAG('daily_processing', default_args=default_args)\n\ntask_1 = PythonOperator(\n    task_id='print_the_context',\n    provide_context=True,\n    python_callable=correctness_prediction,\n    dag=dag)\n</code></pre>\n\n<p>On running the script, it doesn't show any errors but when I check for <code>dags</code> in <code>Web-UI</code> it doesn't show under <strong>Menu</strong>-><strong>DAGs</strong></p>\n\n<p><a href=\"https://i.sstatic.net/PeBid.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/PeBid.png\" alt=\"enter image description here\"></a></p>\n\n<p>But I can see the scheduled <code>job</code> under <strong>Menu</strong>-><strong>Browse</strong>-><strong>Jobs</strong></p>\n\n<p><a href=\"https://i.sstatic.net/PB1xe.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/PB1xe.png\" alt=\"enter image description here\"></a></p>\n\n<p>I also cannot see anything in $AIRFLOW_HOME/dags. Is it supposed to be like this only? Can someone explain why?</p>\n"}, {"tags": ["python", "python-3.x", "parallel-processing", "airflow"], "owner": {"account_id": 12394026, "reputation": 1539, "user_id": 9464073, "user_type": "registered", "profile_image": "https://i.sstatic.net/YOTvg.png?s=256", "display_name": "Mr. President", "link": "https://stackoverflow.com/users/9464073/mr-president"}, "is_answered": true, "view_count": 100137, "accepted_answer_id": 52741626, "answer_count": 3, "score": 44, "last_activity_date": 1725784780, "creation_date": 1539178647, "last_edit_date": 1568957131, "question_id": 52741536, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/52741536/running-airflow-tasks-dags-in-parallel", "title": "Running airflow tasks/dags in parallel", "body": "<p>I'm using airflow to orchestrate some python scripts. I have a \"main\" dag from which several subdags are run. My main dag is supposed to run according to the following overview:</p>\n\n<p><a href=\"https://i.sstatic.net/X8sNT.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/X8sNT.png\" alt=\"enter image description here\"></a></p>\n\n<p>I've managed to get to this structure in my main dag by using the following lines:</p>\n\n<pre><code>etl_internal_sub_dag1 &gt;&gt; etl_internal_sub_dag2 &gt;&gt; etl_internal_sub_dag3\netl_internal_sub_dag3 &gt;&gt; etl_adzuna_sub_dag\netl_internal_sub_dag3 &gt;&gt; etl_adwords_sub_dag\netl_internal_sub_dag3 &gt;&gt; etl_facebook_sub_dag\netl_internal_sub_dag3 &gt;&gt; etl_pagespeed_sub_dag\n\netl_adzuna_sub_dag &gt;&gt; etl_combine_sub_dag\netl_adwords_sub_dag &gt;&gt; etl_combine_sub_dag\netl_facebook_sub_dag &gt;&gt; etl_combine_sub_dag\netl_pagespeed_sub_dag &gt;&gt; etl_combine_sub_dag\n</code></pre>\n\n<p>What I want airflow to do is to first run the <code>etl_internal_sub_dag1</code> then the <code>etl_internal_sub_dag2</code> and then the <code>etl_internal_sub_dag3</code>. When the <code>etl_internal_sub_dag3</code> is finished I want <code>etl_adzuna_sub_dag</code>, <code>etl_adwords_sub_dag</code>, <code>etl_facebook_sub_dag</code>, and <code>etl_pagespeed_sub_dag</code> to run in parallel. Finally, when these last four scripts are finished, I want the <code>etl_combine_sub_dag</code> to run. </p>\n\n<p>However, when I run the main dag, <code>etl_adzuna_sub_dag</code>, <code>etl_adwords_sub_dag</code>, <code>etl_facebook_sub_dag</code>, and <code>etl_pagespeed_sub_dag</code>  are run one by one and not in parallel.</p>\n\n<p><strong>Question:</strong> How do I make sure that the scripts <code>etl_adzuna_sub_dag</code>, <code>etl_adwords_sub_dag</code>, <code>etl_facebook_sub_dag</code>, and <code>etl_pagespeed_sub_dag</code> are run in parallel?</p>\n\n<p><strong>Edit:</strong> My <code>default_args</code> and <code>DAG</code> look like this:</p>\n\n<pre><code>default_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': start_date,\n    'end_date': end_date,\n    'email': ['myname@gmail.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n}\n\nDAG_NAME = 'main_dag'\n\ndag = DAG(DAG_NAME, default_args=default_args, catchup = False)\n</code></pre>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 1916158, "reputation": 945, "user_id": 1728716, "user_type": "registered", "accept_rate": 4, "profile_image": "https://i.sstatic.net/7Qqkc.jpg?s=256", "display_name": "Anup", "link": "https://stackoverflow.com/users/1728716/anup"}, "is_answered": true, "view_count": 86842, "answer_count": 8, "score": 43, "last_activity_date": 1663170996, "creation_date": 1500899239, "last_edit_date": 1609174001, "question_id": 45280650, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/45280650/store-and-access-password-using-apache-airflow", "title": "Store and access password using Apache airflow", "body": "<p>We are using Airflow as a scheduler. I want to invoke a simple bash operator in a DAG. The bash script needs a password as an argument to do further processing.</p>\n<p>How can I store a password securely in Airflow (<code>config/variables/connection</code>) and access it in dag definition file?</p>\n<p>I am new to Airflow and Python so a code snippet will be appreciated.</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 6015594, "reputation": 983, "user_id": 4724582, "user_type": "registered", "accept_rate": 43, "profile_image": "https://lh6.googleusercontent.com/-O0vMFo_Wf40/AAAAAAAAAAI/AAAAAAAAACU/LWWPi_c7U58/photo.jpg?sz=256", "display_name": "Christopher Carlson", "link": "https://stackoverflow.com/users/4724582/christopher-carlson"}, "is_answered": true, "view_count": 66317, "answer_count": 12, "score": 43, "last_activity_date": 1623241455, "creation_date": 1494483207, "last_edit_date": 1498863015, "question_id": 43907813, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/43907813/cant-import-airflow-plugins", "title": "Can&#39;t import Airflow plugins", "body": "<p>Following Airflow tutorial <a href=\"http://michal.karzynski.pl/blog/2017/03/19/developing-workflows-with-apache-airflow/\" rel=\"noreferrer\">here</a>.</p>\n\n<p><strong>Problem</strong>: The webserver returns the following error</p>\n\n<pre><code>Broken DAG: [/usr/local/airflow/dags/test_operator.py] cannot import name \nMyFirstOperator\n</code></pre>\n\n<p><strong>Notes:</strong>\nThe directory structure looks like this:</p>\n\n<pre><code>airflow_home\n\u251c\u2500\u2500 airflow.cfg\n\u251c\u2500\u2500 airflow.db\n\u251c\u2500\u2500 dags\n\u2502   \u2514\u2500\u2500 test_operators.py  \n\u251c\u2500\u2500 plugins\n\u2502   \u2514\u2500\u2500 my_operators.py   \n\u2514\u2500\u2500 unittests.cfg\n</code></pre>\n\n<p>I am attempting to import the plugin in 'test_operators.py' like this:</p>\n\n<pre><code>from airflow.operators import MyFirstOperator\n</code></pre>\n\n<p>The code is all the same as what is found in the tutorial.</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 2263907, "reputation": 3826, "user_id": 1993206, "user_type": "registered", "accept_rate": 97, "profile_image": "https://i.sstatic.net/LhdIi.jpg?s=256", "display_name": "Aleksey Bilogur", "link": "https://stackoverflow.com/users/1993206/aleksey-bilogur"}, "is_answered": true, "view_count": 58719, "accepted_answer_id": 44856401, "answer_count": 4, "score": 43, "last_activity_date": 1636002793, "creation_date": 1498867499, "question_id": 44856214, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/44856214/how-to-add-new-dags-to-airflow", "title": "How to add new DAGs to Airflow?", "body": "<p>I have defined a DAG in a file called <code>tutorial_2.py</code> (actually a copy of the <code>tutorial.py</code> provided in the <code>airflow</code> tutorial, except with the <code>dag_id</code> changed to <code>tutorial_2</code>).</p>\n\n<p>When I look inside my default, unmodified <code>airflow.cfg</code> (located in <code>~/airflow</code>), I see that <code>dags_folder</code> is set to <code>/home/alex/airflow/dags</code>. </p>\n\n<p>I do <code>cd /home/alex/airflow; mkdir dags; cd dags; cp [...]/tutorial_2.py tutorial_2.py</code>. Now I have a <code>dags</code> folder matching the path set in <code>airflow.cfg</code> , containing the <code>tutorial_2.py</code> file I created earlier.</p>\n\n<p>However, when I run <code>airflow list_dags</code>, I only get the names corresponding with the default, tutorial DAGs.</p>\n\n<p>I would like to have <code>tutorial_2</code> show up in my DAG list, so that I can begin interacting with. Neither <code>python tutorial_2.py</code> nor <code>airflow resetdb</code> have caused it to appear in the list.</p>\n\n<p>How do I remedy this?</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 12313231, "reputation": 455, "user_id": 8984147, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/008e0ff58ec2e44fb7231ed5e90789ec?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "hotchocolate", "link": "https://stackoverflow.com/users/8984147/hotchocolate"}, "is_answered": true, "view_count": 48940, "answer_count": 3, "score": 43, "last_activity_date": 1627292509, "creation_date": 1525336156, "last_edit_date": 1559857689, "question_id": 50150384, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/50150384/importing-local-module-python-script-in-airflow-dag", "title": "Importing local module (python script) in Airflow DAG", "body": "<p>I'm trying to import a local module (a python script) to my DAG.  </p>\n\n<p>Directory structure:  </p>\n\n<pre><code>airflow/\n\u251c\u2500\u2500 dag\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my_DAG.py\n\u2514\u2500\u2500 script\n    \u2514\u2500\u2500 subfolder\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 local_module.py\n\n</code></pre>\n\n<p>Sample code in my_DAG.py:</p>\n\n<pre><code>#trying to import from local module\nfrom script.subfolder import local_module  \n\n#calling a function in local_module.py  \na = some_function()  \n</code></pre>\n\n<p>I get an error in Airflow saying 'Broken DAG: my_DAG. No module named 'local_module'.  </p>\n\n<p>I've updated Airflow to 1.9.0 but this doesn't fix the issue. </p>\n\n<ul>\n<li>What is the solution here?  </li>\n<li>I also read somewhere that I could solve this by creating a plugin. Can anyone point to how I can do this?  </li>\n</ul>\n\n<p>Thanks. </p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 1252232, "reputation": 3677, "user_id": 1212419, "user_type": "registered", "accept_rate": 61, "profile_image": "https://www.gravatar.com/avatar/0dfd482bda37a2237d0e0db6935de140?s=256&d=identicon&r=PG", "display_name": "mdivk", "link": "https://stackoverflow.com/users/1212419/mdivk"}, "is_answered": true, "view_count": 99296, "accepted_answer_id": 54895796, "answer_count": 2, "score": 41, "last_activity_date": 1599781075, "creation_date": 1551216582, "question_id": 54894418, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/54894418/how-to-pass-parameter-to-pythonoperator-in-airflow", "title": "How to pass parameter to PythonOperator in Airflow", "body": "<p>I just started using <strong>Airflow</strong>, can anyone enlighten me how to pass a parameter into <strong>PythonOperator</strong> like below:</p>\n\n<pre><code>t5_send_notification = PythonOperator(\n    task_id='t5_send_notification',\n    provide_context=True,\n    python_callable=SendEmail,\n    op_kwargs=None,\n    #op_kwargs=(key1='value1', key2='value2'),\n    dag=dag,\n)\n\ndef SendEmail(**kwargs):\n    msg = MIMEText(\"The pipeline for client1 is completed, please check.\")\n    msg['Subject'] = \"xxxx\"\n    msg['From'] = \"xxxx\"\n    ......\n    s = smtplib.SMTP('localhost')\n    s.send_message(msg)\n    s.quit()\n</code></pre>\n\n<p>I would like to be able to pass some parameters into the <code>t5_send_notification</code>'s callable which is <code>SendEmail</code>, ideally I want to attach the full log and/or part of the log (which is essentially from the kwargs) to the email to be sent out, guessing the <code>t5_send_notification</code> is the place to gather those information.</p>\n\n<p>Thank you very much.</p>\n"}, {"tags": ["airflow", "airflow-scheduler"], "owner": {"account_id": 924352, "reputation": 4959, "user_id": 955422, "user_type": "registered", "accept_rate": 86, "profile_image": "https://i.sstatic.net/pJ3AT.png?s=256", "display_name": "Ikar Pohorsk&#253;", "link": "https://stackoverflow.com/users/955422/ikar-pohorsk%c3%bd"}, "is_answered": true, "view_count": 20903, "accepted_answer_id": 46608415, "answer_count": 2, "score": 41, "last_activity_date": 1653391934, "creation_date": 1507298517, "last_edit_date": 1507298905, "question_id": 46607552, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/46607552/how-to-define-airflow-dag-task-that-shouldnt-run-periodically", "title": "How to define Airflow DAG/task that shouldn&#39;t run periodically", "body": "<p>The goal is pretty simple: I need to create a DAG for a manual task that should not run periodically, but only when admin presses the \"Run\" button. Ideally without a need to switch \"unpause\" and \"pause\" the DAG (you know someone will surely forget to pause).</p>\n\n<p>So far I only came with <code>schedule_interval=\"0 0 30 2 *\"</code> (30th Feb hopefully never occurs), but there must be a better way!</p>\n\n<p>Is there?</p>\n"}, {"tags": ["java", "python", "apache-spark", "directed-acyclic-graphs", "airflow"], "owner": {"account_id": 7784751, "reputation": 487, "user_id": 5890044, "user_type": "registered", "accept_rate": 75, "profile_image": "https://lh3.googleusercontent.com/-XXtyL6ZqmI4/AAAAAAAAAAI/AAAAAAAAAAw/e-acUb6FZp4/photo.jpg?sz=256", "display_name": "Ruslan Lomov", "link": "https://stackoverflow.com/users/5890044/ruslan-lomov"}, "is_answered": true, "view_count": 45270, "accepted_answer_id": 39833103, "answer_count": 4, "score": 41, "last_activity_date": 1687820579, "creation_date": 1475484969, "last_edit_date": 1513791584, "question_id": 39827804, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/39827804/how-to-run-spark-code-in-airflow", "title": "How to run Spark code in Airflow?", "body": "<p>Hello people of the Earth!\nI'm using Airflow to schedule and run Spark tasks.\nAll I found by this time is python DAGs that Airflow can manage.\n<br> DAG example:</p>\n\n<pre><code>spark_count_lines.py\nimport logging\n\nfrom airflow import DAG\nfrom airflow.operators import PythonOperator\n\nfrom datetime import datetime\n\nargs = {\n  'owner': 'airflow'\n  , 'start_date': datetime(2016, 4, 17)\n  , 'provide_context': True\n}\n\ndag = DAG(\n  'spark_count_lines'\n  , start_date = datetime(2016, 4, 17)\n  , schedule_interval = '@hourly'\n  , default_args = args\n)\n\ndef run_spark(**kwargs):\n  import pyspark\n  sc = pyspark.SparkContext()\n  df = sc.textFile('file:///opt/spark/current/examples/src/main/resources/people.txt')\n  logging.info('Number of lines in people.txt = {0}'.format(df.count()))\n  sc.stop()\n\nt_main = PythonOperator(\n  task_id = 'call_spark'\n  , dag = dag\n  , python_callable = run_spark\n)\n</code></pre>\n\n<p>The problem is I'm not good in Python code and have some tasks written in Java. My question is how to run Spark Java jar in python DAG? Or maybe there is other way yo do it? I found spark submit: <a href=\"http://spark.apache.org/docs/latest/submitting-applications.html\" rel=\"noreferrer\">http://spark.apache.org/docs/latest/submitting-applications.html</a>\n<br>But I don't know how to connect everything together. Maybe someone used it before and has working example. Thank you for your time!</p>\n"}, {"tags": ["python", "apache-nifi", "airflow"], "owner": {"account_id": 5497574, "reputation": 1943, "user_id": 4368131, "user_type": "registered", "accept_rate": 39, "profile_image": "https://www.gravatar.com/avatar/78ab13784d401dcde994ce4689ffa2ca?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "CMPE", "link": "https://stackoverflow.com/users/4368131/cmpe"}, "is_answered": true, "view_count": 34928, "closed_date": 1474615595, "accepted_answer_id": 39481470, "answer_count": 1, "score": 41, "last_activity_date": 1561539866, "creation_date": 1473363993, "question_id": 39399065, "link": "https://stackoverflow.com/questions/39399065/airbnb-airflow-vs-apache-nifi", "closed_reason": "Opinion-based", "title": "Airbnb Airflow vs Apache Nifi", "body": "<p>Are Airflow and Nifi perform the same job on workflows? What are the pro/con for each one?\nI need to read some json files, add more custom metadata to it and put it in a Kafka queue to be processed. I was able to do it in Nifi. I am still working on Airflow. I am trying to choose the best workflow engine for my project\nThank you! </p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 4213648, "reputation": 2249, "user_id": 3450594, "user_type": "registered", "accept_rate": 73, "profile_image": "https://www.gravatar.com/avatar/518579f030074e130bd77ba962591e5b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "JChao", "link": "https://stackoverflow.com/users/3450594/jchao"}, "is_answered": true, "view_count": 30850, "accepted_answer_id": 51863960, "answer_count": 5, "score": 40, "last_activity_date": 1645049474, "creation_date": 1534355350, "question_id": 51863881, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51863881/is-there-a-way-to-create-modify-connections-through-airflow-api", "title": "Is there a way to create/modify connections through Airflow API", "body": "<p>Going through <code>Admin -&gt; Connections</code>, we have the ability to create/modify a connection's params, but I'm wondering if I can do the same through API so I can programmatically set the connections</p>\n\n<p><code>airflow.models.Connection</code> seems like it only deals with actually connecting to the instance instead of saving it to the list. It seems like a function that should have been implemented, but I'm not sure where I can find the docs for this specific function.</p>\n"}, {"tags": ["python", "jupyter-notebook", "airflow", "google-colaboratory"], "owner": {"account_id": 17945490, "reputation": 417, "user_id": 13039877, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/1b0ea5994e5549dbb7838669670df582?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Sado", "link": "https://stackoverflow.com/users/13039877/sado"}, "is_answered": true, "view_count": 64172, "answer_count": 5, "score": 39, "last_activity_date": 1680347453, "creation_date": 1589318028, "question_id": 61762045, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61762045/running-google-colab-every-day-at-a-specific-time", "title": "Running google colab every day at a specific time", "body": "<p>I recently have built a Python program that runs on Google Colaboratory, I need to run the program every day at a specific time, So Is there any way to schedule it to run on Google Colab?</p>\n"}, {"tags": ["python", "ssh", "airflow"], "owner": {"account_id": 5497574, "reputation": 1943, "user_id": 4368131, "user_type": "registered", "accept_rate": 39, "profile_image": "https://www.gravatar.com/avatar/78ab13784d401dcde994ce4689ffa2ca?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "CMPE", "link": "https://stackoverflow.com/users/4368131/cmpe"}, "is_answered": true, "view_count": 71700, "protected_date": 1689053227, "accepted_answer_id": 39494330, "answer_count": 4, "score": 38, "last_activity_date": 1688745296, "creation_date": 1473709108, "last_edit_date": 1473867262, "question_id": 39457592, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/39457592/airflow-how-to-ssh-and-run-bashoperator-from-a-different-server", "title": "Airflow: How to SSH and run BashOperator from a different server", "body": "<p>Is there a way to ssh to different server and run BashOperator using Airbnb's Airflow?\nI am trying to run a hive sql command with Airflow but I need to SSH to a different box in order to run the hive shell.\nMy tasks should look like this:</p>\n\n<ol>\n<li>SSH to server1</li>\n<li>start Hive shell</li>\n<li>run Hive command</li>\n</ol>\n\n<p>Thanks!</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 2533204, "reputation": 2401, "user_id": 2201037, "user_type": "registered", "accept_rate": 71, "profile_image": "https://www.gravatar.com/avatar/85b7d0d3d0655bda0f0b5195cee3438e?s=256&d=identicon&r=PG", "display_name": "nono", "link": "https://stackoverflow.com/users/2201037/nono"}, "is_answered": true, "view_count": 35031, "accepted_answer_id": 44527779, "answer_count": 3, "score": 38, "last_activity_date": 1672331629, "creation_date": 1496879252, "question_id": 44424473, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/44424473/airflow-structure-organization-of-dags-and-tasks", "title": "Airflow structure/organization of Dags and tasks", "body": "<p>My questions :</p>\n\n<ul>\n<li>What is a good directory structure in order to organize your dags and tasks? (the dags examples show only couple of tasks)</li>\n<li>I currently have my dags at the root of the dags folder and my tasks in separate directories, not sure is the way to do it ?</li>\n<li>Should we use zip files ? <a href=\"https://github.com/apache/incubator-airflow/blob/a1f4227bee1a70531cfa90769149322513cb6f92/airflow/models.py#L280\" rel=\"noreferrer\">https://github.com/apache/incubator-airflow/blob/a1f4227bee1a70531cfa90769149322513cb6f92/airflow/models.py#L280</a></li>\n</ul>\n"}, {"tags": ["python", "smtp", "airflow"], "owner": {"account_id": 7430406, "reputation": 419, "user_id": 5651595, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-Cz8fPbszabc/AAAAAAAAAAI/AAAAAAAAABE/opxCb1ZCJz0/photo.jpg?sz=256", "display_name": "Peter Cui", "link": "https://stackoverflow.com/users/5651595/peter-cui"}, "is_answered": true, "view_count": 88752, "accepted_answer_id": 51837049, "answer_count": 3, "score": 38, "last_activity_date": 1692700822, "creation_date": 1534188013, "question_id": 51829200, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51829200/how-to-set-up-airflow-send-email", "title": "How to set up Airflow Send Email?", "body": "<p>I followed online tutorial to set up Email SMTP server in airflow.cfg as below:</p>\n\n<pre><code>[email]\nemail_backend = airflow.utils.email.send_email_smtp\n\n\n[smtp]\n# If you want airflow to send emails on retries, failure, and you want to use\n# the airflow.utils.email.send_email_smtp function, you have to configure an\n# smtp server here\nsmtp_host = smtp.gmail.com\nsmtp_starttls = True\nsmtp_ssl = False\n# Uncomment and set the user/pass settings if you want to use SMTP AUTH \n# smtp_user =                       \n# smtp_password =  \nsmtp_port = 587\nsmtp_mail_from = myemail@gmail.com\n</code></pre>\n\n<p>And my DAG is as below:</p>\n\n<pre><code>from datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.email_operator import EmailOperator\n\ndef print_hello():\n    return 'Hello world!'\n\ndefault_args = {\n        'owner': 'peter',\n        'start_date':datetime(2018,8,11),\n}\n\ndag = DAG('hello_world', description='Simple tutorial DAG',\n          schedule_interval='* * * * *',\n          default_args = default_args, catchup=False)\n\ndummy_operator = DummyOperator(task_id='dummy_task', retries=3, dag=dag)\n\nhello_operator = PythonOperator(task_id='hello_task', python_callable=print_hello, dag=dag)\n\nemail = EmailOperator(\n        task_id='send_email',\n        to='to@gmail.com',\n        subject='Airflow Alert',\n        html_content=\"\"\" &lt;h3&gt;Email Test&lt;/h3&gt; \"\"\",\n        dag=dag\n)\n\nemail &gt;&gt; dummy_operator &gt;&gt; hello_operator\n</code></pre>\n\n<p>I assumed the email operator will run after the other two operators and then send me an email.\nBut email was not sent to me.\nI really appreciate your help. Thank you very much.</p>\n\n<p>Best</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 17992595, "reputation": 393, "user_id": 13076373, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gg8yr5CgPaignhK4yHutcmN3iXar-K0qet7cuvZig=k-s256", "display_name": "Masood Bashamaq", "link": "https://stackoverflow.com/users/13076373/masood-bashamaq"}, "is_answered": true, "view_count": 9447, "accepted_answer_id": 61102826, "answer_count": 3, "score": 38, "last_activity_date": 1594401846, "creation_date": 1586352033, "last_edit_date": 1586394917, "question_id": 61101729, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61101729/starting-airflow-webserver-fails-with-sqlalchemy-exc-noinspectionavailable-no-i", "title": "Starting Airflow webserver fails with sqlalchemy.exc.NoInspectionAvailable: No inspection system is available", "body": "<p>Installation done properly. db initiated properly and trying to start the webserver shows the following error.</p>\n\n<p>I reinstalled everything but its still not working.</p>\n\n<p>I will appreciate if anyone help me.</p>\n\n<p>Console output:</p>\n\n<pre><code>$:~/airflow# airflow webserver -p 8080\n  ____________       _____________\n ____    |__( )_________  __/__  /________      __\n____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n[2020-04-08 13:14:20,573] {__init__.py:51} INFO - Using executor SequentialExecutor\n[2020-04-08 13:14:20,574] {dagbag.py:403} INFO - Filling up the DagBag from /home/cato_service/airflow/dags\nTraceback (most recent call last):\n  File \"/usr/local/bin/airflow\", line 37, in &lt;module&gt;\n    args.func(args)\n  File \"/usr/local/lib/python3.6/dist-packages/airflow/utils/cli.py\", line 75, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/airflow/bin/cli.py\", line 900, in webserver\n    app = cached_app_rbac(None) if settings.RBAC else cached_app(None)\n  File \"/usr/local/lib/python3.6/dist-packages/airflow/www/app.py\", line 233, in cached_app\n    app = create_app(config, testing)\n  File \"/usr/local/lib/python3.6/dist-packages/airflow/www/app.py\", line 103, in create_app\n    models.Chart, Session, name=\"Charts\", category=\"Data Profiling\"))\n  File \"/usr/local/lib/python3.6/dist-packages/flask_admin/contrib/sqla/view.py\", line 330, in __init__\n    menu_icon_value=menu_icon_value)\n  File \"/usr/local/lib/python3.6/dist-packages/flask_admin/model/base.py\", line 818, in __init__\n    self._refresh_cache()\n  File \"/usr/local/lib/python3.6/dist-packages/flask_admin/model/base.py\", line 913, in _refresh_cache\n    self._search_supported = self.init_search()\n  File \"/usr/local/lib/python3.6/dist-packages/flask_admin/contrib/sqla/view.py\", line 581, in init_search\n    if tools.is_hybrid_property(self.model, name):\n  File \"/usr/local/lib/python3.6/dist-packages/flask_admin/contrib/sqla/tools.py\", line 209, in is_hybrid_property\n    return last_name in get_hybrid_properties(last_model)\n  File \"/usr/local/lib/python3.6/dist-packages/flask_admin/contrib/sqla/tools.py\", line 190, in get_hybrid_properties\n    for key, prop in inspect(model).all_orm_descriptors.items()\n  File \"/usr/local/lib/python3.6/dist-packages/sqlalchemy/inspection.py\", line 72, in inspect\n    \"available for object of type %s\" % type_\nsqlalchemy.exc.NoInspectionAvailable: No inspection system is available for object of type &lt;class 'method'&gt;\n</code></pre>\n"}, {"tags": ["airflow", "gunicorn", "psycopg2"], "owner": {"account_id": 46053, "reputation": 779, "user_id": 135949, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/1f3aebdbe2d380dadd4df616dfa5ef04?s=256&d=identicon&r=PG", "display_name": "Brett", "link": "https://stackoverflow.com/users/135949/brett"}, "is_answered": false, "view_count": 9490, "answer_count": 0, "score": 37, "last_activity_date": 1672876580, "creation_date": 1532956861, "last_edit_date": 1672876580, "question_id": 51594947, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51594947/psycopg2-could-not-translate-host-name", "title": "psycopg2 could not translate host name", "body": "<p>My airflow server periodically fails. When I check the gunicorn logs, the error before all works shutting down looks like this:</p>\n<pre><code>OperationalError: (psycopg2.OperationalError) could not translate host name &quot;my-airflow-db.l9zijaslosu.us-east-1.rds.amazonaws.com&quot; to address: Name or service not known\n (Background on this error at: http://sqlalche.me/e/e3q8)\n</code></pre>\n<p>I immediately verify that the host name is correct and the database is accepting requests from other tools.</p>\n<p>If I restart the Ariflow webserver, the the server operates correctly for 4-5 days, and then the same error occurs.</p>\n<p>This issue has been asked before but is typically resolve by telling other developers to not use localhost or postrgres host names. My host name is a fully qualified host name on AWS's domain. It seems exceedingly unlikely that this is a DNS error on Amazon's part.</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 401250, "reputation": 1153, "user_id": 767698, "user_type": "registered", "accept_rate": 63, "profile_image": "https://www.gravatar.com/avatar/dda2bf7cd1041ed6845d132c5c7c5867?s=256&d=identicon&r=PG", "display_name": "devj", "link": "https://stackoverflow.com/users/767698/devj"}, "is_answered": true, "view_count": 82802, "accepted_answer_id": 53606745, "answer_count": 4, "score": 36, "last_activity_date": 1641392398, "creation_date": 1493280748, "last_edit_date": 1493280902, "question_id": 43652192, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/43652192/accessing-configuration-parameters-passed-to-airflow-through-cli", "title": "Accessing configuration parameters passed to Airflow through CLI", "body": "<p>I am trying to pass the following configuration parameters to Airflow CLI while triggering a dag run. Following is the  trigger_dag command I am using.</p>\n\n<pre><code>airflow trigger_dag  -c '{\"account_list\":\"[1,2,3,4,5]\", \"start_date\":\"2016-04-25\"}'  insights_assembly_9900 \n</code></pre>\n\n<p>My problem is that how can I access the con parameters passed inside an operator in the dag run.</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 1098374, "reputation": 2391, "user_id": 1091463, "user_type": "registered", "accept_rate": 68, "profile_image": "https://i.sstatic.net/EuSAO.png?s=256", "display_name": "jiminssy", "link": "https://stackoverflow.com/users/1091463/jiminssy"}, "is_answered": true, "view_count": 70788, "protected_date": 1641228068, "accepted_answer_id": 63175235, "answer_count": 6, "score": 36, "last_activity_date": 1653991907, "creation_date": 1548178376, "last_edit_date": 1548180501, "question_id": 54313601, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/54313601/airflow-signals-sigterm-to-subprocesses-unexpectedly", "title": "Airflow signals SIGTERM to subprocesses unexpectedly", "body": "<p>I am using the PythonOperator to call a function that parallelizes data engineering process as an Airflow task. This is done simply by wrapping a simple function with a callable wrapper function called by Airflow.</p>\n\n<pre><code>def wrapper(ds, **kwargs):\n    process_data()\n</code></pre>\n\n<p>process_data achieves parallelization using the multiprocessing module that spawns subprocesses. When I run process_data all by itself from jupyter notebook, it runs to the end with no problem. However when I run it using Airflow, the task fails and the log for the task shows something like this.</p>\n\n<pre><code>[2019-01-22 17:16:46,966] {models.py:1610} ERROR - Received SIGTERM. Terminating subprocesses.\n[2019-01-22 17:16:46,969] {logging_mixin.py:95} WARNING - Process ForkPoolWorker-129:\n\n[2019-01-22 17:16:46,973] {logging_mixin.py:95} WARNING - Traceback (most recent call last):\n\n[2019-01-22 17:16:46,973] {logging_mixin.py:95} WARNING -   File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n    self.run()\n\n[2019-01-22 17:16:46,973] {logging_mixin.py:95} WARNING -   File \"/home/airflow/.env/lib/python3.5/site-packages/airflow/models.py\", line 1612, in signal_handler\n    raise AirflowException(\"Task received SIGTERM signal\")\n\n[2019-01-22 17:16:46,973] {logging_mixin.py:95} WARNING - airflow.exceptions.AirflowException: Task received SIGTERM signal\n\n[2019-01-22 17:16:46,993] {models.py:1610} ERROR - Received SIGTERM. Terminating subprocesses.\n[2019-01-22 17:16:46,996] {logging_mixin.py:95} WARNING - Process ForkPoolWorker-133:\n\n[2019-01-22 17:16:46,999] {logging_mixin.py:95} WARNING - Traceback (most recent call last):\n\n[2019-01-22 17:16:46,999] {logging_mixin.py:95} WARNING -   File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n    self.run()\n\n[2019-01-22 17:16:46,999] {logging_mixin.py:95} WARNING -   File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n\n[2019-01-22 17:16:46,999] {logging_mixin.py:95} WARNING -   File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n    task = get()\n\n[2019-01-22 17:16:46,999] {logging_mixin.py:95} WARNING -   File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n    res = self._reader.recv_bytes()\n\n[2019-01-22 17:16:46,999] {logging_mixin.py:95} WARNING -   File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 99, in __exit__\n    return self._semlock.__exit__(*args)\n\n[2019-01-22 17:16:46,999] {logging_mixin.py:95} WARNING -   File \"/home/airflow/.env/lib/python3.5/site-packages/airflow/models.py\", line 1612, in signal_handler\n    raise AirflowException(\"Task received SIGTERM signal\")\n\n[2019-01-22 17:16:46,999] {logging_mixin.py:95} WARNING - airflow.exceptions.AirflowException: Task received SIGTERM signal\n\n[2019-01-22 17:16:47,086] {logging_mixin.py:95} INFO - file parsing and processing 256.07\n\n[2019-01-22 17:17:12,938] {logging_mixin.py:95} INFO - combining and sorting 25.85\n</code></pre>\n\n<p>I am not quite sure why the task receives SIGTERM. My guess is that some higher level process is sending those to the subprocesses. What should I do to debug this issue?</p>\n\n<p>Just noticed that towards the end of the log for the task, it clearly states that </p>\n\n<pre><code>airflow.exceptions.AirflowException: Task received SIGTERM signal\n[2019-01-22 12:31:39,196] {models.py:1764} INFO - Marking task as FAILED.\n</code></pre>\n"}, {"tags": ["airflow"], "owner": {"account_id": 3292987, "reputation": 11665, "user_id": 2770850, "user_type": "registered", "accept_rate": 85, "profile_image": "https://i.sstatic.net/fwn9J.jpg?s=256", "display_name": "Nabin", "link": "https://stackoverflow.com/users/2770850/nabin"}, "is_answered": true, "view_count": 29898, "accepted_answer_id": 57486645, "answer_count": 3, "score": 35, "last_activity_date": 1708313949, "creation_date": 1565711024, "last_edit_date": 1707908247, "question_id": 57481237, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57481237/use-case-of-dummy-operator-in-apache-airflow", "title": "Use case of dummy operator in Apache Airflow", "body": "<p>I was learning Apache Airflow and found that there is an operator called <code>DummyOperator</code>. I googled about its use case, but couldn't find anything that I can understand. Can anyone here please discuss its use case along with some example code?</p>\n"}, {"tags": ["airflow", "airflow-scheduler"], "owner": {"account_id": 2997585, "reputation": 725, "user_id": 2543803, "user_type": "registered", "accept_rate": 41, "profile_image": "https://i.sstatic.net/PPfvI.jpg?s=256", "display_name": "Norio Akagi", "link": "https://stackoverflow.com/users/2543803/norio-akagi"}, "is_answered": true, "view_count": 97252, "answer_count": 8, "score": 35, "last_activity_date": 1718524198, "creation_date": 1503549206, "question_id": 45853013, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running", "title": "Airflow tasks get stuck at &quot;queued&quot; status and never gets running", "body": "<p>I'm using Airflow v1.8.1 and run all components (worker, web, flower, scheduler) on kubernetes &amp; Docker.\nI use Celery Executor with Redis and my tasks are looks like:</p>\n\n<pre><code>(start) -&gt; (do_work_for_product1)\n     \u251c  -&gt; (do_work_for_product2)\n     \u251c  -&gt; (do_work_for_product3)\n     \u251c  \u2026\n</code></pre>\n\n<p>So the <code>start</code> task has multiple downstreams. \nAnd I setup concurrency related configuration as below:</p>\n\n<pre><code>parallelism = 3\ndag_concurrency = 3\nmax_active_runs = 1\n</code></pre>\n\n<p>Then when I run this DAG manually (not sure if it never happens on a scheduled task) , some downstreams get executed, but others stuck at \"queued\" status.</p>\n\n<p>If I clear the task from Admin UI, it gets executed.\nThere is no worker log (after processing some first downstreams, it just doesn't output any log). </p>\n\n<p>Web server's log (not sure <code>worker exiting</code> is related)</p>\n\n<pre><code>/usr/local/lib/python2.7/dist-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead.\n  .format(x=modname), ExtDeprecationWarning\n[2017-08-24 04:20:56,496] [51] {models.py:168} INFO - Filling up the DagBag from /usr/local/airflow_dags\n[2017-08-24 04:20:57 +0000] [27] [INFO] Handling signal: ttou\n[2017-08-24 04:20:57 +0000] [37] [INFO] Worker exiting (pid: 37)\n</code></pre>\n\n<p>There is no error log on scheduler, too. And a number of tasks get stuck is changing whenever I try this.</p>\n\n<p>Because I also use Docker I'm wondering if this is related:\n<a href=\"https://github.com/puckel/docker-airflow/issues/94\" rel=\"noreferrer\">https://github.com/puckel/docker-airflow/issues/94</a>\nBut so far, no clue.</p>\n\n<p>Has anyone faced with a similar issue or have some idea what I can investigate for this issue...?</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 5533657, "reputation": 6716, "user_id": 4392784, "user_type": "registered", "accept_rate": 39, "profile_image": "https://lh4.googleusercontent.com/-EG88fsSohpM/AAAAAAAAAAI/AAAAAAAAAGA/CR3igr5KRvw/photo.jpg?sz=256", "display_name": "Jeremy Lewi", "link": "https://stackoverflow.com/users/4392784/jeremy-lewi"}, "is_answered": true, "view_count": 69877, "answer_count": 14, "score": 34, "last_activity_date": 1700472300, "creation_date": 1502039409, "question_id": 45534535, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/45534535/airflow-not-loading-dags-in-usr-local-airflow-dags", "title": "Airflow not loading dags in /usr/local/airflow/dags", "body": "<p>Airflow seems to be skipping the dags I added to /usr/local/airflow/dags.</p>\n\n<p>When I run</p>\n\n<pre><code>airflow list_dags\n</code></pre>\n\n<p>The output shows</p>\n\n<pre><code>[2017-08-06 17:03:47,220] {models.py:168} INFO - Filling up the DagBag from /usr/local/airflow/dags\n\n\n-------------------------------------------------------------------\nDAGS\n-------------------------------------------------------------------\nexample_bash_operator\nexample_branch_dop_operator_v3\nexample_branch_operator\nexample_http_operator\nexample_passing_params_via_test_command\nexample_python_operator\nexample_short_circuit_operator\nexample_skip_dag\nexample_subdag_operator\nexample_subdag_operator.section-1\nexample_subdag_operator.section-2\nexample_trigger_controller_dag\nexample_trigger_target_dag\nexample_xcom\nlatest_only\nlatest_only_with_trigger\ntest_utils\ntutorial\n</code></pre>\n\n<p>But this doesn't include the dags in /usr/local/airflow/dags</p>\n\n<pre><code>ls -la /usr/local/airflow/dags/\ntotal 20\ndrwxr-xr-x 3 airflow airflow 4096 Aug  6 17:08 .\ndrwxr-xr-x 4 airflow airflow 4096 Aug  6 16:57 ..\n-rw-r--r-- 1 airflow airflow 1645 Aug  6 17:03 custom_example_bash_operator.py\ndrwxr-xr-x 2 airflow airflow 4096 Aug  6 17:08 __pycache__\n</code></pre>\n\n<p>Is there some other condition that neededs to be satisfied for airflow to identify a DAG and load it?</p>\n"}, {"tags": ["python", "apache-spark", "scheduling", "reload", "airflow"], "owner": {"account_id": 9701958, "reputation": 1007, "user_id": 7196609, "user_type": "registered", "accept_rate": 0, "profile_image": "https://i.sstatic.net/YpyVA.jpg?s=256", "display_name": "Abhishek Pansotra", "link": "https://stackoverflow.com/users/7196609/abhishek-pansotra"}, "is_answered": true, "view_count": 95574, "answer_count": 4, "score": 34, "last_activity_date": 1625583057, "creation_date": 1484777178, "last_edit_date": 1592644375, "question_id": 41730297, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/41730297/python-script-scheduling-in-airflow", "title": "Python script scheduling in airflow", "body": "<p><em>Hi everyone,</em></p>\n<p>I need to schedule my python <strong>files(which contains data extraction from sql and some joins)</strong> using airflow. I have successfully installed airflow into my linux server and webserver of airflow is available with me. But even after going through documentation I am not clear <em><strong>where exactly I need to write script for scheduling and how will that script be available into airflow webserver so I could see the status</strong></em></p>\n<p>As far as the configuration is concerned I know where the dag folder is located in my home directory and also where example dags are located.</p>\n<p><strong>Note:</strong> Please dont mark this as duplicate with How to run bash script file in Airflow as I need to run python files lying in some different location.</p>\n<h2>Please find the configuration in Airflow webserver as :</h2>\n<p><a href=\"https://i.sstatic.net/tExH0.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/tExH0.png\" alt=\"enter image description here\" /></a></p>\n<h2>Below is the screenshot of dag folder in AIRFLOW_HOME dir</h2>\n<p><a href=\"https://i.sstatic.net/FqLDH.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/FqLDH.png\" alt=\"enter image description here\" /></a></p>\n<h2>Also find the below screenshot for DAG creation screenshot and Missing DAG error</h2>\n<p><a href=\"https://i.sstatic.net/aYbd5.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/aYbd5.png\" alt=\"enter image description here\" /></a></p>\n<p><a href=\"https://i.sstatic.net/HLMGd.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/HLMGd.png\" alt=\"enter image description here\" /></a></p>\n<h2>After i select the <strong>simple</strong> DAG following error of missing DAG is populated</h2>\n<p><a href=\"https://i.sstatic.net/gKQGW.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/gKQGW.png\" alt=\"enter image description here\" /></a></p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 995724, "reputation": 45561, "user_id": 1011724, "user_type": "registered", "accept_rate": 99, "profile_image": "https://i.sstatic.net/2JAXi.jpg?s=256", "display_name": "Dan", "link": "https://stackoverflow.com/users/1011724/dan"}, "is_answered": true, "view_count": 47248, "accepted_answer_id": 51527638, "answer_count": 3, "score": 34, "last_activity_date": 1655987571, "creation_date": 1532538100, "question_id": 51524244, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51524244/where-do-you-view-the-output-from-airflow-jobs", "title": "Where do you view the output from airflow jobs", "body": "<p>In the airflow tutorial, the <code>BashOperator</code>s have output (via <code>echo</code>). If the task runs in the scheduler, where do you view the output? Is there a console or something? I'm sure I'm just not looking in the right place.</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 1182634, "reputation": 11913, "user_id": 1157237, "user_type": "registered", "accept_rate": 71, "profile_image": "https://www.gravatar.com/avatar/f2f62795a806ed2f35d96af1ba9a539e?s=256&d=identicon&r=PG", "display_name": "gcbenison", "link": "https://stackoverflow.com/users/1157237/gcbenison"}, "is_answered": true, "view_count": 39800, "answer_count": 4, "score": 33, "last_activity_date": 1686723815, "creation_date": 1492641508, "question_id": 43507252, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/43507252/example-dag-gets-stuck-in-running-state-indefinitely", "title": "Example DAG gets stuck in &quot;running&quot; state indefinitely", "body": "<p>In my first foray into airflow, I am trying to run one of the example DAGS that comes with the installation.  This is v.1.8.0.  Here are my steps:</p>\n\n<pre><code>$ airflow trigger_dag example_bash_operator\n[2017-04-19 15:32:38,391] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-04-19 15:32:38,676] {models.py:167} INFO - Filling up the DagBag from /Users/gbenison/software/kludge/airflow/dags\n[2017-04-19 15:32:38,947] {cli.py:185} INFO - Created &lt;DagRun example_bash_operator @ 2017-04-19 15:32:38: manual__2017-04-19T15:32:38, externally triggered: True&gt;\n$ airflow dag_state example_bash_operator '2017-04-19 15:32:38'\n[2017-04-19 15:33:12,918] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-04-19 15:33:13,229] {models.py:167} INFO - Filling up the DagBag from /Users/gbenison/software/kludge/airflow/dags\nrunning\n</code></pre>\n\n<p>The dag state remains \"running\" for a long time (at least 20 minutes by now), although from a quick inspection of this task it should take a matter of seconds.  How can I troubleshoot this?  How can I see which step it is stuck on?</p>\n"}, {"tags": ["debian", "google-compute-engine", "airflow"], "owner": {"account_id": 369948, "reputation": 5124, "user_id": 717600, "user_type": "registered", "accept_rate": 32, "profile_image": "https://i.sstatic.net/yYVqR.jpg?s=256", "display_name": "Md Sirajus Salayhin", "link": "https://stackoverflow.com/users/717600/md-sirajus-salayhin"}, "is_answered": true, "view_count": 49926, "accepted_answer_id": 51209922, "answer_count": 4, "score": 33, "last_activity_date": 1606800122, "creation_date": 1530444348, "last_edit_date": 1596233610, "question_id": 51122849, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51122849/i-am-getting-bash-airflow-command-not-found", "title": "I am getting &quot;bash: airflow: command not found&quot;", "body": "<p>I am getting</p>\n<blockquote>\n<p>-bash: airflow: command not found</p>\n</blockquote>\n<p>after installing <a href=\"https://en.wikipedia.org/wiki/Apache_Airflow\" rel=\"noreferrer\">Apache Airflow</a>. I am using Google Cloud Compute Engine and OS is Debian\u00a09 (Stretch).</p>\n<p>I have followed the below steps:</p>\n<pre><code>export AIRFLOW_HOME=~/airflow\n\npip install apache-airflow\n</code></pre>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 2528379, "reputation": 2320, "user_id": 2197234, "user_type": "registered", "accept_rate": 84, "profile_image": "https://i.sstatic.net/2aTcP.jpg?s=256", "display_name": "fildred13", "link": "https://stackoverflow.com/users/2197234/fildred13"}, "is_answered": true, "view_count": 51559, "accepted_answer_id": 54100781, "answer_count": 5, "score": 33, "last_activity_date": 1707287686, "creation_date": 1514408135, "question_id": 47998552, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/47998552/apache-airflow-dag-cannot-import-local-module", "title": "Apache Airflow DAG cannot import local module", "body": "<p>I do not seem to understand how to import modules into an apache airflow DAG definition file. I would want to do this to be able to create a library which makes declaring tasks with similar settings less verbose, for instance.</p>\n\n<p>Here is the simplest example I can think of that replicates the issue: I modified the airflow tutorial (<a href=\"https://airflow.apache.org/tutorial.html#recap\" rel=\"noreferrer\">https://airflow.apache.org/tutorial.html#recap</a>) to simply import a module and run a definition from that module. Like so:</p>\n\n<p>Directory structure:</p>\n\n<pre><code>- dags/\n-- __init__.py\n-- lib.py\n-- tutorial.py\n</code></pre>\n\n<p>tutorial.py:</p>\n\n<pre><code>\"\"\"\nCode that goes along with the Airflow located at:\nhttp://airflow.readthedocs.org/en/latest/tutorial.html\n\"\"\"\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\n\n# Here is my added import\nfrom lib import print_double\n\n# And my usage of the imported def\nprint_double(2)\n\n## -- snip, because this is just the tutorial code, \n## i.e., some standard DAG defintion stuff --\n</code></pre>\n\n<p><code>print_double</code> is just a simple def which multiplies whatever input you give it by 2, and prints the result, but obviously that doesn't even matter because this is an import issue.</p>\n\n<p>I am able to run <code>airflow test tutorial print_date 2015-06-01</code> as per the tutorial docs successfully - the dag runs, and moreover the print_double succeeds. <code>4</code> is printed to the console, as expected. All appears well.</p>\n\n<p>Then I go the web UI, and am greeted by <code>Broken DAG: [/home/airflow/airflow/dags/tutorial.py] No module named 'lib'</code>. Unpausing the dag and attempting a manual run using the UI causes a \"running\" status, but it never succeeds or fails. It just sits on \"running\" forever. I can queue up as many as I'd like, but they'll all just sit on \"running\" status.</p>\n\n<p>I've checked the airflow logs, and don't see any useful debug information there.</p>\n\n<p>So what am I missing?</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 9261719, "reputation": 431, "user_id": 6878567, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f4806a8c0b8f9abc154124c8f2dcae73?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "nervokid", "link": "https://stackoverflow.com/users/6878567/nervokid"}, "is_answered": true, "view_count": 17303, "answer_count": 4, "score": 33, "last_activity_date": 1695301711, "creation_date": 1534162129, "question_id": 51822029, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51822029/get-exception-details-on-airflow-on-failure-callback-context", "title": "Get Exception details on Airflow on_failure_callback context", "body": "<p>Is there any way to get the exception details on the airflow <code>on_failure_callback</code>?</p>\n\n<p>I've noticed it's not part of <code>context</code>. I'd like to create a generic exception handling mechanism which posts to Slack information about the errors, including details about the exception. I've now managed to trigger/execute the callback and post to Slack, but can't post the exception details.</p>\n\n<p>Thanks.</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 1915111, "reputation": 10046, "user_id": 1727828, "user_type": "registered", "accept_rate": 89, "profile_image": "https://i.sstatic.net/iZ4RR.png?s=256", "display_name": "mxxk", "link": "https://stackoverflow.com/users/1727828/mxxk"}, "is_answered": true, "view_count": 28580, "accepted_answer_id": 47554601, "answer_count": 4, "score": 32, "last_activity_date": 1636513763, "creation_date": 1498865108, "last_edit_date": 1498887744, "question_id": 44855949, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/44855949/make-custom-airflow-macros-expand-other-macros", "title": "Make custom Airflow macros expand other macros", "body": "<p>Is there any way to make a user-defined macro in Airflow which is itself computed from other macros?</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\n\ndag = DAG(\n    'simple',\n    schedule_interval='0 21 * * *',\n    user_defined_macros={\n        'next_execution_date': '{{ dag.following_schedule(execution_date) }}',\n    },\n)\n\ntask = BashOperator(\n    task_id='bash_op',\n    bash_command='echo \"{{ next_execution_date }}\"',\n    dag=dag,\n)\n</code></pre>\n\n<p>The use case here is to back-port the new Airflow v1.8 <code>next_execution_date</code> macro to work in Airflow v1.7. Unfortunately, this template is rendered without macro expansion:</p>\n\n<pre><code>$ airflow render simple bash_op 2017-08-09 21:00:00\n    # ----------------------------------------------------------\n    # property: bash_command\n    # ----------------------------------------------------------\n    echo \"{{ dag.following_schedule(execution_date) }}\"\n</code></pre>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 7698018, "reputation": 704, "user_id": 5831744, "user_type": "registered", "accept_rate": 67, "profile_image": "https://i.sstatic.net/a84Af.jpg?s=256", "display_name": "Pravin Umamaheswaran", "link": "https://stackoverflow.com/users/5831744/pravin-umamaheswaran"}, "is_answered": true, "view_count": 34026, "accepted_answer_id": 42512686, "answer_count": 7, "score": 32, "last_activity_date": 1638906617, "creation_date": 1487803646, "question_id": 42403755, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/42403755/running-job-on-airflow-based-on-webrequest", "title": "Running Job On Airflow Based On Webrequest", "body": "<p>I wanted to know if airflow tasks can be executed upon getting a request over HTTP. I am not interested in the scheduling part of Airflow. I just want to use it as a substitute for Celery.</p>\n\n<p>So an example operation would be something like this.</p>\n\n<ol>\n<li>User submits a form requesting for some report.</li>\n<li>Backend receives the request and sends the user a notification that the request has been received.</li>\n<li>The backend then schedules a job using Airflow to run immediately.</li>\n<li>Airflow then executes a series of tasks associated with a DAG. For example, pull data from redshift first, pull data from MySQL, make some operations on the two result sets, combine them and then upload the results to Amazon S3, send an email.</li>\n</ol>\n\n<p>From whatever I read online, you can run airflow jobs by executing <code>airflow ...</code> on the command line. I was wondering if there is a python api which can execute the same thing.</p>\n\n<p>Thanks.</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 8166845, "reputation": 558, "user_id": 6442810, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-SJl_YXy9MT4/AAAAAAAAAAI/AAAAAAAASA4/-CYNDkz8eNQ/photo.jpg?sz=256", "display_name": "user6442810", "link": "https://stackoverflow.com/users/6442810/user6442810"}, "is_answered": true, "view_count": 14446, "answer_count": 4, "score": 32, "last_activity_date": 1697362622, "creation_date": 1465426576, "last_edit_date": 1533896050, "question_id": 37714261, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/37714261/what-does-the-landing-time-mean-in-airflow", "title": "What does the landing time mean in airflow?", "body": "<p>There is a section called \"landing time\" in the DAG view on the web console of airflow.</p>\n\n<p>An example screen shot taken from airbnb's blog:</p>\n\n<p><a href=\"https://i.sstatic.net/BCcqp.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/BCcqp.png\" alt=\"\"></a></p>\n\n<p>But what does it mean? There is no definition in the documents or in their repository.</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 8492367, "reputation": 3310, "user_id": 6368214, "user_type": "registered", "accept_rate": 31, "profile_image": "https://www.gravatar.com/avatar/7eaffa83913fadd4b0c556b323d3ea05?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "tensor", "link": "https://stackoverflow.com/users/6368214/tensor"}, "is_answered": true, "view_count": 37155, "accepted_answer_id": 41388139, "answer_count": 4, "score": 31, "last_activity_date": 1724325420, "creation_date": 1474443606, "last_edit_date": 1555968759, "question_id": 39610313, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/39610313/how-to-use-airflow-to-run-a-folder-of-python-files", "title": "How to use AirFlow to run a folder of python files?", "body": "<p>I have a series of Python tasks inside a folder of python files: file1.py, file2.py, ...</p>\n\n<p>I read the Airflow docs, but I don't see how to specify the folder and filename of the python files in the DAG?</p>\n\n<p>I would like to execute those python files (not the Python function through Python Operator).</p>\n\n<p>Task1:  Execute file1.py   (with some import package)</p>\n\n<p>Task2:  Execute file2.py   (with some other import package)</p>\n\n<p>It would be helpful. Thanks, regards</p>\n"}, {"tags": ["python", "sqlalchemy", "pip", "airflow"], "owner": {"account_id": 175403, "reputation": 1419, "user_id": 405396, "user_type": "registered", "accept_rate": 62, "profile_image": "https://www.gravatar.com/avatar/7d888ae535b951c29b41ca302635edda?s=256&d=identicon&r=PG", "display_name": "VKarthik", "link": "https://stackoverflow.com/users/405396/vkarthik"}, "is_answered": true, "view_count": 23547, "accepted_answer_id": 57515985, "answer_count": 1, "score": 31, "last_activity_date": 1612651099, "creation_date": 1565898819, "last_edit_date": 1565938194, "question_id": 57515434, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57515434/why-do-i-get-no-such-table-error-when-installing-apache-airflow-on-mac", "title": "Why do I get no such table error when installing Apache Airflow on Mac?", "body": "<p>It was so hard to put that right title. Ok, here it goes. I was following this tutorial to install Apache Airflow on my Mac (Mojave version) -</p>\n\n<p><a href=\"https://towardsdatascience.com/getting-started-with-apache-airflow-df1aa77d7b1b\" rel=\"noreferrer\">https://towardsdatascience.com/getting-started-with-apache-airflow-df1aa77d7b1b</a></p>\n\n<p>Right at the first step after performing the pip install airflow task, when I run the airflow version command I am getting the following error and then the airflow version appears -</p>\n\n<blockquote>\n  <p>ERROR - Failed on pre-execution callback using  Traceback (most recent call last): \n  File\n  \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py\",\n  line 1244, in _execute_context\n      cursor, statement, parameters, context   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/default.py\",\n  line 552, in do_execute\n      cursor.execute(statement, parameters) sqlite3.OperationalError: no such table: log</p>\n</blockquote>\n\n<p>The above exception was the direct cause of the following exception:</p>\n\n<blockquote>\n  <p>Traceback (most recent call last):   File\n  \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/airflow/utils/cli_action_loggers.py\",\n  line 68, in on_pre_execution\n      cb(**kwargs)   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/airflow/utils/cli_action_loggers.py\",\n  line 99, in default_action_log\n      session.add(log)   File \"/Users/karthikv/anaconda3/lib/python3.7/contextlib.py\", line 119, in\n  <strong>exit</strong>\n      next(self.gen)   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/airflow/utils/db.py\",\n  line 45, in create_session\n      session.commit()   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py\",\n  line 1026, in commit\n      self.transaction.commit()   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py\",\n  line 493, in commit\n      self._prepare_impl()   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py\",\n  line 472, in _prepare_impl\n      self.session.flush()   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py\",\n  line 2451, in flush\n      self._flush(objects)   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py\",\n  line 2589, in _flush\n      transaction.rollback(_capture_exception=True)   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py\",\n  line 68, in <strong>exit</strong>\n      compat.reraise(exc_type, exc_value, exc_tb)   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/util/compat.py\",\n  line 129, in reraise\n      raise value   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py\",\n  line 2549, in _flush\n      flush_context.execute()   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py\",\n  line 422, in execute\n      rec.execute(self)   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py\",\n  line 589, in execute\n      uow,   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py\",\n  line 245, in save_obj\n      insert,   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py\",\n  line 1120, in _emit_insert_statements\n      statement, params   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py\",\n  line 988, in execute\n      return meth(self, multiparams, params)   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\",\n  line 287, in _execute_on_connection\n      return connection._execute_clauseelement(self, multiparams, params)   File\n  \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py\",\n  line 1107, in _execute_clauseelement\n      distilled_params,   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py\",\n  line 1248, in _execute_context\n      e, statement, parameters, cursor, context   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py\",\n  line 1466, in _handle_dbapi_exception\n      util.raise_from_cause(sqlalchemy_exception, exc_info)   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/util/compat.py\",\n  line 383, in raise_from_cause\n      reraise(type(exception), exception, tb=exc_tb, cause=cause)   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/util/compat.py\",\n  line 128, in reraise\n      raise value.with_traceback(tb)   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py\",\n  line 1244, in _execute_context\n      cursor, statement, parameters, context   File \"/Users/karthikv/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/default.py\",\n  line 552, in do_execute\n      cursor.execute(statement, parameters) sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such\n  table: log [SQL: INSERT INTO log (dttm, dag_id, task_id, event,\n  execution_date, owner, extra) VALUES (?, ?, ?, ?, ?, ?, ?)]\n  [parameters: ('2019-08-12 20:50:24.960006', None, None, 'cli_version',\n  None, 'karthikv', '{\"host_name\": \"192-168-1-6.tpgi.com.au\",\n  \"full_command\": \"[\\'/Users/karthikv/anaconda3/bin/airflow\\',\n  \\'version\\']\"}')] </p>\n</blockquote>\n\n<p>(Background on this error at: <a href=\"http://sqlalche.me/e/e3q8\" rel=\"noreferrer\">http://sqlalche.me/e/e3q8</a>)</p>\n\n<p>Can someone help me what this error means and how to solve it? I understand from the instructions that by default SQLLite db gets installed and a single DAG restrictions would be in place before we get into setting up backend database say PostgreSQL.</p>\n\n<p>I tried to uninstall using pip uninstall airflow to perform clean installation again. I get the following error -</p>\n\n<blockquote>\n  <p>WARNING: Skipping airflow as it is not installed.</p>\n</blockquote>\n\n<p>Kindly help me in solving the issue (or) pointing me to resources where I can do further reading.</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 2350931, "reputation": 1313, "user_id": 4289046, "user_type": "registered", "accept_rate": 80, "profile_image": "https://www.gravatar.com/avatar/dc0a2ac262c77e5a60fd2090100578ea?s=256&d=identicon&r=PG", "display_name": "arbazkhan002", "link": "https://stackoverflow.com/users/4289046/arbazkhan002"}, "is_answered": true, "view_count": 51039, "accepted_answer_id": 46615319, "answer_count": 6, "score": 31, "last_activity_date": 1660042975, "creation_date": 1494615264, "question_id": 43944741, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/43944741/debugging-broken-dags", "title": "Debugging Broken DAGs", "body": "<p>When the airflow webserver shows up errors like <code>Broken DAG: [&lt;path/to/dag&gt;] &lt;error&gt;</code>, how and where can we find the full stacktrace for these exceptions?</p>\n\n<p>I tried these locations:</p>\n\n<p><code>/var/log/airflow/webserver</code> -- had no logs in the timeframe of execution, other logs were in binary and decoding with <code>strings</code> gave no useful information.</p>\n\n<p><code>/var/log/airflow/scheduler</code> -- had some logs but were in binary form, tried to read them and looked to be mostly sqlalchemy logs probably for airflow's database.</p>\n\n<p><code>/var/log/airflow/worker</code> -- shows up the logs for running DAGs, (same as the ones you see on the airflow page)</p>\n\n<p>and then also under <code>/var/log/airflow/rotated</code> -- couldn't find the stacktrace I was looking for.</p>\n\n<p>I am using airflow v1.7.1.3</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 448272, "reputation": 3986, "user_id": 842384, "user_type": "registered", "accept_rate": 80, "profile_image": "https://www.gravatar.com/avatar/fba526937889a6d06d1a36af2ac56a1d?s=256&d=identicon&r=PG", "display_name": "samarth", "link": "https://stackoverflow.com/users/842384/samarth"}, "is_answered": true, "view_count": 58649, "accepted_answer_id": 42092728, "answer_count": 4, "score": 30, "last_activity_date": 1646149048, "creation_date": 1484546209, "question_id": 41670256, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/41670256/what-is-the-difference-between-airflow-trigger-rule-all-done-and-all-success", "title": "What is the difference between airflow trigger rule &quot;all_done&quot; and &quot;all_success&quot;?", "body": "<p>One of the requirement in the workflow I am working on is to wait for some event to happen for given time, if it does not happen mark the task as failed still the downstream task should be executed.</p>\n\n<p>I am wondering if \"all_done\" means all the dependency tasks are done no matter if they have succeeded or not.</p>\n"}, {"tags": ["docker", "sed", "deployment", "kubernetes", "airflow"], "owner": {"account_id": 1363265, "reputation": 2004, "user_id": 1300224, "user_type": "registered", "accept_rate": 70, "profile_image": "https://i.sstatic.net/6V7CT.jpg?s=256", "display_name": "spaghettifunk", "link": "https://stackoverflow.com/users/1300224/spaghettifunk"}, "is_answered": true, "view_count": 105342, "accepted_answer_id": 49644474, "answer_count": 2, "score": 30, "last_activity_date": 1532348024, "creation_date": 1522682923, "last_edit_date": 1522683435, "question_id": 49614034, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/49614034/kubernetes-deployment-read-only-filesystem-error", "title": "Kubernetes deployment read-only filesystem error", "body": "<p>I am facing an error while deploying Airflow on Kubernetes (precisely this version of Airflow <a href=\"https://github.com/puckel/docker-airflow/blob/1.8.1/Dockerfile\" rel=\"noreferrer\">https://github.com/puckel/docker-airflow/blob/1.8.1/Dockerfile</a>) regarding writing permissions onto the filesystem.</p>\n\n<p>The error displayed on the logs of the pod is:</p>\n\n<pre><code>sed: couldn't open temporary file /usr/local/airflow/sed18bPUH: Read-only file system\nsed: -e expression #1, char 131: unterminated `s' command\nsed: -e expression #1, char 118: unterminated `s' command\nInitialize database...\nsed: couldn't open temporary file /usr/local/airflow/sedouxZBL: Read-only file system\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/airflow/configuration.py\", line 769, in\n    ....\n    with open(TEST_CONFIG_FILE, 'w') as f:\nIOError: [Errno 30] Read-only file system: '/usr/local/airflow/unittests.cfg'\n</code></pre>\n\n<p>It seems that the filesystem is read-only but I do not understand why it is. I am not sure if it is a <strong>Kubernetes misconfiguration</strong> (do I need a special RBAC for pods ? No idea) or if it is a problem with the <strong>Dockerfile</strong>. </p>\n\n<p>The deployment file looks like the following:</p>\n\n<pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: airflow\n  namespace: test\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  template:\n    metadata:\n      labels:\n        app: airflow\n    spec:\n      restartPolicy: Always\n      containers:\n      - name: webserver\n        image: davideberdin/docker-airflow:0.0.4\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 1\n            memory: 1Gi\n          requests:\n            cpu: 50m\n            memory: 128Mi\n        securityContext:  #does not have any effect\n          runAsUser: 0    #does not have any effect\n        ports:\n        - name: airflow-web\n          containerPort: 8080\n        args: [\"webserver\"]\n        volumeMounts:\n          - name: airflow-config-volume\n            mountPath: /usr/local/airflow\n            readOnly: false #does not have any effect\n          - name: airflow-logs\n            mountPath: /usr/local/logs\n            readOnly: false #does not have any effect\n      volumes:\n      - name: airflow-config-volume\n        secret:\n          secretName: airflow-config-secret\n      - name: airflow-parameters-volume\n        secret:\n          secretName: airflow-parameters-secret\n      - name: airflow-logs\n        emptyDir: {}\n</code></pre>\n\n<p>Any idea how I can make the filesystem writable? The container is running as <strong>USER airflow</strong> but I think that this user has root privileges.</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 2512165, "reputation": 1221, "user_id": 2184373, "user_type": "registered", "accept_rate": 77, "profile_image": "https://www.gravatar.com/avatar/78744f9fdcb1b092e1ed97f454e0ff15?s=256&d=identicon&r=PG", "display_name": "Alessandro Mariani", "link": "https://stackoverflow.com/users/2184373/alessandro-mariani"}, "is_answered": true, "view_count": 38422, "accepted_answer_id": 46091929, "answer_count": 4, "score": 29, "last_activity_date": 1673925151, "creation_date": 1490290077, "question_id": 42982986, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/42982986/external-files-in-airflow-dag", "title": "External files in Airflow DAG", "body": "<p>I'm trying to access external files in a Airflow Task to read some sql, and I'm getting \"file not found\". Has anyone come across this?</p>\n\n<pre><code>from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\n\ndag = DAG(\n    'my_dat',\n    start_date=datetime(2017, 1, 1),\n    catchup=False,\n    schedule_interval=timedelta(days=1)\n)\n\ndef run_query():\n    # read the query\n    query = open('sql/queryfile.sql')\n    # run the query\n    execute(query)\n\ntas = PythonOperator(\n    task_id='run_query', dag=dag, python_callable=run_query)\n</code></pre>\n\n<p>The log state the following:</p>\n\n<pre><code>IOError: [Errno 2] No such file or directory: 'sql/queryfile.sql'\n</code></pre>\n\n<p>I understand that I could simply copy and paste the query inside the same file, it's really not at neat solution. There are multiple queries and the text is really big, embed it with the Python code would compromise readability. </p>\n"}, {"tags": ["python", "python-3.x", "airflow"], "owner": {"account_id": 9288030, "reputation": 792, "user_id": 6897259, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b2ad739d189249c1cce425ab52bfe32c?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "WeiHao", "link": "https://stackoverflow.com/users/6897259/weihao"}, "is_answered": true, "view_count": 64546, "answer_count": 1, "score": 29, "last_activity_date": 1539694067, "creation_date": 1517990219, "last_edit_date": 1539694067, "question_id": 48658594, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/48658594/airflow-depends-on-past-explanation", "title": "Airflow depends_on_past explanation", "body": "<p>According to the <a href=\"https://airflow.apache.org/faq.html\" rel=\"noreferrer\">official Airflow docs</a>, \nThe <code>task instances</code> directly upstream from the <code>task</code> need to be in a success state. Also, if you have set depends_on_past=True, the previous task instance needs to have succeeded (except if it is the first run for that task).</p>\n\n<p>As all know, the task is kind of 'instantiated &amp; parameteriazed' operator.</p>\n\n<p>Now this is what confuse me. For example:</p>\n\n<pre><code>DAG: {op_1} -&gt; {op_2} -&gt; {op_3}\n</code></pre>\n\n<p>{op_2} is a simple PythonOperator that takes 1 parameter from {op_1} and do stuff;</p>\n\n<p>To my understanding, op_2(param_1) &amp; op_2(param_2) are considered as 2 different tasks.</p>\n\n<p>Given <code>depends_on_past</code> is set to True, then:</p>\n\n<ol>\n<li>If op_2(param_1) is still running; can op_2(param_2) be run?</li>\n<li>If op_2(param_1) fails in the previous run; can op_2(param_1) be run in the current run?</li>\n</ol>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 8671438, "reputation": 927, "user_id": 6490241, "user_type": "registered", "accept_rate": 15, "profile_image": "https://www.gravatar.com/avatar/5369c0d971f09f2ff612e4b3e3aceb3e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Teja", "link": "https://stackoverflow.com/users/6490241/teja"}, "is_answered": true, "view_count": 81155, "accepted_answer_id": 50149522, "answer_count": 1, "score": 28, "last_activity_date": 1668100905, "creation_date": 1525331926, "last_edit_date": 1525346909, "question_id": 50149085, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/50149085/python-airflow-return-result-from-pythonoperator", "title": "Python Airflow - Return result from PythonOperator", "body": "<p>I have written a DAG with multiple PythonOperators </p>\n\n<pre><code>task1 = af_op.PythonOperator(task_id='Data_Extraction_Environment',\n                          provide_context=True,\n                          python_callable=Task1, dag=dag1)\n\ndef Task1(**kwargs):\n    return(kwargs['dag_run'].conf.get('file'))\n</code></pre>\n\n<p>From PythonOperator i am calling \"Task1\" method. That method is returning a value,that value i need to pass to the next PythonOperator.How can i get the value from the \"task1\" variable or How can i get the value which is returned from Task1 method?</p>\n\n<p>updated :</p>\n\n<pre><code>    def Task1(**kwargs):\n          file_name = kwargs['dag_run'].conf.get[file]\n          task_instance = kwargs['task_instance']\n          task_instance.xcom_push(key='file', value=file_name) \n          return file_name\n\n  t1 = PythonOperator(task_id = 'Task1',provide_context=True,python_callable=Task1,dag=dag)\n\n  t2 =   BashOperator(\n      task_id='Moving_bucket', \n      bash_command='python /home/raw.py {{ task_instance.xcom_pull(task_ids='Task1',key='file') }} ',\n      dag=dag,\n    )\n\nt2.set_upstream(t1)\n</code></pre>\n"}, {"tags": ["airflow", "airflow-scheduler"], "owner": {"account_id": 282668, "reputation": 1817, "user_id": 579705, "user_type": "registered", "accept_rate": 79, "profile_image": "https://www.gravatar.com/avatar/eebc6af148d70db2b0cb2631d231faf5?s=256&d=identicon&r=PG", "display_name": "Mukul Jain", "link": "https://stackoverflow.com/users/579705/mukul-jain"}, "is_answered": true, "view_count": 35569, "accepted_answer_id": 49019665, "answer_count": 5, "score": 28, "last_activity_date": 1634560341, "creation_date": 1519642282, "question_id": 48986732, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/48986732/airflow-creating-a-dag-in-airflow-via-ui", "title": "Airflow: Creating a DAG in airflow via UI", "body": "<p>Airflow veterans please help,</p>\n\n<p>I was looking for a cron replacement and came across apache airflow.</p>\n\n<p>We have a setup where multiple users should be able to create their own DAGs and schedule their jobs. </p>\n\n<p>Our users are a mix of people who may not know how to write the DAG python file. Also, they may not have access to the server where airflow is running.</p>\n\n<p>Is it possible to create an airflow DAG via UI. I could not find any reference to the same. All examples speak about creating a python file and uploading it to the $AIRFLOW_HOME/dag/ directory. Users will not have access to this directory.</p>\n\n<p>Rundeck for example allows user to add workflows and task dependencies via UI. Is there a plugin/functionality similar to this in airflow.</p>\n\n<p>PS: I really like the way airflow shows the dependency graphs and want to try it out. But If creating a DAG is so complicated, then it will be a major problem for lots of my end users. </p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 1561969, "reputation": 2561, "user_id": 1451653, "user_type": "registered", "accept_rate": 54, "profile_image": "https://www.gravatar.com/avatar/35b633a1a3858414c8a89b666a8d6d44?s=256&d=identicon&r=PG", "display_name": "Chengzhi", "link": "https://stackoverflow.com/users/1451653/chengzhi"}, "is_answered": true, "view_count": 10393, "answer_count": 2, "score": 28, "last_activity_date": 1629757676, "creation_date": 1501510353, "question_id": 45418285, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/45418285/airflow-python-unit-test", "title": "Airflow Python Unit Test?", "body": "<p>I'd like to add some unit tests for our DAGs, but could not find any. Is there a framework for unit test for DAGs? There is an End-to-End testing framework that exists but I guess it's dead: <a href=\"https://issues.apache.org/jira/browse/AIRFLOW-79\" rel=\"noreferrer\">https://issues.apache.org/jira/browse/AIRFLOW-79</a>. Please suggest, Thanks!</p>\n"}, {"tags": ["boto3", "airflow", "airflow-scheduler"], "owner": {"account_id": 4004282, "reputation": 6303, "user_id": 3299397, "user_type": "registered", "accept_rate": 61, "profile_image": "https://i.sstatic.net/HvJkt.jpg?s=256", "display_name": "Kyle Bridenstine", "link": "https://stackoverflow.com/users/3299397/kyle-bridenstine"}, "is_answered": true, "view_count": 33144, "accepted_answer_id": 50592573, "answer_count": 2, "score": 28, "last_activity_date": 1609337343, "creation_date": 1527623282, "last_edit_date": 1527626876, "question_id": 50591886, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/50591886/airflow-s3keysensor-how-to-make-it-continue-running", "title": "Airflow S3KeySensor - How to make it continue running", "body": "<p>With the help of <a href=\"https://stackoverflow.com/a/40774361/3299397\">this Stackoverflow post</a> I just made a program (the one shown in the post) where when a file is placed inside an S3 bucket a task in one of my running DAGs is triggered and then I perform some work using the BashOperator. Once it's done though the DAG is no longer in a running state but instead goes into a success state and if I want to have it pick up another file I need to clear all the 'Past', 'Future', 'Upstream', 'Downstream' activity. I would like to make this program so that it's always running and anytime a new file is placed inside the S3 bucket the program kicks off the tasks.</p>\n\n<p>Can I continue using the S3KeySenor to do this or do I need to figure out a way of setting up an <a href=\"https://airflow.apache.org/scheduler.html\" rel=\"noreferrer\">External Trigger</a> to run my DAG? As of now my S3KeySensor is pretty pointless if it's only going to ever run once.</p>\n\n<pre><code>from airflow import DAG\nfrom airflow.operators import SimpleHttpOperator, HttpSensor, EmailOperator, S3KeySensor\nfrom datetime import datetime, timedelta\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2018, 5, 29),\n    'email': ['something@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 5,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG('s3_triggered_emr_cluster_dag', default_args=default_args, schedule_interval= '@once')\n\n# This Activity runs a Python script that creates an AWS EMR cluster and then does EMR activity on the EMR cluster.\nt2 = BashOperator(\n    task_id='create_emr_cluster_1',\n    bash_command='python /home/ec2-user/aws-python-sample/Create_EMR_Then_Do_EMR_Activities.py',\n    retries=1,\n    dag=dag)\n\nt1 = BashOperator(\n    task_id='success_log',\n    bash_command='echo \"Dag ran successfully\" &gt;&gt; /home/ec2-user/s3_triggered_dag.txt',\n    dag=dag)\n\nsensor = S3KeySensor(\n    task_id='new_s3_file_in_foobar-bucket',\n    bucket_key='*',\n    wildcard_match=True,\n    bucket_name='foobar-bucket',\n    s3_conn_id='s3://foobar-bucket',\n    timeout=18*60*60,\n    poke_interval=120,\n    dag=dag)\n\nt1.set_upstream(sensor)\nt2.set_upstream(t1)\n</code></pre>\n\n<p>I'm wondering if this is not possible because it then wouldn't be a Directed Acyclic Graph but rather it would have a loop that repeated <strong>sensor -> t1 -> t2 -> sensor -> t1 -> t2 -> sensor -> ... keep repeating</strong>.</p>\n\n<p><strong>Update:</strong></p>\n\n<p>My use case is pretty simple, anytime a new file is placed inside a designated AWS S3 Bucket I want my DAG to be triggered and start my process of various tasks. The tasks will do things like instantiate a new AWS EMR Cluster, extract the files from the AWS S3 Bucket, perform some AWS EMR Activities, then shut down the AWS EMR Cluster. From there the DAG would go back into a waiting state where it would wait for new files to arrive in the AWS S3 Bucket and then repeat the process indefinitely.</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 234710, "reputation": 1247, "user_id": 501044, "user_type": "registered", "accept_rate": 50, "profile_image": "https://www.gravatar.com/avatar/966aa8d494c9d9d8bcead88e3a8bd5f4?s=256&d=identicon&r=PG", "display_name": "bsd", "link": "https://stackoverflow.com/users/501044/bsd"}, "is_answered": true, "view_count": 55560, "answer_count": 3, "score": 27, "last_activity_date": 1675259198, "creation_date": 1502786391, "question_id": 45689391, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/45689391/schedule-a-dag-in-airflow-to-run-every-5-minutes", "title": "Schedule a DAG in airflow to run every 5 minutes", "body": "<p>I have a DAG in airflow and for now it is running each hour (@hourly).\nIs it possible to have it running each 5 minutes ?</p>\n"}, {"tags": ["python-3.x", "logging", "airflow", "etl"], "owner": {"account_id": 4339204, "reputation": 557, "user_id": 3542930, "user_type": "registered", "accept_rate": 57, "profile_image": "https://www.gravatar.com/avatar/db114becb7eb2280e8f0c572eeccc85d?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "user3542930", "link": "https://stackoverflow.com/users/3542930/user3542930"}, "is_answered": true, "view_count": 34028, "accepted_answer_id": 41841689, "answer_count": 3, "score": 27, "last_activity_date": 1635599082, "creation_date": 1476839258, "last_edit_date": 1635599082, "question_id": 40120467, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/40120467/adding-logs-to-airflow-logs", "title": "Adding logs to Airflow Logs", "body": "<p>How can I add my own logs onto the Apache Airflow logs that are automatically generated? any print statements wont get logged in there, so I was wondering how I can add my logs so that it shows up on the UI as well?</p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 9112902, "reputation": 398, "user_id": 6780923, "user_type": "registered", "accept_rate": 100, "profile_image": "https://lh4.googleusercontent.com/-sSUCghFaF5Y/AAAAAAAAAAI/AAAAAAAAACs/6nM_Ucw5GPQ/photo.jpg?sz=256", "display_name": "Justin Besteman", "link": "https://stackoverflow.com/users/6780923/justin-besteman"}, "is_answered": true, "view_count": 36776, "accepted_answer_id": 54048104, "answer_count": 2, "score": 27, "last_activity_date": 1645314771, "creation_date": 1546630819, "question_id": 54045048, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/54045048/with-code-how-do-you-update-an-airflow-variable", "title": "With code, how do you update an airflow variable?", "body": "<p>I need to update a variable I have made in Airflow programmatically but I can not find the answer on how to do that with code.</p>\n\n<p>I have retrieved my variable with this code:</p>\n\n<p><code>column_number = Variable.get('column_number')</code></p>\n\n<p>At the end of the function, I would like to increment the column_number by one</p>\n\n<p>I have tried this:\n<code>Variable.set_val(\"column_number\", int(column_number) + 1)</code></p>\n\n<p>And it does not work.</p>\n\n<p>Here is the full code for reference:</p>\n\n<pre><code>import airflow\nfrom datetime import datetime, timedelta\nfrom random import randint\nfrom airflow import DAG\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Variable\nfrom airflow.operators.python_operator import PythonOperator\n\nargs = {\n    'owner': 'besteman',\n    'start_date': datetime.utcnow(),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=30)\n}\n\ndag = DAG(dag_id='test-postgres', default_args=args, schedule_interval='@hourly')\n\ndef add_columns_and_values():\n\n    column_number = Variable.get('column_number')\n\n    pg_hook = PostgresHook(postgres_conn_id='airflow-test')\n\n    add_columns = f'ALTER TABLE students ADD COLUMN test{column_number} smallint;'\n\n    pg_hook.run(add_columns) \n\n    for i in range(8):\n        add_values = f\"UPDATE students SET test{column_number} = '{randint(50, 100)}' WHERE id = {i+1};\"\n        pg_hook.run(add_values)\n\n    Variable.set_val(\"column_number\", int(column_number) + 1)\n\n\nt1 = PythonOperator(task_id='add_columns_values',\n    python_callable=add_columns_and_values,\n    dag=dag)\n</code></pre>\n"}, {"tags": ["airflow", "airflow-scheduler"], "owner": {"account_id": 7392181, "reputation": 460, "user_id": 5625266, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bffe05336b7a95ee6c61b823f26f3dd0?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Prasann", "link": "https://stackoverflow.com/users/5625266/prasann"}, "is_answered": true, "view_count": 29894, "answer_count": 2, "score": 27, "last_activity_date": 1571276073, "creation_date": 1511426087, "last_edit_date": 1532470337, "question_id": 47450855, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/47450855/airflow-scheduler-is-slow-to-schedule-subsequent-tasks", "title": "Airflow scheduler is slow to schedule subsequent tasks", "body": "<p>When I try to run a DAG in Airflow 1.8.0 I find that it takes a lot of time between the time of completion predecessor task and the time at which the successor task is picked up for execution (usually greater the execution times of individual tasks). The same is the scenario for Sequential, Local and Celery Executors. Is there a way to lessen the overhead time mentioned? (like any parameters in airflow.cfg that can speed up the DAG execution?)\nGantt chart has been added for reference:\n<img src=\"https://i.sstatic.net/MCE9d.png\" alt=\"Gantt chart\"></p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 3492357, "reputation": 1363, "user_id": 2921683, "user_type": "registered", "profile_image": "https://graph.facebook.com/696088208/picture?type=large", "display_name": "Yannick Widmer", "link": "https://stackoverflow.com/users/2921683/yannick-widmer"}, "is_answered": true, "view_count": 22139, "accepted_answer_id": 59056612, "answer_count": 2, "score": 27, "last_activity_date": 1574790019, "creation_date": 1524805553, "last_edit_date": 1525218975, "question_id": 50055582, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/50055582/how-to-mark-an-airflow-dag-run-as-failed-if-any-task-fails", "title": "How to mark an Airflow DAG run as failed if any task fails?", "body": "<p>Is it possible to make an Airflow DAG fail if any task fails?</p>\n\n<p>I usually have some cleaning up tasks at the end of a DAG and as it is now, whenever the last task succeeds the whole DAG is marked as a success.</p>\n"}, {"tags": ["python", "python-3.x", "airflow", "directed-acyclic-graphs", "airflow-scheduler"], "owner": {"account_id": 12175174, "reputation": 903, "user_id": 8888469, "user_type": "registered", "accept_rate": 29, "profile_image": "https://www.gravatar.com/avatar/660c07720f13cf8b41506a6d274d9ada?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "aeapen", "link": "https://stackoverflow.com/users/8888469/aeapen"}, "is_answered": true, "view_count": 58259, "accepted_answer_id": 61534071, "answer_count": 3, "score": 26, "last_activity_date": 1673218999, "creation_date": 1588213465, "last_edit_date": 1588242401, "question_id": 61514887, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61514887/how-to-trigger-a-dag-on-the-success-of-a-another-dag-in-airflow-using-python", "title": "How to Trigger a DAG on the success of a another DAG in Airflow using Python?", "body": "<p>I have a python DAG <code>Parent Job</code> and DAG <code>Child Job</code>. The tasks in the <code>Child Job</code> should be triggered on the successful completion of the <code>Parent Job</code> tasks which are run daily. How can add external job trigger ?</p>\n\n<p><strong>MY CODE</strong></p>\n\n<pre><code>from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.postgres_operator import PostgresOperator\nfrom utils import FAILURE_EMAILS\n\nyesterday = datetime.combine(datetime.today() - timedelta(1), datetime.min.time())\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': yesterday,\n    'email': FAILURE_EMAILS,\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG('Child Job', default_args=default_args, schedule_interval='@daily')\n\nexecute_notebook = PostgresOperator(\n  task_id='data_sql',\n  postgres_conn_id='REDSHIFT_CONN',\n  sql=\"SELECT * FROM athena_rs.shipments limit 5\",\n  dag=dag\n)\n</code></pre>\n"}, {"tags": ["google-cloud-dataflow", "airflow", "apache-beam", "google-cloud-composer"], "owner": {"user_type": "does_not_exist", "display_name": "user10503628"}, "is_answered": true, "view_count": 24196, "closed_date": 1682548911, "answer_count": 4, "score": 26, "last_activity_date": 1682512455, "creation_date": 1547245223, "last_edit_date": 1585422833, "question_id": 54154816, "link": "https://stackoverflow.com/questions/54154816/using-dataflow-vs-cloud-composer", "closed_reason": "Opinion-based", "title": "Using Dataflow vs. Cloud Composer", "body": "<p>I'd like to get some clarification on whether Cloud Dataflow or Cloud Composer is the right tool for the job, and I wasn't clear from the Google Documentation.</p>\n\n<p>Currently, I'm using Cloud Dataflow to read a non-standard csv file -- do some basic processing -- and load it into BigQuery.</p>\n\n<p>Let me give a very basic example:</p>\n\n<pre><code># file.csv\ntype\\x01date\nhouse\\x0112/27/1982\ncar\\x0111/9/1889\n</code></pre>\n\n<p>From this file we detect the schema and create a BigQuery table, something like this:</p>\n\n<pre><code>`table`\ntype (STRING)\ndate (DATE)\n</code></pre>\n\n<p>And, we also format our data to insert (in python) into BigQuery:</p>\n\n<pre><code>DATA = [\n    (\"house\", \"1982-12-27\"),\n    (\"car\", \"1889-9-11\")\n]\n</code></pre>\n\n<p>This is a vast simplification of what's going on, but this is how we're currently using Cloud Dataflow.</p>\n\n<p>My question then is, where does <code>Cloud Composer</code> come into the picture? What additional features could it provide on the above? In other words, why would it be used \"on top of\" Cloud Dataflow?</p>\n"}, {"tags": ["celery", "dask", "airflow"], "owner": {"account_id": 106330, "reputation": 95496, "user_id": 283296, "user_type": "registered", "accept_rate": 77, "profile_image": "https://i.sstatic.net/ilsZ4.jpg?s=256", "display_name": "Amelio Vazquez-Reina", "link": "https://stackoverflow.com/users/283296/amelio-vazquez-reina"}, "is_answered": true, "view_count": 19977, "accepted_answer_id": 49319651, "answer_count": 3, "score": 26, "last_activity_date": 1641980789, "creation_date": 1521152263, "question_id": 49310136, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/49310136/airflow-celery-or-dask-for-what-when", "title": "Airflow + celery or dask. For what, when?", "body": "<p>I read in the official Airflow documentation <a href=\"https://airflow.apache.org/configuration.html#scaling-out-with-celery\" rel=\"noreferrer\">the following</a>:</p>\n\n<p><a href=\"https://i.sstatic.net/BeHip.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/BeHip.png\" alt=\"enter image description here\"></a></p>\n\n<p>What does this mean exactly? What do the authors mean by scaling out? That is, <strong>when</strong> is it <strong>not</strong> enough to use Airflow or when would anyone use Airflow in combination with something like Celery? (same for <code>dask</code>)</p>\n"}, {"tags": ["ubuntu", "pip", "airflow"], "owner": {"account_id": 2416583, "reputation": 1297, "user_id": 2110476, "user_type": "registered", "accept_rate": 47, "profile_image": "https://www.gravatar.com/avatar/0fc4e19036802393e13653d7afaa7dc4?s=256&d=identicon&r=PG", "display_name": "Chris", "link": "https://stackoverflow.com/users/2110476/chris"}, "is_answered": true, "view_count": 62316, "protected_date": 1615136352, "answer_count": 12, "score": 26, "last_activity_date": 1624370898, "creation_date": 1462445951, "last_edit_date": 1542904501, "question_id": 37048856, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/37048856/how-to-install-airflow", "title": "How to install airflow?", "body": "<p>I seem to be doing sth. wrong.</p>\n\n<p><a href=\"https://pythonhosted.org/airflow/start.html\" rel=\"noreferrer\">https://pythonhosted.org/airflow/start.html</a></p>\n\n<pre><code>$ export AIRFLOW_HOME=~/airflow\n$ pip install apache-airflow\nRequirement already satisfied\n$ airflow initdb\nairflow: Command not found \n</code></pre>\n\n<blockquote>\n  <p>python --version<br>\n  Python 2.7.10</p>\n</blockquote>\n\n<p>It's weird - the installation seemed to have worked fine (with some warnings - nothing serious) saying: airflow, flask, etc. successfully installed. But even after restarting the PC (Ubuntu 15.10) airflow seems not to be a command.</p>\n"}, {"tags": ["jobs", "oozie", "airflow", "airflow-scheduler"], "owner": {"account_id": 10179797, "reputation": 438, "user_id": 7517051, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d207ee13c4863a5fc4058da80a9d9f99?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Vishal786btc", "link": "https://stackoverflow.com/users/7517051/vishal786btc"}, "is_answered": true, "view_count": 24313, "answer_count": 1, "score": 26, "last_activity_date": 1551978148, "creation_date": 1513873537, "last_edit_date": 1526312936, "question_id": 47928995, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/47928995/which-one-to-choose-apache-oozie-or-apache-airflow-need-a-comparison", "title": "Which one to choose Apache Oozie or Apache Airflow? Need a comparison", "body": "<p>I am new to job schedulers and was looking out for one to run jobs on big data cluster. I was quite confused with the available choices. Found Oozie to have many limitations as compared to the already existing ones such as TWS, Autosys, etc. </p>\n\n<p>Need some comparison points on Oozie vs. Airflow.</p>\n\n<p>Appreciate your help.</p>\n"}, {"tags": ["python", "airflow"], "owner": {"account_id": 8732827, "reputation": 4624, "user_id": 6532319, "user_type": "registered", "accept_rate": 86, "profile_image": "https://www.gravatar.com/avatar/5e3cc0d2204f85bd023806007967165c?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "DougKruger", "link": "https://stackoverflow.com/users/6532319/dougkruger"}, "is_answered": true, "view_count": 62757, "accepted_answer_id": 40147039, "answer_count": 2, "score": 26, "last_activity_date": 1617550966, "creation_date": 1476944821, "last_edit_date": 1491352601, "question_id": 40146934, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/40146934/how-to-run-bash-script-file-in-airflow", "title": "How to run bash script file in Airflow", "body": "<p>I have a bash script that creates a file (if it does not exist) that I want to run in Airflow, but when I try it fails. How do I do this?</p>\n\n<pre><code>#!/bin/bash\n#create_file.sh\n\nfile=filename.txt\n\nif [ ! -e \"$file\" ] ; then\n    touch \"$file\"\nfi\n\nif [ ! -w \"$file\" ] ; then\n    echo cannot write to $file\n    exit 1\nfi\n</code></pre>\n\n<p>and here's how I'm calling it in Airflow:</p>\n\n<pre><code>create_command = \"\"\"\n ./scripts/create_file.sh\n\"\"\"\nt1 = BashOperator(\n        task_id= 'create_file',\n        bash_command=create_command,\n        dag=dag\n)\n\nlib/python2.7/site-packages/airflow/operators/bash_operator.py\", line 83, in execute\n    raise AirflowException(\"Bash command failed\")\nairflow.exceptions.AirflowException: Bash command failed\n</code></pre>\n"}, {"tags": ["python", "jinja2", "airflow"], "owner": {"account_id": 368, "reputation": 45041, "user_id": 459, "user_type": "registered", "accept_rate": 81, "profile_image": "https://www.gravatar.com/avatar/0db193fbf6f0929c49dd9704698204ff?s=256&d=identicon&r=PG", "display_name": "dlamblin", "link": "https://stackoverflow.com/users/459/dlamblin"}, "is_answered": true, "view_count": 37700, "accepted_answer_id": 45887539, "answer_count": 5, "score": 25, "last_activity_date": 1657638080, "creation_date": 1484853623, "last_edit_date": 1484865381, "question_id": 41749974, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/41749974/airflow-using-template-files-for-pythonoperator", "title": "Airflow using template files for PythonOperator", "body": "<p>The method of getting a <code>BashOperator</code> or <code>SqlOperator</code> to pick up an external file for its template is somewhat clearly documented, but looking at the <a href=\"https://github.com/apache/incubator-airflow/blob/851adc5547597ec51743be4bc47d634c77d6dc17/airflow/operators/python_operator.py\" rel=\"noreferrer\"><code>PythonOperator</code></a> my test of what I understand from the docs is not working. I am not sure how the <code>templates_exts</code> and <code>templates_dict</code> parameters would correctly interact to pick up a file.</p>\n\n<p>In my dags folder I've created: <code>pyoptemplate.sql</code> and <code>pyoptemplate.t</code> as well as <code>test_python_operator_template.py</code>:</p>\n\n<h3>pyoptemplate.sql:</h3>\n\n<pre class=\"lang-sql prettyprint-override\"><code>SELECT * FROM {{params.table}};\n</code></pre>\n\n<h3>pyoptemplate.t:</h3>\n\n<pre class=\"lang-sql prettyprint-override\"><code>SELECT * FROM {{params.table}};\n</code></pre>\n\n<h3>test_python_operator_template.py:</h3>\n\n<pre class=\"lang-python prettyprint-override\"><code># coding: utf-8\n# vim:ai:si:et:sw=4 ts=4 tw=80\n\"\"\"\n# A Test of Templates in PythonOperator\n\"\"\"\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\n\nimport pprint\n\npp = pprint.PrettyPrinter(indent=4)\n\n\ndef templated_function(ds, **kwargs):\n    \"\"\"This function will try to use templates loaded from external files\"\"\"\n    pp.pprint(ds)\n    pp.pprint(kwargs)\n\n\n# Define the DAG\ndag = DAG(dag_id='test_python_operator_template_dag',\n          default_args={\"owner\": \"lamblin\",\n                        \"start_date\": datetime.now()},\n          template_searchpath=['/Users/daniellamblin/airflow/dags'],\n          schedule_interval='@once')\n\n\n# Define the single task in this controller example DAG\nop = PythonOperator(task_id='test_python_operator_template',\n                    provide_context=True,\n                    python_callable=templated_function,\n                    templates_dict={\n                        'pyoptemplate': '',\n                        'pyoptemplate.sql': '',\n                        'sql': 'pyoptemplate',\n                        'file1':'pyoptemplate.sql',\n                        'file2':'pyoptemplate.t',\n                        'table': '{{params.table}}'},\n                    templates_exts=['.sql','.t'],\n                    params={'condition_param': True,\n                            'message': 'Hello World',\n                            'table': 'TEMP_TABLE'},\n                    dag=dag)\n</code></pre>\n\n<p>The result from a run shows that <code>table</code> was templated correctly as a string, but the others did not pull in any files for templating.</p>\n\n<pre><code>dlamblin$ airflow test test_python_operator_template_dag test_python_operator_template 2017-01-18\n[2017-01-18 23:58:06,698] {__init__.py:36} INFO - Using executor SequentialExecutor\n[2017-01-18 23:58:07,342] {models.py:154} INFO - Filling up the DagBag from /Users/daniellamblin/airflow/dags\n[2017-01-18 23:58:07,620] {models.py:1196} INFO - \n--------------------------------------------------------------------------------\nStarting attempt 1 of 1\n--------------------------------------------------------------------------------\n\n[2017-01-18 23:58:07,620] {models.py:1219} INFO - Executing &lt;Task(PythonOperator): test_python_operator_template&gt; on 2017-01-18 00:00:00\n'2017-01-18'\n{   u'END_DATE': '2017-01-18',\n    u'conf': &lt;module 'airflow.configuration' from '/Library/Python/2.7/site-packages/airflow/configuration.pyc'&gt;,\n    u'dag': &lt;DAG: test_python_operator_template_dag&gt;,\n    u'dag_run': None,\n    u'ds_nodash': u'20170118',\n    u'end_date': '2017-01-18',\n    u'execution_date': datetime.datetime(2017, 1, 18, 0, 0),\n    u'latest_date': '2017-01-18',\n    u'macros': &lt;module 'airflow.macros' from '/Library/Python/2.7/site-packages/airflow/macros/__init__.pyc'&gt;,\n    u'params': {   'condition_param': True,\n                   'message': 'Hello World',\n                   'table': 'TEMP_TABLE'},\n    u'run_id': None,\n    u'tables': None,\n    u'task': &lt;Task(PythonOperator): test_python_operator_template&gt;,\n    u'task_instance': &lt;TaskInstance: test_python_operator_template_dag.test_python_operator_template 2017-01-18 00:00:00 [running]&gt;,\n    u'task_instance_key_str': u'test_python_operator_template_dag__test_python_operator_template__20170118',\n    'templates_dict': {   'file1': u'pyoptemplate.sql',\n                          'file2': u'pyoptemplate.t',\n                          'pyoptemplate': u'',\n                          'pyoptemplate.sql': u'',\n                          'sql': u'pyoptemplate',\n                          'table': u'TEMP_TABLE'},\n    u'test_mode': True,\n    u'ti': &lt;TaskInstance: test_python_operator_template_dag.test_python_operator_template 2017-01-18 00:00:00 [running]&gt;,\n    u'tomorrow_ds': '2017-01-19',\n    u'tomorrow_ds_nodash': u'20170119',\n    u'ts': '2017-01-18T00:00:00',\n    u'ts_nodash': u'20170118T000000',\n    u'yesterday_ds': '2017-01-17',\n    u'yesterday_ds_nodash': u'20170117'}\n[2017-01-18 23:58:07,634] {python_operator.py:67} INFO - Done. Returned value was: None\n</code></pre>\n"}, {"tags": ["airflow"], "owner": {"account_id": 4298463, "reputation": 1039, "user_id": 3512820, "user_type": "registered", "profile_image": "https://i.sstatic.net/E4OYU.png?s=256", "display_name": "Oleg Alexander", "link": "https://stackoverflow.com/users/3512820/oleg-alexander"}, "is_answered": true, "view_count": 27982, "answer_count": 3, "score": 25, "last_activity_date": 1722235160, "creation_date": 1571449465, "question_id": 58459855, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/58459855/how-to-delete-a-dag-run-in-apache-airflow", "title": "How to delete a DAG run in Apache Airflow?", "body": "<p>I know there is a way to delete a DAG. But is it possible to delete a DAG run with a specific run_id? Something like:</p>\n\n<p><code>airflow delete_dag_run &lt;dag_id&gt; &lt;run_id&gt;</code></p>\n"}, {"tags": ["airflow"], "owner": {"account_id": 1206193, "reputation": 2943, "user_id": 1175788, "user_type": "registered", "accept_rate": 28, "profile_image": "https://www.gravatar.com/avatar/ed6991f13be59c3996322a3afd84708e?s=256&d=identicon&r=PG", "display_name": "simplycoding", "link": "https://stackoverflow.com/users/1175788/simplycoding"}, "is_answered": true, "view_count": 33983, "answer_count": 1, "score": 25, "last_activity_date": 1510548737, "creation_date": 1497369391, "question_id": 44526354, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/44526354/how-does-airflows-branchpythonoperator-work", "title": "How does Airflow&#39;s BranchPythonOperator work?", "body": "<p>I'm struggling to understand how BranchPythonOperator in Airflow works. I know it's primarily used for branching, but am confused by the documentation as to what to pass into a task and what I need to pass/expect from the task upstream. </p>\n\n<p>Given the simple example in the documentation <a href=\"https://airflow.incubator.apache.org/concepts.html#branching\" rel=\"noreferrer\">on this page</a> what would the source code look like for the upstream task called <code>run_this_first</code> and the 2 downstream ones that are branched? How exactly does Airflow know to run <code>branch_a</code> instead of <code>branch_b</code>? Where does the upstream task's` output get noticed/read?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 294}