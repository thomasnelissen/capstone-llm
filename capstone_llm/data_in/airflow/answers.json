{"items": [{"owner": {"account_id": 421686, "reputation": 9417, "user_id": 800053, "user_type": "registered", "accept_rate": 76, "profile_image": "https://www.gravatar.com/avatar/13fc4f89838b715532469a38e098fb77?s=256&d=identicon&r=PG", "display_name": "jhnclvr", "link": "https://stackoverflow.com/users/800053/jhnclvr"}, "is_accepted": false, "score": 183, "last_activity_date": 1678522376, "last_edit_date": 1678522376, "creation_date": 1492182746, "answer_id": 43414326, "question_id": 43410836, "content_license": "CC BY-SA 4.0", "body": "<p>When you startup airflow, make sure you set:</p>\n<p><code>load_examples = False</code></p>\n<p>inside your <code>airflow.cfg</code></p>\n<p>If you have already started airflow with this not set to false, you can set it to false and run <code>airflow db reset</code>(if you are using 1.x <code>airflow resetdb</code>) in the cli (!which will destroy all current dag information!).</p>\n<p>Alternatively you can go into the <code>airflow_db</code> and manually delete those entries from the <code>dag</code> table.</p>\n"}, {"owner": {"account_id": 1468834, "reputation": 90751, "user_id": 1380918, "user_type": "registered", "accept_rate": 98, "profile_image": "https://www.gravatar.com/avatar/6e7c96a05fdd3ab2a61d07e78cc0f59c?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "hexacyanide", "link": "https://stackoverflow.com/users/1380918/hexacyanide"}, "is_accepted": true, "score": 161, "last_activity_date": 1696450515, "last_edit_date": 1696450515, "creation_date": 1559181628, "answer_id": 56370721, "question_id": 56370720, "content_license": "CC BY-SA 4.0", "body": "<p>Here's an expanded list of configuration options that are available since Airflow v1.10.2. Some can be set on a per-DAG or per-operator basis, but may also fall back to the setup-wide defaults when they are not specified.</p>\n<hr />\n<p>Options that can be specified <strong>on a per-DAG basis</strong>:</p>\n<ul>\n<li><code>concurrency</code>: the number of task instances allowed to run concurrently across all active runs of the DAG this is set on. Defaults to <code>core.dag_concurrency</code> if not set</li>\n<li><code>max_active_runs</code>: maximum number of active runs for this DAG. The scheduler will not create new active DAG runs once this limit is hit. Defaults to <code>core.max_active_runs_per_dag</code> if not set</li>\n</ul>\n<p>Examples:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Only allow one run of this DAG to be running at any given time\ndag = DAG('my_dag_id', max_active_runs=1)\n\n# Allow a maximum of 10 tasks to be running across a max of 2 active DAG runs\ndag = DAG('example2', concurrency=10, max_active_runs=2)\n</code></pre>\n<hr />\n<p>Options that can be specified <strong>on a per-operator basis</strong>:</p>\n<ul>\n<li><code>pool</code>: the pool to execute the task in. <a href=\"https://airflow.apache.org/concepts.html#pools\" rel=\"noreferrer\">Pools</a> can be used to limit parallelism for <em>only a subset</em> of tasks</li>\n<li><code>max_active_tis_per_dag</code>: controls the number of concurrent running task instances across dag_runs per task.</li>\n</ul>\n<p>Example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>t1 = BaseOperator(pool='my_custom_pool', max_active_tis_per_dag=12)\n</code></pre>\n<hr />\n<p>Options that are specified <strong>across an entire Airflow setup</strong>:</p>\n<ul>\n<li><code>core.parallelism</code>: maximum number of tasks running across an entire Airflow installation</li>\n<li><code>core.dag_concurrency</code>: max number of tasks that can be running per DAG (across multiple <em>DAG runs</em>)</li>\n<li><code>core.non_pooled_task_slot_count</code>: number of task slots allocated to tasks not running in a pool</li>\n<li><code>core.max_active_runs_per_dag</code>: maximum number of active DAG <em>runs</em>, per DAG</li>\n<li><code>scheduler.max_threads</code>: how many threads the scheduler process should use to use to schedule DAGs</li>\n<li><code>celery.worker_concurrency</code>: max number of task instances that a worker will process at a time <em>if using CeleryExecutor</em></li>\n<li><code>celery.sync_parallelism</code>: number of processes CeleryExecutor should use to sync task state</li>\n</ul>\n"}, {"owner": {"account_id": 4983282, "reputation": 5138, "user_id": 4007615, "user_type": "registered", "profile_image": "https://graph.facebook.com/10153641288441322/picture?type=large", "display_name": "cryanbhu", "link": "https://stackoverflow.com/users/4007615/cryanbhu"}, "is_accepted": true, "score": 151, "last_activity_date": 1618896865, "last_edit_date": 1618896865, "creation_date": 1532506136, "answer_id": 51513902, "question_id": 50249759, "content_license": "CC BY-SA 4.0", "body": "<p>The other answers are quite technical and hard to understand. I was in your position before so I'll explain in <em>simple terms</em>.</p>\n<p><strong>Airflow</strong> can do <em>anything</em>. It has <code>BashOperator</code> and <code>PythonOperator</code> which means it can run any bash script or any Python script.<br />\nIt is a way to organize (setup complicated data pipeline DAGs), schedule, monitor, trigger re-runs of data pipelines, in a easy-to-view and use UI.<br />\nAlso, it is easy to setup and everything is in familiar Python code.<br />\nDoing pipelines in an organized manner (i.e using Airflow) means you don't waste time debugging a mess of data processing (<code>cron</code>) scripts all over the place.<br />\nNowadays (roughly year 2020 onwards), we call it an <strong>orchestration</strong> tool.</p>\n<p><strong>Apache Beam</strong> is a wrapper for the many data processing frameworks (Spark, Flink etc.) out there.<br />\nThe intent is so you just learn Beam and can run on multiple backends (Beam runners).<br />\nIf you are familiar with Keras and TensorFlow/Theano/Torch, the relationship between Keras and its backends is similar to the relationship between Beam and its data processing backends.</p>\n<p>Google Cloud Platform's Cloud Dataflow is one backend for running Beam on.<br />\nThey call it the <a href=\"https://beam.apache.org/documentation/runners/dataflow/\" rel=\"noreferrer\">Dataflow runner</a>.</p>\n<p>GCP's offering, <strong>Cloud Composer</strong>, is a <strong>managed Airflow implementation</strong> as a service, running in a Kubernetes cluster in Google Kubernetes Engine (GKE).</p>\n<p>So you can either:</p>\n<ul>\n<li>manual Airflow implementation, doing data processing on the instance itself (if your data is small (or your instance is powerful enough), you can process data on the machine running Airflow. This is why many are confused if Airflow can process data or not)</li>\n<li>manual Airflow implementation calling Beam jobs</li>\n<li>Cloud Composer (managed Airflow as a service) calling jobs in Cloud Dataflow</li>\n<li>Cloud Composer running data processing containers in Composer's Kubernetes cluster environment itself, using Airflow's <code>KubernetesPodOperator (KPO)</code></li>\n<li>Cloud Composer running data processing containers in Composer's Kubernetes cluster environment with Airflow's <code>KPO</code>, but this time in a better <strong>isolated</strong> fashion by creating a new node-pool and specifying that the <code>KPO</code> pods are to be run in the new node-pool</li>\n</ul>\n<p><strong>My personal experience</strong>:<br />\nAirflow is lightweight and not difficult to learn (easy to implement), you should use it for your data pipelines whenever possible.<br />\nAlso, since many companies are looking for experience using Airflow, if you're looking to be a data engineer you should probably learn it<br />\nAlso, managed Airflow (I've only used GCP's Composer so far) is much more convenient than running Airflow yourself, and managing the airflow <code>webserver</code> and <code>scheduler</code> processes.</p>\n"}, {"owner": {"account_id": 421686, "reputation": 9417, "user_id": 800053, "user_type": "registered", "accept_rate": 76, "profile_image": "https://www.gravatar.com/avatar/13fc4f89838b715532469a38e098fb77?s=256&d=identicon&r=PG", "display_name": "jhnclvr", "link": "https://stackoverflow.com/users/800053/jhnclvr"}, "is_accepted": true, "score": 122, "last_activity_date": 1491849093, "creation_date": 1491849093, "answer_id": 43330451, "question_id": 43270820, "content_license": "CC BY-SA 3.0", "body": "<p>In the UI:</p>\n\n<ol>\n<li>Go to the dag, and dag run of the run you want to change</li>\n<li>Click on GraphView </li>\n<li>Click on task A</li>\n<li>Click \"Clear\"</li>\n</ol>\n\n<p>This will let task A run again, and if it succeeds, task C should run.\nThis works because when you clear a task's status, the scheduler will treat it as if it hadn't run before for this dag run.</p>\n"}, {"owner": {"account_id": 7489236, "reputation": 18632, "user_id": 5691525, "user_type": "registered", "accept_rate": 78, "profile_image": "https://i.sstatic.net/i3e64.jpg?s=256", "display_name": "kaxil", "link": "https://stackoverflow.com/users/5691525/kaxil"}, "is_accepted": true, "score": 121, "last_activity_date": 1613070172, "creation_date": 1613070172, "answer_id": 66161165, "question_id": 66160780, "content_license": "CC BY-SA 4.0", "body": "<p>There is no default username and password created if you are just using python wheel.</p>\n<p>Run the following to create a user:</p>\n<p><strong>For Airflow &gt;=2.0.0</strong>:</p>\n<pre><code>airflow users  create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin\n</code></pre>\n<p>OR</p>\n<p><strong>For Airflow &lt;1.10.14</strong>:</p>\n<pre><code>airflow create_user -r Admin -u admin -e admin@example.com -f admin -l user -p admin\n</code></pre>\n"}, {"owner": {"account_id": 13729676, "reputation": 3297, "user_id": 9907481, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/53e46ecea950971c9f3d3d7419db0ac2?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "amsh", "link": "https://stackoverflow.com/users/9907481/amsh"}, "is_accepted": true, "score": 117, "last_activity_date": 1615352610, "last_edit_date": 1615352610, "creation_date": 1602058313, "answer_id": 64239918, "question_id": 64016869, "content_license": "CC BY-SA 4.0", "body": "<p>I have worked on both Apache Airflow and AWS Step Functions and here are some insights:</p>\n<ul>\n<li>Step Functions provide out of the box maintenance. It has high availability and scalability that is required for your use-case, for Airflow we'll have to do to it with auto-scaling/load balancing on servers or containers (kubernetes).*</li>\n<li>Both Airflow and Step Functions have user friendly UI's. While Airflow supports multiple representations of the state machine, Step Functions only display state machine as DAG's.</li>\n<li>As of version 2.0, Airflow's Rest API is now <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html\" rel=\"noreferrer\">stable</a>. AWS Step Functions are also supported by a range of production graded cli and <a href=\"https://aws.amazon.com/tools/\" rel=\"noreferrer\">SDK's</a>.</li>\n<li>Airflow has server costs while Step Functions have 4000/month free step executions (free tier) and $0.000025/step after that. e.g. if you use 10K steps for AWS Batch that run once daily, you will be priced $0.25 per day ($7.5 per month). The price for Airflow server (t2.large ec2 1 year reserved instance) is $41.98 per month. We will have to use AWS Batch for either case.**</li>\n<li>AWS Batch can integrate to both <a href=\"https://airflow.apache.org/docs/stable/_api/airflow/contrib/operators/awsbatch_operator/index.html\" rel=\"noreferrer\">Airflow</a> and <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/connect-batch.html\" rel=\"noreferrer\">Step Functions</a>.</li>\n<li>You can clear and rerun a failed task in Apache Airflow, but in Step Functions you will have to create a <a href=\"https://aws.amazon.com/blogs/compute/resume-aws-step-functions-from-any-state/\" rel=\"noreferrer\">custom implementation</a> to handle that. You may handle <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html\" rel=\"noreferrer\">automated retries with back-offs</a> in Step Functions definition as well.</li>\n<li>For failed task in Step Functions you will get a visual representation of failed state and the detailed message when you click it. You may also use aws cli or sdk to get the details.</li>\n<li>Step Functions use easy to use JSON as state machine definition, while Airflow uses Python script.</li>\n<li>Step Functions support <a href=\"https://aws.amazon.com/about-aws/whats-new/2019/05/aws-step-functions-support-callback-patterns/\" rel=\"noreferrer\">async callbacks</a>, i.e. state machine pauses until an external source notifies it to resume. While Airflow has <a href=\"https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-28%3A+Add+AsyncExecutor+option\" rel=\"noreferrer\">yet to add</a> this feature.</li>\n</ul>\n<p>Overall, I see more advantages of using AWS Step Functions. You will have to consider maintenance cost and development cost for both services as per your use case.</p>\n<p><strong>UPDATES</strong> (AWS Managed Workflows for Apache Airflow Service):</p>\n<ul>\n<li>*With AWS Managed Workflows for Apache Airflow service, you can offload deployment, maintenance, autoscaling/load balancing and security of your Airflow Service to AWS. But please consider the version number you're willing to settle for, as AWS managed services are mostly behind the latest version. (e.g. As of March 08, 2021, the latest version of open source airflow is 2.01, while MWAA allows version 1.10.12)</li>\n<li>**MWAA costs on environment, instance and storage. <a href=\"https://aws.amazon.com/managed-workflows-for-apache-airflow/pricing/\" rel=\"noreferrer\">More details here</a>.</li>\n</ul>\n"}, {"owner": {"account_id": 7489236, "reputation": 18632, "user_id": 5691525, "user_type": "registered", "accept_rate": 78, "profile_image": "https://i.sstatic.net/i3e64.jpg?s=256", "display_name": "kaxil", "link": "https://stackoverflow.com/users/5691525/kaxil"}, "is_accepted": true, "score": 112, "last_activity_date": 1554453352, "last_edit_date": 1554453352, "creation_date": 1536260464, "answer_id": 52210596, "question_id": 52203441, "content_license": "CC BY-SA 4.0", "body": "<p>Try the following:</p>\n\n<pre><code>export AIRFLOW_GPL_UNIDECODE=yes\n</code></pre>\n\n<p>OR</p>\n\n<pre><code>export SLUGIFY_USES_TEXT_UNIDECODE=yes\n</code></pre>\n\n<p>Using <code>export</code> makes the environment variable available to all the subprocesses.</p>\n\n<p>Also, make sure you are using <code>pip install apache-airflow[postgres]</code> and not <code>pip install airflow[postgres]</code></p>\n\n<p>Which should you use: if using AIRFLOW_GPL_UNIDECODE, airflow will install a dependency that is under GPL license, which means you won't be able to distribute your resulting application commercially. If that's a problem for you, go for SLUGIFY_USES_TEXT_UNIDECODE.</p>\n"}, {"owner": {"account_id": 2046694, "reputation": 8183, "user_id": 5191221, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/afa07313f2833407b490a276545d9875?s=256&d=identicon&r=PG", "display_name": "tobi6", "link": "https://stackoverflow.com/users/5191221/tobi6"}, "is_accepted": false, "score": 110, "last_activity_date": 1519900710, "creation_date": 1519900710, "answer_id": 49047832, "question_id": 49021055, "content_license": "CC BY-SA 3.0", "body": "<p>Airflow can be a bit tricky to setup.</p>\n\n<ul>\n<li>Do you have the <code>airflow scheduler</code> running?</li>\n<li>Do you have the <code>airflow webserver</code> running? </li>\n<li>Have you checked that all DAGs you want to run are set to <em>On</em> in the web ui?</li>\n<li>Do all the DAGs you want to run have a start date which is in the past?</li>\n<li>Do all the DAGs you want to run have a proper schedule which is shown in the web ui?</li>\n<li>If nothing else works, you can use the web ui to click on the dag, then on <em>Graph View</em>. Now select the first task and click on <em>Task Instance</em>. In the paragraph <em>Task Instance Details</em> you will see why a DAG is waiting or not running. </li>\n</ul>\n\n<p>I've had for instance a DAG which was wrongly set to <code>depends_on_past: True</code> which forbid the current instance to start correctly.</p>\n\n<p>Also a great resource directly in the docs, which has a few more hints: <a href=\"https://airflow.apache.org/faq.html#why-isn-t-my-task-getting-scheduled\" rel=\"noreferrer\">Why isn't my task getting scheduled?</a>.</p>\n"}, {"owner": {"account_id": 1113994, "reputation": 2923, "user_id": 1103817, "user_type": "registered", "accept_rate": 59, "profile_image": "https://www.gravatar.com/avatar/4f7ef2f36726859f4c28c945526d37bf?s=256&d=identicon&r=PG", "display_name": "Roger", "link": "https://stackoverflow.com/users/1103817/roger"}, "is_accepted": true, "score": 108, "last_activity_date": 1491206308, "creation_date": 1491206308, "answer_id": 43179385, "question_id": 38200666, "content_license": "CC BY-SA 3.0", "body": "<p><strong>parallelism:</strong> not a very descriptive name. The description says it sets the maximum task instances for the airflow installation, which is a bit ambiguous \u2014 if I have two hosts running airflow workers, I'd have airflow installed on two hosts, so that should be two installations, but based on context 'per installation' here means 'per Airflow state database'. I'd name this max_active_tasks.</p>\n\n<p><strong>dag_concurrency:</strong> Despite the name based on the comment this is actually the task concurrency, and it's per worker. I'd name this max_active_tasks_for_worker (per_worker would suggest that it's a global setting for workers, but I think you can have workers with different values set for this).</p>\n\n<p><strong>max_active_runs_per_dag</strong>: This one's kinda alright, but since it seems to be just a default value for the matching DAG kwarg, it might be nice to reflect that in the name, something like default_max_active_runs_for_dags\nSo let's move on to the DAG kwargs:</p>\n\n<p><strong>concurrency</strong>: Again, having a general name like this, coupled with the fact that concurrency is used for something different elsewhere makes this pretty confusing. I'd call this max_active_tasks.</p>\n\n<p><strong>max_active_runs</strong>: This one sounds alright to me.</p>\n\n<p>source: <a href=\"https://issues.apache.org/jira/browse/AIRFLOW-57\" rel=\"noreferrer\">https://issues.apache.org/jira/browse/AIRFLOW-57</a></p>\n\n<hr>\n\n<p><strong>max_threads</strong> gives the user some control over cpu usage. It specifies scheduler parallelism.</p>\n"}, {"owner": {"account_id": 372294, "reputation": 1539, "user_id": 721270, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/23697706add465d2cdf061c456ea03dc?s=256&d=identicon&r=PG", "display_name": "Anselmo", "link": "https://stackoverflow.com/users/721270/anselmo"}, "is_accepted": true, "score": 106, "last_activity_date": 1606018090, "last_edit_date": 1606018090, "creation_date": 1479936176, "answer_id": 40774361, "question_id": 39997714, "content_license": "CC BY-SA 4.0", "body": "<p>EDIT: This answer stores your secret key in <em>plain text</em> which can be a <strong>security risk</strong> and is not recommended. The best way is to put access key and secret key in the login/password fields, as mentioned in other answers below.\nEND EDIT</p>\n<p>It's hard to find references, but after digging a bit  I was able to make it work.</p>\n<h3>TLDR</h3>\n<p>Create a new connection with the following attributes:</p>\n<p><strong>Conn Id:</strong> my_conn_S3</p>\n<p><strong>Conn Type:</strong> S3</p>\n<p><strong>Extra:</strong></p>\n<pre><code>{&quot;aws_access_key_id&quot;:&quot;_your_aws_access_key_id_&quot;, &quot;aws_secret_access_key&quot;: &quot;_your_aws_secret_access_key_&quot;}\n</code></pre>\n<hr />\n<h3>Long version, setting up UI connection:</h3>\n<ul>\n<li>On Airflow UI, go to Admin &gt; Connections</li>\n<li>Create a new connection with the following attributes:</li>\n<li>Conn Id: <code>my_conn_S3</code></li>\n<li>Conn Type: <code>S3</code></li>\n<li>Extra: <code>{&quot;aws_access_key_id&quot;:&quot;_your_aws_access_key_id_&quot;, &quot;aws_secret_access_key&quot;: &quot;_your_aws_secret_access_key_&quot;}</code></li>\n<li>Leave all the other fields (Host, Schema, Login) blank.</li>\n</ul>\n<p>To use this connection, below you can find a simple S3 Sensor Test. The idea of this test is to set up a sensor that watches files in S3 (T1 task) and once below condition is satisfied it triggers a bash command (T2 task).</p>\n<h3>Testing</h3>\n<ul>\n<li>Before running the DAG, ensure you've an S3 bucket named 'S3-Bucket-To-Watch'.</li>\n<li>Add below s3_dag_test.py to airflow dags folder (~/airflow/dags)</li>\n<li>Start <code>airflow webserver</code>.</li>\n<li>Go to Airflow UI (http://localhost:8383/)</li>\n<li>Start <code>airflow scheduler</code>.</li>\n<li>Turn on 's3_dag_test' DAG on the main DAGs view.</li>\n<li>Select 's3_dag_test' to show the dag details.</li>\n<li>On the Graph View you should be able to see it's current state.</li>\n<li>'check_s3_for_file_in_s3' task should be active and running.</li>\n<li>Now, add a file named 'file-to-watch-1' to your 'S3-Bucket-To-Watch'.</li>\n<li>First tasks should have been completed, second should be started and finish.</li>\n</ul>\n<p>The schedule_interval in the dag definition is set to '@once', to facilitate debugging.</p>\n<p>To run it again, leave everything as it's, remove files in the bucket and try  again by selecting the first task (in the graph view) and selecting 'Clear' all 'Past','Future','Upstream','Downstream' .... activity.  This should kick off the DAG again.</p>\n<p>Let me know how it went.</p>\n<h3>s3_dag_test.py ;</h3>\n<pre><code>&quot;&quot;&quot;\nS3 Sensor Connection Test\n&quot;&quot;&quot;\n\nfrom airflow import DAG\nfrom airflow.operators import SimpleHttpOperator, HttpSensor,   BashOperator, EmailOperator, S3KeySensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2016, 11, 1),\n    'email': ['something@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 5,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG('s3_dag_test', default_args=default_args, schedule_interval= '@once')\n\nt1 = BashOperator(\n    task_id='bash_test',\n    bash_command='echo &quot;hello, it should work&quot; &gt; s3_conn_test.txt',\n    dag=dag)\n\nsensor = S3KeySensor(\n    task_id='check_s3_for_file_in_s3',\n    bucket_key='file-to-watch-*',\n    wildcard_match=True,\n    bucket_name='S3-Bucket-To-Watch',\n    s3_conn_id='my_conn_S3',\n    timeout=18*60*60,\n    poke_interval=120,\n    dag=dag)\n\nt1.set_upstream(sensor)\n</code></pre>\n<hr />\nMain References:\n<ul>\n<li><a href=\"https://gitter.im/apache/incubator-airflow\" rel=\"noreferrer\">https://gitter.im/apache/incubator-airflow</a></li>\n<li><a href=\"https://groups.google.com/forum/#!topic/airbnb_airflow/TXsJNOBBfig\" rel=\"noreferrer\">https://groups.google.com/forum/#!topic/airbnb_airflow/TXsJNOBBfig</a></li>\n<li><a href=\"https://github.com/apache/incubator-airflow\" rel=\"noreferrer\">https://github.com/apache/incubator-airflow</a></li>\n</ul>\n"}, {"owner": {"account_id": 4881680, "reputation": 6742, "user_id": 3935325, "user_type": "registered", "accept_rate": 86, "profile_image": "https://i.sstatic.net/jjwPe.jpg?s=256", "display_name": "villasv", "link": "https://stackoverflow.com/users/3935325/villasv"}, "is_accepted": false, "score": 97, "last_activity_date": 1669216598, "last_edit_date": 1669216598, "creation_date": 1507477838, "answer_id": 46633026, "question_id": 43678408, "content_license": "CC BY-SA 4.0", "body": "<h2>Airflow 2.x</h2>\n<p>Airflow provides a <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html#branching\" rel=\"noreferrer\">branching decorator</a> that allows you to return the task_id (or list of task_ids) that should run:</p>\n<pre class=\"lang-py prettyprint-override\"><code>@task.branch(task_id=&quot;branch_task&quot;)\ndef branch_func(ti):\n    xcom_value = int(ti.xcom_pull(task_ids=&quot;start_task&quot;))\n    if xcom_value &gt;= 5:\n        return &quot;big_task&quot; # run just this one task, skip all else\n    elif xcom_value &gt;= 3:\n        return [&quot;small_task&quot;, &quot;warn_task&quot;] # run these, skip all else\n    else:\n        return None # skip everything\n</code></pre>\n<p>You can also inherit directly from <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/branch/index.html#airflow.operators.branch.BaseBranchOperator\" rel=\"noreferrer\">BaseBranchOperator</a> overriding the <code>choose_branch</code> method, but for simple branching logic the decorator is best.</p>\n<h2>Airflow 1.x</h2>\n<p>Airflow has a <a href=\"https://airflow.apache.org/docs/apache-airflow/1.10.6/_api/airflow/operators/python_operator/index.html?highlight=branchpythonoperator#airflow.operators.python_operator.BranchPythonOperator\" rel=\"noreferrer\">BranchPythonOperator</a> that can be used to express the branching dependency more directly.</p>\n<p>The <a href=\"https://airflow.apache.org/docs/apache-airflow/1.10.6/concepts.html?highlight=branch%20operator#branching\" rel=\"noreferrer\">docs</a> describe its use:</p>\n<blockquote>\n<p>The BranchPythonOperator is much like the PythonOperator except that it expects a python_callable that returns a task_id. The task_id returned is followed, and all of the other paths are skipped. The task_id returned by the Python function has to be referencing a task directly downstream from the BranchPythonOperator task.</p>\n</blockquote>\n<blockquote>\n<p>If you want to skip some tasks, keep in mind that you can\u2019t have an empty path, if so make a dummy task.</p>\n</blockquote>\n<p><img src=\"https://i.sstatic.net/zOW2B.png\" alt=\"\" /></p>\n<h3>Code Example</h3>\n<pre><code>def dummy_test():\n    return 'branch_a'\n\nA_task = DummyOperator(task_id='branch_a', dag=dag)\nB_task = DummyOperator(task_id='branch_false', dag=dag)\n\nbranch_task = BranchPythonOperator(\n    task_id='branching',\n    python_callable=dummy_test,\n    dag=dag,\n)\n\nbranch_task &gt;&gt; A_task \nbranch_task &gt;&gt; B_task\n</code></pre>\n<p>If you're installing an Airflow version &gt;=1.10.3, you can also return a list of task ids, allowing you to skip multiple downstream paths in a single Operator and don't have to use a dummy task before joining.</p>\n<p><img src=\"https://user-images.githubusercontent.com/6249654/48800846-3a19e980-ed0b-11e8-89d0-29ceba2ce2fb.png\" alt=\"\" /></p>\n"}, {"owner": {"account_id": 7489236, "reputation": 18632, "user_id": 5691525, "user_type": "registered", "accept_rate": 78, "profile_image": "https://i.sstatic.net/i3e64.jpg?s=256", "display_name": "kaxil", "link": "https://stackoverflow.com/users/5691525/kaxil"}, "is_accepted": true, "score": 94, "last_activity_date": 1709066853, "last_edit_date": 1709066853, "creation_date": 1544297533, "answer_id": 53686174, "question_id": 53663534, "content_license": "CC BY-SA 4.0", "body": "<p>You can pass parameters from the CLI using <code>--conf '{&quot;key&quot;:&quot;value&quot;}'</code> and then use it in the DAG file as <code>&quot;{{ dag_run.conf[&quot;key&quot;] }}&quot;</code> in templated field.</p>\n<p><strong>CLI</strong>:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>airflow trigger_dag 'example_dag_conf' -r 'run_id' --conf '{&quot;message&quot;:&quot;value&quot;}'\n</code></pre>\n<p><strong>DAG File</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>args = {\n    'start_date': datetime.utcnow(),\n    'owner': 'airflow',\n}\n\ndag = DAG(\n    dag_id='example_dag_conf',\n    default_args=args,\n    schedule_interval=None,\n)\n\ndef run_this_func(ds, **kwargs):\n    print(&quot;Remotely received value of {} for key=message&quot;.\n          format(kwargs['dag_run'].conf['message']))\n\n\nrun_this = PythonOperator(\n    task_id='run_this',\n    provide_context=True,\n    python_callable=run_this_func,\n    dag=dag,\n)\n\n# You can also access the DagRun object in templates\nbash_task = BashOperator(\n    task_id=&quot;bash_task&quot;,\n    bash_command='echo &quot;Here is the message: '\n                 '{{ dag_run.conf[&quot;message&quot;] if dag_run else &quot;&quot; }}&quot; ',\n    dag=dag,\n)\n</code></pre>\n<hr />\n<p>PSA for new airflow users: it's worth making the switch to <code>--conf</code>. Params are a nice pattern, but they are an architectural dead end because they don't appear widely supported for programmatic use. Meanwhile, --conf is exposed in services like google Cloud Composer's trigger dag run.</p>\n"}, {"owner": {"account_id": 8010072, "reputation": 1465, "user_id": 6042129, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/f5810b5aa9d8a3dcaca4581f1d1eef0b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "J.Fratzke", "link": "https://stackoverflow.com/users/6042129/j-fratzke"}, "is_accepted": true, "score": 88, "last_activity_date": 1551990588, "last_edit_date": 1551990588, "creation_date": 1484245482, "answer_id": 41620340, "question_id": 40120474, "content_license": "CC BY-SA 4.0", "body": "<p>You can import the logging module into your code and write to logs that way</p>\n\n<pre><code>import logging\n\nlogging.info('Hello')\n</code></pre>\n\n<p>Here are some more options</p>\n\n<pre><code>import logging    \n\nlogging.debug('This is a debug message')\nlogging.info('This is an info message')\nlogging.warning('This is a warning message')\nlogging.error('This is an error message')\nlogging.critical('This is a critical message')\n</code></pre>\n"}, {"owner": {"account_id": 7074026, "reputation": 1193, "user_id": 5414722, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-0MVYWwk5sxY/AAAAAAAAAAI/AAAAAAAABOs/5dEgVbUSQBE/photo.jpg?sz=256", "display_name": "Rishi Barve", "link": "https://stackoverflow.com/users/5414722/rishi-barve"}, "is_accepted": true, "score": 87, "last_activity_date": 1608331478, "last_edit_date": 1608331478, "creation_date": 1486970744, "answer_id": 42198617, "question_id": 42147514, "content_license": "CC BY-SA 4.0", "body": "<p>This is a pitfall of airflow. Add a space at the end of your bash_command and it should run fine</p>\n<p>Source:\n<a href=\"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=62694614\" rel=\"noreferrer\">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=62694614</a></p>\n"}, {"owner": {"account_id": 5009025, "reputation": 7841, "user_id": 4025874, "user_type": "registered", "accept_rate": 67, "profile_image": "https://i.sstatic.net/JcYk0.jpg?s=256", "display_name": "Daniel Lee", "link": "https://stackoverflow.com/users/4025874/daniel-lee"}, "is_accepted": false, "score": 85, "last_activity_date": 1629183775, "last_edit_date": 1629183775, "creation_date": 1500990940, "answer_id": 45305477, "question_id": 45280650, "content_license": "CC BY-SA 4.0", "body": "<p>You can store the password in a Hook - this will be encrypted so long as you have setup your fernet key.</p>\n<p>Here is how you can create a connection via the UI:</p>\n<p><a href=\"https://i.sstatic.net/T7PdI.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/T7PdI.png\" alt=\"Main Menu\" /></a>\nThen:\n<a href=\"https://i.sstatic.net/lhSgE.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/lhSgE.png\" alt=\"Create Connection\" /></a></p>\n<p>To access this password:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from airflow.hooks.base_hook import BaseHook  # Deprecated in Airflow 2\n\nconnection = BaseHook.get_connection(&quot;username_connection&quot;)\npassword = connection.password # This is a getter that returns the unencrypted password.\n</code></pre>\n<p><strong>Update since Airflow 2 launch</strong></p>\n<p>The library <code>airflow.hooks.base_hook</code> <a href=\"https://airflow.apache.org/docs/apache-airflow/2.1.2/_api/airflow/hooks/base_hook/index.html#module-airflow.hooks.base_hook\" rel=\"noreferrer\">has been deprecated</a> and you must use <a href=\"https://airflow.apache.org/docs/apache-airflow/2.1.2/_api/airflow/hooks/base/index.html#module-airflow.hooks.base\" rel=\"noreferrer\"><code>airflow.hooks.base</code></a> instead.</p>\n"}, {"owner": {"account_id": 97387, "reputation": 6572, "user_id": 264177, "user_type": "registered", "accept_rate": 32, "profile_image": "https://www.gravatar.com/avatar/4bec61fe6165806be5ec752ba87245dd?s=256&d=identicon&r=PG", "display_name": "JanKanis", "link": "https://stackoverflow.com/users/264177/jankanis"}, "is_accepted": false, "score": 83, "last_activity_date": 1556400961, "creation_date": 1556400961, "answer_id": 55885068, "question_id": 50249759, "content_license": "CC BY-SA 4.0", "body": "<p>Apache Airflow and Apache Beam look quite similar on the surface. Both of them allow you to organise a set of steps that process your data and both ensure the steps run in the right order and have their dependencies satisfied. Both allow you to visualise the steps and dependencies as a directed acyclic graph (DAG) in a GUI.</p>\n\n<p>But when you dig a bit deeper there are big differences in what they do and the programming models they support.</p>\n\n<p>Airflow is a task management system. The nodes of the DAG are tasks and Airflow makes sure to run them in the proper order, making sure one task only starts once its dependency tasks have finished. Dependent tasks don't run at the same time but only one after another. Independent tasks can run concurrently.</p>\n\n<p>Beam is a dataflow engine. The nodes of the DAG form a (possibly branching) pipeline. All the nodes in the DAG are active at the same time, and they pass data elements from one to the next, each doing some processing on it.</p>\n\n<p>The two have some overlapping use cases but there are a lot of things only one of the two can do well.</p>\n\n<p>Airflow manages tasks, which depend on one another. While this dependency can consist of one task passing data to the next one, that is not a requirement. In fact Airflow doesn't even care what the tasks do, it just needs to start them and see if they finished or failed. If tasks need to pass data to one another you need to co-ordinate that yourself, telling each task where to read and write its data, e.g. a local file path or a web service somewhere. Tasks can consist of Python code but they can also be any external program or a web service call. </p>\n\n<p>In Beam, your step definitions are tightly integrated with the engine. You define the steps in a supported programming language and they run inside a Beam process. Handling the computation in an external process would be difficult if possible at all*, and is certainly not the way Beam is supposed to be used. Your steps only need to worry about the computation they're performing, not about storing or transferring the data. Transferring the data between different steps is handled entirely by the framework.</p>\n\n<p>In Airflow, if your tasks process data, a single task invocation typically does some transformation on the entire dataset. In Beam, the data processing is part of the core interfaces so it can't really do anything else. An invocation of a Beam step typically handles a single or a few data elements and not the full dataset. Because of this Beam also supports unbounded length datasets, which is not something Airflow can natively cope with.</p>\n\n<p>Another difference is that Airflow is a framework by itself, but Beam is actually an abstraction layer. Beam pipelines can run on Apache Spark, Apache Flink, Google Cloud Dataflow and others. All of these support a more or less similar programming model. Google has also cloudified Airflow into a service as Google Cloud Compose by the way.</p>\n\n<p>*Apache Spark's support for Python is actually implemented by running a full Python interpreter in a subprocess, but this is implemented at the framework level.</p>\n"}, {"owner": {"account_id": 2126495, "reputation": 4474, "user_id": 1888503, "user_type": "registered", "accept_rate": 79, "profile_image": "https://i.sstatic.net/z24JX.png?s=256", "display_name": "sage88", "link": "https://stackoverflow.com/users/1888503/sage88"}, "is_accepted": true, "score": 82, "last_activity_date": 1490889455, "creation_date": 1490889455, "answer_id": 43122799, "question_id": 38751872, "content_license": "CC BY-SA 3.0", "body": "<p>Upgrade to airflow version 1.8 and use catchup_by_default=False in the airflow.cfg or apply catchup=False to each of your dags.</p>\n\n<p><a href=\"https://github.com/apache/incubator-airflow/blob/master/UPDATING.md#catchup_by_default\" rel=\"noreferrer\">https://github.com/apache/incubator-airflow/blob/master/UPDATING.md#catchup_by_default</a></p>\n"}, {"owner": {"account_id": 157367, "reputation": 6459, "user_id": 375267, "user_type": "registered", "accept_rate": 83, "profile_image": "https://i.sstatic.net/gv81y.jpg?s=256", "display_name": "aaron", "link": "https://stackoverflow.com/users/375267/aaron"}, "is_accepted": false, "score": 81, "last_activity_date": 1619201647, "last_edit_date": 1619201647, "creation_date": 1508360559, "answer_id": 46819184, "question_id": 46059161, "content_license": "CC BY-SA 4.0", "body": "<p>Upvoted both the question and the answer, but I think that this can be made a little more clear for those users who just want to pass small data objects between <code>PythonOperator</code> tasks in their DAGs. Referencing this question and <a href=\"https://github.com/trbs/airflow-examples/blob/master/dags/example_xcom.py\" rel=\"noreferrer\">this XCom example</a> got me to the following solution. Super simple:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\nfrom airflow.models import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\nDAG = DAG(\n  dag_id='example_dag',\n  start_date=datetime.now(),\n  schedule_interval='@once'\n)\n\ndef push_function(**kwargs):\n    ls = ['a', 'b', 'c']\n    return ls\n\npush_task = PythonOperator(\n    task_id='push_task', \n    python_callable=push_function,\n    provide_context=True,\n    dag=DAG)\n\ndef pull_function(**kwargs):\n    ti = kwargs['ti']\n    ls = ti.xcom_pull(task_ids='push_task')\n    print(ls)\n\npull_task = PythonOperator(\n    task_id='pull_task', \n    python_callable=pull_function,\n    provide_context=True,\n    dag=DAG)\n\npush_task &gt;&gt; pull_task\n</code></pre>\n<p>I'm not sure why this works, but it does. A few questions for the community:</p>\n<ul>\n<li>What's happening with <code>ti</code> here? How is that built in to <code>**kwargs</code>?</li>\n<li>Is <code>provide_context=True</code> necessary for both functions?</li>\n</ul>\n<p>Any edits to make this answer clearer are very welcome!</p>\n"}, {"owner": {"account_id": 50136, "reputation": 12916, "user_id": 149428, "user_type": "registered", "accept_rate": 60, "profile_image": "https://i.sstatic.net/FIX3s.jpg?s=256", "display_name": "Taylor D. Edmiston", "link": "https://stackoverflow.com/users/149428/taylor-d-edmiston"}, "is_accepted": true, "score": 81, "last_activity_date": 1539977618, "last_edit_date": 1539977618, "creation_date": 1522974929, "answer_id": 49683543, "question_id": 40651783, "content_license": "CC BY-SA 4.0", "body": "<p><strong>Edit 8/27/18 - Airflow 1.10 is now released on PyPI!</strong></p>\n\n<p><a href=\"https://pypi.org/project/apache-airflow/1.10.0/\" rel=\"noreferrer\">https://pypi.org/project/apache-airflow/1.10.0/</a></p>\n\n<hr>\n\n<h2>How to delete a DAG completely</h2>\n\n<p>We have this feature now in Airflow \u2265 1.10!</p>\n\n<p>The PR <a href=\"https://github.com/apache/incubator-airflow/pull/2199\" rel=\"noreferrer\">#2199</a> (Jira: <a href=\"https://issues.apache.org/jira/browse/AIRFLOW-1002\" rel=\"noreferrer\">AIRFLOW-1002</a>) adding DAG removal to Airflow has now been merged which allows fully deleting a DAG's entries from all of the related tables.</p>\n\n<p>The core <a href=\"https://github.com/apache/incubator-airflow/blob/7488f2938da4e08645060531aa363204db7f50a5/airflow/api/common/experimental/delete_dag.py#L28-L55\" rel=\"noreferrer\">delete_dag(...)</a> code is now part of the experimental API, and there are entrypoints available <a href=\"https://github.com/apache/incubator-airflow/blob/9dba430b683361fc0ed7f50de6daa03c971a476b/airflow/bin/cli.py#L214-L232\" rel=\"noreferrer\">via the CLI</a> and also <a href=\"https://github.com/apache/incubator-airflow/blob/7488f2938da4e08645060531aa363204db7f50a5/airflow/www/api/experimental/endpoints.py#L89-L103\" rel=\"noreferrer\">via the REST API</a>.</p>\n\n<p>CLI:</p>\n\n<pre><code>airflow delete_dag my_dag_id\n</code></pre>\n\n<p>REST API (running webserver locally):</p>\n\n<pre><code>curl -X \"DELETE\" http://127.0.0.1:8080/api/experimental/dags/my_dag_id\n</code></pre>\n\n<hr>\n\n<p><strong>Warning regarding the REST API</strong>: Ensure that your Airflow cluster <a href=\"https://airflow.apache.org/api.html#authentication\" rel=\"noreferrer\">uses authentication</a> in production.</p>\n\n<h2>Installing / upgrading to Airflow 1.10 (current)</h2>\n\n<p>To upgrade, run either:</p>\n\n<pre><code>export SLUGIFY_USES_TEXT_UNIDECODE=yes\n</code></pre>\n\n<p>or:</p>\n\n<pre><code>export AIRFLOW_GPL_UNIDECODE=yes\n</code></pre>\n\n<p>Then:</p>\n\n<pre><code>pip install -U apache-airflow\n</code></pre>\n\n<p>Remember to check <a href=\"https://github.com/apache/incubator-airflow/blob/master/UPDATING.md\" rel=\"noreferrer\">UPDATING.md</a> first for the full details!</p>\n"}, {"owner": {"account_id": 2231790, "reputation": 6478, "user_id": 1969152, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8acc442341104960e3b61e07f5698449?s=256&d=identicon&r=PG", "display_name": "Daniel Huang", "link": "https://stackoverflow.com/users/1969152/daniel-huang"}, "is_accepted": true, "score": 78, "last_activity_date": 1644325904, "last_edit_date": 1644325904, "creation_date": 1504635776, "answer_id": 46061162, "question_id": 46059161, "content_license": "CC BY-SA 4.0", "body": "<p>Templates like <code>{{ ti.xcom_pull(...) }}</code> can only be used inside of parameters that support templates or they won't be rendered prior to execution. See the <code>template_fields</code>, <code>template_fields_renderers</code> and <code>template_ext</code> attributes of the <a href=\"https://github.com/apache/airflow/blob/main/airflow/operators/python.py#L124-L125\" rel=\"noreferrer\">PythonOperator</a> and <a href=\"https://github.com/apache/airflow/blob/main/airflow/operators/bash.py#L125-L130\" rel=\"noreferrer\">BashOperator</a>.</p>\n<p>So <code>templates_dict</code> is what you use to pass templates to your python operator:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def func_archive_s3_file(**context):\n    archive(context['templates_dict']['s3_path_filename'])\n\ntask_archive_s3_file = PythonOperator(\n    task_id='archive_s3_file',\n    dag=dag,\n    python_callable=obj.func_archive_s3_file,\n    provide_context=True,  # must pass this because templates_dict gets passed via context\n    templates_dict={'s3_path_filename': &quot;{{ ti.xcom_pull(task_ids='submit_file_to_spark') }}&quot; })\n</code></pre>\n<p>However in the case of fetching an XCom value, another alternative is just using the <code>TaskInstance</code> object made available to you via context:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def func_archive_s3_file(**context):\n    archive(context['ti'].xcom_pull(task_ids='submit_file_to_spark'))\n\ntask_archive_s3_file = PythonOperator(\n    task_id='archive_s3_file',\n    dag=dag,\n    python_callable=obj.func_archive_s3_file,\n    provide_context=True,\n</code></pre>\n"}, {"owner": {"account_id": 10879361, "reputation": 769, "user_id": 7998847, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b2200d39d58b5f57bece81982722565d?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Nick", "link": "https://stackoverflow.com/users/7998847/nick"}, "is_accepted": false, "score": 76, "last_activity_date": 1496942288, "last_edit_date": 1496942288, "creation_date": 1496941931, "answer_id": 44441890, "question_id": 44360853, "content_license": "CC BY-SA 3.0", "body": "<p>All operators have an argument <code>trigger_rule</code> which can be set to <code>'all_done'</code>, which will trigger that task regardless of the failure or success of the previous task(s).</p>\n\n<p>You could set the trigger rule for the task you want to run to <code>'all_done'</code> instead of the default <code>'all_success'</code>.</p>\n\n<p>A simple bash operator task with that argument would look like:</p>\n\n<pre><code>task = BashOperator(\n    task_id=\"hello_world\",\n    bash_command=\"echo Hello World!\",\n    trigger_rule=\"all_done\",\n    dag=dag\n    )\n</code></pre>\n"}, {"owner": {"account_id": 8367824, "reputation": 6183, "user_id": 6283849, "user_type": "registered", "accept_rate": 92, "profile_image": "https://i.sstatic.net/e5TQ0.png?s=256", "display_name": "Alexis.Rolland", "link": "https://stackoverflow.com/users/6283849/alexis-rolland"}, "is_accepted": true, "score": 74, "last_activity_date": 1564188696, "last_edit_date": 1564188696, "creation_date": 1516625362, "answer_id": 48382000, "question_id": 48378712, "content_license": "CC BY-SA 4.0", "body": "<p>Here is how we manage it for our team.</p>\n\n<p>First in terms of naming convention, each of our <strong>DAG file name</strong> matches the <strong>DAG Id</strong> from the content of the DAG itself (including the DAG version). This is useful because ultimately it's the DAG Id that you see in the Airflow UI so you will know exactly which file has been used behind each DAG.</p>\n\n<p>Example for a DAG like this:</p>\n\n<pre><code>from airflow import DAG\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n  'owner': 'airflow',\n  'depends_on_past': False,\n  'start_date': datetime(2017,12,05,23,59),\n  'email': ['me@mail.com'],\n  'email_on_failure': True\n}\n\ndag = DAG(\n  'my_nice_dag-v1.0.9', #update version whenever you change something\n  default_args=default_args,\n  schedule_interval=\"0,15,30,45 * * * *\",\n  dagrun_timeout=timedelta(hours=24),\n  max_active_runs=1)\n  [...]\n</code></pre>\n\n<p>The name of the <strong>DAG file</strong> would be: <strong>my_nice_dag-v1.0.9.py</strong></p>\n\n<ul>\n<li>All our DAG files are stored in a Git repository (among other things)</li>\n<li>Everytime a merge request is done in our master branch, our Continuous Integration pipeline starts a new build and packages our DAG files into a zip (we use Atlassian Bamboo but there's other solutions like Jenkins, Circle CI, Travis...)</li>\n<li>In Bamboo we configured a deployment script (shell) which unzips the package and places the DAG files on the Airflow server in the <strong>/dags</strong> folder.</li>\n<li>We usually deploy the DAGs in DEV for testing, then to UAT and finally PROD. The deployment is done with the click of a button in Bamboo UI thanks to the shell script mentioned above.</li>\n</ul>\n\n<p><strong>Benefits</strong></p>\n\n<ol>\n<li>Because you have included the DAG version in your file name, the previous version of your DAG file is not overwritten in the DAG folder so you can easily come back to it</li>\n<li>When your new DAG file is loaded in Airflow you can recognize it in the UI thanks to the version number.</li>\n<li>Because your DAG file name = DAG Id you could even improve the deployment script by adding some Airflow command line to automatically switch ON your new DAGs once they are deployed.</li>\n<li>Because every version of the DAGs is historicized in Git, we can always comeback to previous versions if needed.</li>\n</ol>\n"}, {"owner": {"account_id": 9874163, "reputation": 874, "user_id": 7312589, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/35ab16e75b937c24ab19a9acca00a4cb?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "bricca", "link": "https://stackoverflow.com/users/7312589/bricca"}, "is_accepted": true, "score": 73, "last_activity_date": 1619676110, "last_edit_date": 1619676110, "creation_date": 1485088347, "answer_id": 41790921, "question_id": 39882204, "content_license": "CC BY-SA 4.0", "body": "<p>When you change the scheduler toggle to &quot;on&quot; for a DAG, the scheduler will trigger a backfill of all dag run instances for which it has no status recorded, starting with the start_date you specify in your &quot;default_args&quot;.</p>\n<p>For example: If the start date was &quot;2017-01-21&quot; and you turned on the scheduling toggle at &quot;2017-01-22T00:00:00&quot; and your dag was configured to run hourly, then the scheduler will backfill 24 dag runs and then start running on the scheduled interval.</p>\n<p>This is essentially what is happening in both of your question. In #1, it is filling in the 3 missing runs from the 30 seconds which you turned off the scheduler. In #2, it is filling in all of the DAG runs from start_date until &quot;now&quot;.</p>\n<p>There are 2 ways around this:</p>\n<ol>\n<li>Set the start_date to a date in the future so that it will only start scheduling dag runs once that date is reached. Note that if you change the start_date of a DAG, you must change the name of the DAG as well due to the way the start date is stored in airflow's DB.</li>\n<li>Manually run backfill <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#backfill\" rel=\"noreferrer\">from the command line</a> with the &quot;-m&quot; (--mark-success) flag which tells airflow not to actually run the DAG, rather just mark it as successful in the DB.</li>\n</ol>\n<p>e.g.</p>\n<pre><code>airflow backfill MY_tutorial -m -s 2016-10-04 -e 2017-01-22T14:28:30\n</code></pre>\n"}, {"owner": {"account_id": 3986966, "reputation": 6040, "user_id": 3287250, "user_type": "registered", "profile_image": "https://i.sstatic.net/nVoOs.jpg?s=256", "display_name": "Simon D", "link": "https://stackoverflow.com/users/3287250/simon-d"}, "is_accepted": false, "score": 70, "last_activity_date": 1721651455, "last_edit_date": 1721651455, "creation_date": 1520947635, "answer_id": 49257418, "question_id": 49231340, "content_license": "CC BY-SA 4.0", "body": "<p>You've put the <code>'max_active_runs': 1</code> into the <code>default_args</code> parameter and not into the correct spot.</p>\n<p><code>max_active_runs</code> is a constructor argument for a DAG and should not be put into the <code>default_args</code> dictionary.</p>\n<p>Here is an example DAG that shows where you need to move it to:</p>\n<pre class=\"lang-py prettyprint-override\"><code>dag_args = { \n    'owner': 'Owner',\n    # 'max_active_runs': 1, # &lt;--- Here is where you had it.\n    'depends_on_past': False,                          # |\n    'start_date': datetime(2018, 01, 1, 12, 00),       # |\n    'email_on_failure': False                          # |\n}                                                      # |\n                                                       # |\nsched = timedelta(hours=1)                             # |\ndag = DAG(                                             # |\n          job_id,                                      # |\n          default_args=dag_args,                       # |\n          schedule_interval=sched,                     # V\n          max_active_runs=1 # &lt;---- Here is where it is supposed to be\n      ) \n</code></pre>\n<p>If the tasks that your dag is running are actually sub-dags then you may need to pass <code>max_active_runs</code> into the subdags too but not 100% sure on this.</p>\n"}, {"owner": {"account_id": 13101295, "reputation": 2526, "user_id": 9465561, "user_type": "registered", "profile_image": "https://i.sstatic.net/Npc92.jpg?s=256", "display_name": "Ryan Yuan", "link": "https://stackoverflow.com/users/9465561/ryan-yuan"}, "is_accepted": true, "score": 70, "last_activity_date": 1551223665, "creation_date": 1551223665, "answer_id": 54895796, "question_id": 54894418, "content_license": "CC BY-SA 4.0", "body": "<ol>\n<li>Pass a dict object to <em>op_kwargs</em></li>\n<li><p>Use the keys to access their value from <em>kwargs</em> dict in your python callable</p>\n\n<pre><code>def SendEmail(**kwargs):\n    print(kwargs['key1'])\n    print(kwargs['key2'])\n    msg = MIMEText(\"The pipeline for client1 is completed, please check.\")\n    msg['Subject'] = \"xxxx\"\n    msg['From'] = \"xxxx\"\n    ......\n    s = smtplib.SMTP('localhost')\n    s.send_message(msg)\n    s.quit()\n\n\nt5_send_notification = PythonOperator(\n    task_id='t5_send_notification',\n    provide_context=True,\n    python_callable=SendEmail,\n    op_kwargs={'key1': 'value1', 'key2': 'value2'},\n    dag=dag,\n)\n</code></pre></li>\n</ol>\n"}, {"owner": {"account_id": 3920925, "reputation": 1316, "user_id": 3243108, "user_type": "registered", "accept_rate": 50, "profile_image": "https://www.gravatar.com/avatar/f847be91df2dfacbb13ef910625a1145?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "skwon", "link": "https://stackoverflow.com/users/3243108/skwon"}, "is_accepted": false, "score": 70, "last_activity_date": 1638960212, "last_edit_date": 1638960212, "creation_date": 1600428921, "answer_id": 63955004, "question_id": 56370720, "content_license": "CC BY-SA 4.0", "body": "<p>An illustration for three major concurrency control variables:</p>\n<p><a href=\"https://i.sstatic.net/P9jbQ.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/P9jbQ.png\" alt=\"an illustration\" /></a></p>\n<p>From airflow version 2.2, <code>task_concurrency</code> parameter is deprecated by <code>max_active_tis_per_dag</code>.</p>\n<p><a href=\"https://airflow.apache.org/docs/stable/faq.html#how-can-my-airflow-dag-run-faster\" rel=\"noreferrer\">https://airflow.apache.org/docs/stable/faq.html#how-can-my-airflow-dag-run-faster</a></p>\n"}, {"owner": {"account_id": 7731610, "reputation": 656, "user_id": 5854561, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/07c801e2e39318c494ad0d3fd7efa115?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Erik Schuchmann", "link": "https://stackoverflow.com/users/5854561/erik-schuchmann"}, "is_accepted": true, "score": 64, "last_activity_date": 1617663426, "last_edit_date": 1617663426, "creation_date": 1465529720, "answer_id": 37739468, "question_id": 36730714, "content_license": "CC BY-SA 4.0", "body": "<p>The <code>BashOperator</code>'s <code>bash_command</code> <em>argument</em> is a <em>template</em>. You can access <code>execution_date</code> in any template as a <code>datetime</code> <em>object</em> using the <code>execution_date</code> variable. In the template, you can use any <code>jinja2</code> methods to manipulate it.</p>\n<p>Using the following as your <code>BashOperator</code> <code>bash_command</code> <em>string</em>:</p>\n<pre><code># pass in the first of the current month\nsome_command.sh {{ execution_date.replace(day=1) }}\n\n# last day of previous month\nsome_command.sh {{ execution_date.replace(day=1) - macros.timedelta(days=1) }}\n</code></pre>\n<p>If you just want the string equivalent of the execution date, <code>ds</code> will return a datestamp (YYYY-MM-DD), <code>ds_nodash</code> returns same without dashes (YYYYMMDD), etc.  More on <code>macros</code> is available in the <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/macros-ref.html\" rel=\"noreferrer\">Api Docs</a>.</p>\n<hr />\n<p>Your final operator would look like:</p>\n<pre><code>command = &quot;&quot;&quot;curl -XPOST '%(hostname)s:8000/run?st={{ ds }}'&quot;&quot;&quot; % locals()\nt1 = BashOperator( task_id='rest-api-1', bash_command=command, dag=dag)\n</code></pre>\n"}, {"owner": {"account_id": 7489236, "reputation": 18632, "user_id": 5691525, "user_type": "registered", "accept_rate": 78, "profile_image": "https://i.sstatic.net/i3e64.jpg?s=256", "display_name": "kaxil", "link": "https://stackoverflow.com/users/5691525/kaxil"}, "is_accepted": true, "score": 63, "last_activity_date": 1692700822, "last_edit_date": 1692700822, "creation_date": 1534235476, "answer_id": 51837049, "question_id": 51829200, "content_license": "CC BY-SA 4.0", "body": "<p><strong>Setting up SMTP Server for Airflow Email alerts using Gmail</strong>:</p>\n<p>Create an email id from which you want to send alerts about DAG failure or if you want to use <strong>EmailOperator</strong>. Edit <code>airflow.cfg</code> file to edit the smtp details for the mail server.</p>\n<p>For demo you can use any gmail account.</p>\n<p>Create a google App Password for your gmail account (<a href=\"https://support.google.com/accounts/answer/185833\" rel=\"nofollow noreferrer\">Instruction here</a>). This is done so that you don't use your original password or 2 Factor authentication.</p>\n<ol>\n<li>Visit your <a href=\"https://security.google.com/settings/security/apppasswords\" rel=\"nofollow noreferrer\">App passwords</a> page. You may be asked to sign in to your Google Account.</li>\n<li>Under &quot;Your app passwords&quot;, click <strong>Select app</strong> and choose &quot;Mail&quot;.</li>\n<li>Click <strong>Select device</strong> and choose &quot;Other (Custom name)&quot; so that you can input &quot;Airflow&quot;.</li>\n<li>Select <strong>Generate</strong>.</li>\n<li>Copy the generated App password (the 16 character code in the yellow bar), for example xxxxyyyyxxxxyyyy.</li>\n<li>Select <strong>Done</strong>.</li>\n</ol>\n<p>Once you are finished, you won\u2019t see that App password code again. However, you will see a list of apps and devices (which you've created App passwords for) in your Google Account.</p>\n<p>Edit <code>airflow.cfg</code> and edit the <code>[smtp]</code> section as shown below:</p>\n<pre><code>[smtp]\nsmtp_host = smtp.gmail.com\nsmtp_starttls = True\nsmtp_ssl = False\nsmtp_user = YOUR_EMAIL_ADDRESS\nsmtp_password = xxxxyyyyxxxxyyyy\nsmtp_port = 587\nsmtp_mail_from = YOUR_EMAIL_ADDRESS\n</code></pre>\n<p>Edit the below parameters to the corresponding values:</p>\n<p><code>YOUR_EMAIL_ADDRESS</code> = Your Gmail address<br />\n<code>xxxxyyyyxxxxyyyy</code> = The App password generated above</p>\n"}, {"owner": {"account_id": 10770356, "reputation": 766, "user_id": 7923985, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/eaac887df7c9abe0eaeae7f3714ef7c1?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Jean S", "link": "https://stackoverflow.com/users/7923985/jean-s"}, "is_accepted": true, "score": 61, "last_activity_date": 1493382505, "creation_date": 1493382505, "answer_id": 43680277, "question_id": 43678408, "content_license": "CC BY-SA 3.0", "body": "<p>You have to use <a href=\"https://airflow.incubator.apache.org/concepts.html#trigger-rules\" rel=\"noreferrer\">airflow trigger rules</a></p>\n\n<p>All operators have a trigger_rule argument which defines the rule by which the generated task get triggered. </p>\n\n<p>The trigger rule possibilities:</p>\n\n<pre><code>ALL_SUCCESS = 'all_success'\nALL_FAILED = 'all_failed'\nALL_DONE = 'all_done'\nONE_SUCCESS = 'one_success'\nONE_FAILED = 'one_failed'\nDUMMY = 'dummy'\n</code></pre>\n\n<p>Here is the idea to solve your problem: </p>\n\n<pre><code>from airflow.operators.ssh_execute_operator import SSHExecuteOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.contrib.hooks import SSHHook\n\nsshHook = SSHHook(conn_id=&lt;YOUR CONNECTION ID FROM THE UI&gt;)\n\ntask_1 = SSHExecuteOperator(\n        task_id='task_1',\n        bash_command=&lt;YOUR COMMAND&gt;,\n        ssh_hook=sshHook,\n        dag=dag)\n\ntask_2 = SSHExecuteOperator(\n        task_id='conditional_task',\n        bash_command=&lt;YOUR COMMAND&gt;,\n        ssh_hook=sshHook,\n        dag=dag)\n\ntask_2a = SSHExecuteOperator(\n        task_id='task_2a',\n        bash_command=&lt;YOUR COMMAND&gt;,\n        trigger_rule=TriggerRule.ALL_SUCCESS,\n        ssh_hook=sshHook,\n        dag=dag)\n\ntask_2b = SSHExecuteOperator(\n        task_id='task_2b',\n        bash_command=&lt;YOUR COMMAND&gt;,\n        trigger_rule=TriggerRule.ALL_FAILED,\n        ssh_hook=sshHook,\n        dag=dag)\n\ntask_3 = SSHExecuteOperator(\n        task_id='task_3',\n        bash_command=&lt;YOUR COMMAND&gt;,\n        trigger_rule=TriggerRule.ONE_SUCCESS,\n        ssh_hook=sshHook,\n        dag=dag)\n\n\ntask_2.set_upstream(task_1)\ntask_2a.set_upstream(task_2)\ntask_2b.set_upstream(task_2)\ntask_3.set_upstream(task_2a)\ntask_3.set_upstream(task_2b)\n</code></pre>\n"}, {"owner": {"account_id": 369948, "reputation": 5124, "user_id": 717600, "user_type": "registered", "accept_rate": 32, "profile_image": "https://i.sstatic.net/yYVqR.jpg?s=256", "display_name": "Md Sirajus Salayhin", "link": "https://stackoverflow.com/users/717600/md-sirajus-salayhin"}, "is_accepted": true, "score": 61, "last_activity_date": 1596233818, "last_edit_date": 1596233818, "creation_date": 1530878044, "answer_id": 51209922, "question_id": 51122849, "content_license": "CC BY-SA 4.0", "body": "<p>I have uninstalled Apache Airflow and installed it with the <code>sudo</code> command (i.e., <code>sudo pip install apache-airflow</code>). After that it worked fine.</p>\n"}], "has_more": true, "quota_max": 300, "quota_remaining": 293}