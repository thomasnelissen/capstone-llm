{"title":"Airflow: how to delete a DAG?","question":"<p>I have started the Airflow webserver and scheduled some dags. I can see the dags on web GUI.</p>\n\n<p>How can I delete a particular DAG from being run and shown in web GUI? Is there an Airflow CLI command to do that?</p>\n\n<p>I looked around but could not find an answer for a simple way of deleting a DAG once it has been loaded and scheduled.</p>\n","answer":"<p><strong>Edit 8/27/18 - Airflow 1.10 is now released on PyPI!</strong></p>\n\n<p><a href=\"https://pypi.org/project/apache-airflow/1.10.0/\" rel=\"noreferrer\">https://pypi.org/project/apache-airflow/1.10.0/</a></p>\n\n<hr>\n\n<h2>How to delete a DAG completely</h2>\n\n<p>We have this feature now in Airflow â‰¥ 1.10!</p>\n\n<p>The PR <a href=\"https://github.com/apache/incubator-airflow/pull/2199\" rel=\"noreferrer\">#2199</a> (Jira: <a href=\"https://issues.apache.org/jira/browse/AIRFLOW-1002\" rel=\"noreferrer\">AIRFLOW-1002</a>) adding DAG removal to Airflow has now been merged which allows fully deleting a DAG's entries from all of the related tables.</p>\n\n<p>The core <a href=\"https://github.com/apache/incubator-airflow/blob/7488f2938da4e08645060531aa363204db7f50a5/airflow/api/common/experimental/delete_dag.py#L28-L55\" rel=\"noreferrer\">delete_dag(...)</a> code is now part of the experimental API, and there are entrypoints available <a href=\"https://github.com/apache/incubator-airflow/blob/9dba430b683361fc0ed7f50de6daa03c971a476b/airflow/bin/cli.py#L214-L232\" rel=\"noreferrer\">via the CLI</a> and also <a href=\"https://github.com/apache/incubator-airflow/blob/7488f2938da4e08645060531aa363204db7f50a5/airflow/www/api/experimental/endpoints.py#L89-L103\" rel=\"noreferrer\">via the REST API</a>.</p>\n\n<p>CLI:</p>\n\n<pre><code>airflow delete_dag my_dag_id\n</code></pre>\n\n<p>REST API (running webserver locally):</p>\n\n<pre><code>curl -X \"DELETE\" http://127.0.0.1:8080/api/experimental/dags/my_dag_id\n</code></pre>\n\n<hr>\n\n<p><strong>Warning regarding the REST API</strong>: Ensure that your Airflow cluster <a href=\"https://airflow.apache.org/api.html#authentication\" rel=\"noreferrer\">uses authentication</a> in production.</p>\n\n<h2>Installing / upgrading to Airflow 1.10 (current)</h2>\n\n<p>To upgrade, run either:</p>\n\n<pre><code>export SLUGIFY_USES_TEXT_UNIDECODE=yes\n</code></pre>\n\n<p>or:</p>\n\n<pre><code>export AIRFLOW_GPL_UNIDECODE=yes\n</code></pre>\n\n<p>Then:</p>\n\n<pre><code>pip install -U apache-airflow\n</code></pre>\n\n<p>Remember to check <a href=\"https://github.com/apache/incubator-airflow/blob/master/UPDATING.md\" rel=\"noreferrer\">UPDATING.md</a> first for the full details!</p>\n"}
{"title":"execution_date in airflow: need to access as a variable","question":"<p>I am really a newbie in this forum. But I have been playing with airflow, for sometime, for our company.  Sorry if this question sounds really dumb.</p>\n<p>I am writing a pipeline using bunch of BashOperators.\nBasically, for each Task, I want to simply call a REST api using 'curl'</p>\n<p>This is what my pipeline looks like(very simplified version):</p>\n<pre class=\"lang-py prettyprint-override\"><code>from airflow import DAG\nfrom airflow.operators import BashOperator, PythonOperator\nfrom dateutil import tz\nimport datetime\n\ndatetime_obj = datetime.datetime\n                                  \ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime.datetime.combine(datetime_obj.today() - datetime.timedelta(1), datetime_obj.min.time()),\n    'email': ['xxxx@xxx.xxx'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 2,\n    'retry_delay': datetime.timedelta(minutes=5),\n}\n\n\ncurrent_datetime = datetime_obj.now(tz=tz.tzlocal())\n\ndag = DAG(\n    'test_run', default_args=default_args, schedule_interval=datetime.timedelta(minutes=60))\n\ncurl_cmd='curl -XPOST &quot;'+hostname+':8000/run?st='+current_datetime +'&quot;'\n\n\nt1 = BashOperator(\n    task_id='rest-api-1',\n    bash_command=curl_cmd,\n    dag=dag)\n</code></pre>\n<p>If you notice I am doing <code>current_datetime= datetime_obj.now(tz=tz.tzlocal())</code>\nInstead what I want here is <em><strong>'execution_date'</strong></em></p>\n<p>How do I use <strong>'execution_date'</strong> directly and assign it to a variable in my python file?</p>\n<p>I have having this general issue of accessing args.\nAny help will be genuinely appreciated.</p>\n<p>Thanks</p>\n","answer":"<p>The <code>BashOperator</code>'s <code>bash_command</code> <em>argument</em> is a <em>template</em>. You can access <code>execution_date</code> in any template as a <code>datetime</code> <em>object</em> using the <code>execution_date</code> variable. In the template, you can use any <code>jinja2</code> methods to manipulate it.</p>\n<p>Using the following as your <code>BashOperator</code> <code>bash_command</code> <em>string</em>:</p>\n<pre><code># pass in the first of the current month\nsome_command.sh {{ execution_date.replace(day=1) }}\n\n# last day of previous month\nsome_command.sh {{ execution_date.replace(day=1) - macros.timedelta(days=1) }}\n</code></pre>\n<p>If you just want the string equivalent of the execution date, <code>ds</code> will return a datestamp (YYYY-MM-DD), <code>ds_nodash</code> returns same without dashes (YYYYMMDD), etc.  More on <code>macros</code> is available in the <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/macros-ref.html\" rel=\"noreferrer\">Api Docs</a>.</p>\n<hr />\n<p>Your final operator would look like:</p>\n<pre><code>command = &quot;&quot;&quot;curl -XPOST '%(hostname)s:8000/run?st={{ ds }}'&quot;&quot;&quot; % locals()\nt1 = BashOperator( task_id='rest-api-1', bash_command=command, dag=dag)\n</code></pre>\n"}
{"title":"Apache Airflow or Apache Beam for data processing and job scheduling","question":"<p>I'm trying to give useful information but I am far from being a data engineer.</p>\n\n<p>I am currently using the python library pandas to execute a long series of transformation to my data which has a lot of inputs (currently CSV and excel files). The outputs are several excel files. I would like to be able to execute scheduled monitored batch jobs with parallel computation (I mean not as sequential as what I'm doing with pandas), once a month.</p>\n\n<p>I don't really know Beam or Airflow, I quickly read through the docs and it seems that both can achieve that. Which one should I use ? </p>\n","answer":"<p>The other answers are quite technical and hard to understand. I was in your position before so I'll explain in <em>simple terms</em>.</p>\n<p><strong>Airflow</strong> can do <em>anything</em>. It has <code>BashOperator</code> and <code>PythonOperator</code> which means it can run any bash script or any Python script.<br />\nIt is a way to organize (setup complicated data pipeline DAGs), schedule, monitor, trigger re-runs of data pipelines, in a easy-to-view and use UI.<br />\nAlso, it is easy to setup and everything is in familiar Python code.<br />\nDoing pipelines in an organized manner (i.e using Airflow) means you don't waste time debugging a mess of data processing (<code>cron</code>) scripts all over the place.<br />\nNowadays (roughly year 2020 onwards), we call it an <strong>orchestration</strong> tool.</p>\n<p><strong>Apache Beam</strong> is a wrapper for the many data processing frameworks (Spark, Flink etc.) out there.<br />\nThe intent is so you just learn Beam and can run on multiple backends (Beam runners).<br />\nIf you are familiar with Keras and TensorFlow/Theano/Torch, the relationship between Keras and its backends is similar to the relationship between Beam and its data processing backends.</p>\n<p>Google Cloud Platform's Cloud Dataflow is one backend for running Beam on.<br />\nThey call it the <a href=\"https://beam.apache.org/documentation/runners/dataflow/\" rel=\"noreferrer\">Dataflow runner</a>.</p>\n<p>GCP's offering, <strong>Cloud Composer</strong>, is a <strong>managed Airflow implementation</strong> as a service, running in a Kubernetes cluster in Google Kubernetes Engine (GKE).</p>\n<p>So you can either:</p>\n<ul>\n<li>manual Airflow implementation, doing data processing on the instance itself (if your data is small (or your instance is powerful enough), you can process data on the machine running Airflow. This is why many are confused if Airflow can process data or not)</li>\n<li>manual Airflow implementation calling Beam jobs</li>\n<li>Cloud Composer (managed Airflow as a service) calling jobs in Cloud Dataflow</li>\n<li>Cloud Composer running data processing containers in Composer's Kubernetes cluster environment itself, using Airflow's <code>KubernetesPodOperator (KPO)</code></li>\n<li>Cloud Composer running data processing containers in Composer's Kubernetes cluster environment with Airflow's <code>KPO</code>, but this time in a better <strong>isolated</strong> fashion by creating a new node-pool and specifying that the <code>KPO</code> pods are to be run in the new node-pool</li>\n</ul>\n<p><strong>My personal experience</strong>:<br />\nAirflow is lightweight and not difficult to learn (easy to implement), you should use it for your data pipelines whenever possible.<br />\nAlso, since many companies are looking for experience using Airflow, if you're looking to be a data engineer you should probably learn it<br />\nAlso, managed Airflow (I've only used GCP's Composer so far) is much more convenient than running Airflow yourself, and managing the airflow <code>webserver</code> and <code>scheduler</code> processes.</p>\n"}
{"title":"How to create a conditional task in Airflow","question":"<p>I would like to create a conditional task in Airflow as described in the schema below. The expected scenario is the following:</p>\n\n<ul>\n<li>Task 1 executes</li>\n<li>If Task 1 succeed, then execute Task 2a</li>\n<li>Else If Task 1 fails, then execute Task 2b</li>\n<li>Finally execute Task 3</li>\n</ul>\n\n<p><a href=\"https://i.sstatic.net/VS1Wm.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/VS1Wm.png\" alt=\"Conditional Task\"></a>\nAll tasks above are SSHExecuteOperator.\nI'm guessing I should be using the ShortCircuitOperator and / or XCom to manage the condition but I am not clear on how to implement that. Could you please describe the solution?</p>\n","answer":"<p>You have to use <a href=\"https://airflow.incubator.apache.org/concepts.html#trigger-rules\" rel=\"noreferrer\">airflow trigger rules</a></p>\n\n<p>All operators have a trigger_rule argument which defines the rule by which the generated task get triggered. </p>\n\n<p>The trigger rule possibilities:</p>\n\n<pre><code>ALL_SUCCESS = 'all_success'\nALL_FAILED = 'all_failed'\nALL_DONE = 'all_done'\nONE_SUCCESS = 'one_success'\nONE_FAILED = 'one_failed'\nDUMMY = 'dummy'\n</code></pre>\n\n<p>Here is the idea to solve your problem: </p>\n\n<pre><code>from airflow.operators.ssh_execute_operator import SSHExecuteOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.contrib.hooks import SSHHook\n\nsshHook = SSHHook(conn_id=&lt;YOUR CONNECTION ID FROM THE UI&gt;)\n\ntask_1 = SSHExecuteOperator(\n        task_id='task_1',\n        bash_command=&lt;YOUR COMMAND&gt;,\n        ssh_hook=sshHook,\n        dag=dag)\n\ntask_2 = SSHExecuteOperator(\n        task_id='conditional_task',\n        bash_command=&lt;YOUR COMMAND&gt;,\n        ssh_hook=sshHook,\n        dag=dag)\n\ntask_2a = SSHExecuteOperator(\n        task_id='task_2a',\n        bash_command=&lt;YOUR COMMAND&gt;,\n        trigger_rule=TriggerRule.ALL_SUCCESS,\n        ssh_hook=sshHook,\n        dag=dag)\n\ntask_2b = SSHExecuteOperator(\n        task_id='task_2b',\n        bash_command=&lt;YOUR COMMAND&gt;,\n        trigger_rule=TriggerRule.ALL_FAILED,\n        ssh_hook=sshHook,\n        dag=dag)\n\ntask_3 = SSHExecuteOperator(\n        task_id='task_3',\n        bash_command=&lt;YOUR COMMAND&gt;,\n        trigger_rule=TriggerRule.ONE_SUCCESS,\n        ssh_hook=sshHook,\n        dag=dag)\n\n\ntask_2.set_upstream(task_1)\ntask_2a.set_upstream(task_2)\ntask_2b.set_upstream(task_2)\ntask_3.set_upstream(task_2a)\ntask_3.set_upstream(task_2b)\n</code></pre>\n"}
{"title":"How to prevent airflow from backfilling dag runs?","question":"<p>Say you have an airflow DAG that doesn't make sense to backfill, meaning that, after it's run once, running it subsequent times quickly would be completely pointless.</p>\n\n<p>For example, if you're loading data from some source that is only updated hourly into your database, backfilling, which occurs in rapid succession, would just be importing the same data again and again.</p>\n\n<p>This is especially annoying when you instantiate a new hourly task, and it runs <code>N</code> amount of times for each hour it missed, doing redundant work, before it starts running on the interval you specified.</p>\n\n<p>The only solution I can think of is something that they specifically advised against in <a href=\"https://airflow.apache.org/faq.html\" rel=\"noreferrer\">FAQ of the docs</a></p>\n\n<blockquote>\n  <p>We recommend against using dynamic values as start_date, especially <code>datetime.now()</code> as it can be quite confusing.</p>\n</blockquote>\n\n<p>Is there any way to disable backfilling for a DAG, or should I do the above?</p>\n","answer":"<p>Upgrade to airflow version 1.8 and use catchup_by_default=False in the airflow.cfg or apply catchup=False to each of your dags.</p>\n\n<p><a href=\"https://github.com/apache/incubator-airflow/blob/master/UPDATING.md#catchup_by_default\" rel=\"noreferrer\">https://github.com/apache/incubator-airflow/blob/master/UPDATING.md#catchup_by_default</a></p>\n"}
{"title":"First time login to Apache Airflow asks for username and password, what is the username and password?","question":"<p>I've just installed Apache Airflow, and I'm launching the webserver for the first time, and it asks me for username and password, I haven't set any username or password.</p>\n<p>Can you let me know what is the default username and password for airflow?</p>\n<p><a href=\"https://i.sstatic.net/bxcdr.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/bxcdr.png\" alt=\"enter image description here\" /></a></p>\n","answer":"<p>There is no default username and password created if you are just using python wheel.</p>\n<p>Run the following to create a user:</p>\n<p><strong>For Airflow &gt;=2.0.0</strong>:</p>\n<pre><code>airflow users  create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin\n</code></pre>\n<p>OR</p>\n<p><strong>For Airflow &lt;1.10.14</strong>:</p>\n<pre><code>airflow create_user -r Admin -u admin -e admin@example.com -f admin -l user -p admin\n</code></pre>\n"}
{"title":"How to control the parallelism or concurrency of an Airflow installation?","question":"<p>In some of my Apache Airflow installations, DAGs or tasks that are scheduled to run do not run even when the scheduler doesn't appear to be fully loaded. How can I increase the number of DAGs or tasks that can run concurrently?</p>\n<p>Similarly, if my installation is under high load and I want to limit how quickly my Airflow workers pull queued tasks (such as to reduce resource consumption), what can I adjust to reduce the average load?</p>\n","answer":"<p>Here's an expanded list of configuration options that are available since Airflow v1.10.2. Some can be set on a per-DAG or per-operator basis, but may also fall back to the setup-wide defaults when they are not specified.</p>\n<hr />\n<p>Options that can be specified <strong>on a per-DAG basis</strong>:</p>\n<ul>\n<li><code>concurrency</code>: the number of task instances allowed to run concurrently across all active runs of the DAG this is set on. Defaults to <code>core.dag_concurrency</code> if not set</li>\n<li><code>max_active_runs</code>: maximum number of active runs for this DAG. The scheduler will not create new active DAG runs once this limit is hit. Defaults to <code>core.max_active_runs_per_dag</code> if not set</li>\n</ul>\n<p>Examples:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Only allow one run of this DAG to be running at any given time\ndag = DAG('my_dag_id', max_active_runs=1)\n\n# Allow a maximum of 10 tasks to be running across a max of 2 active DAG runs\ndag = DAG('example2', concurrency=10, max_active_runs=2)\n</code></pre>\n<hr />\n<p>Options that can be specified <strong>on a per-operator basis</strong>:</p>\n<ul>\n<li><code>pool</code>: the pool to execute the task in. <a href=\"https://airflow.apache.org/concepts.html#pools\" rel=\"noreferrer\">Pools</a> can be used to limit parallelism for <em>only a subset</em> of tasks</li>\n<li><code>max_active_tis_per_dag</code>: controls the number of concurrent running task instances across dag_runs per task.</li>\n</ul>\n<p>Example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>t1 = BaseOperator(pool='my_custom_pool', max_active_tis_per_dag=12)\n</code></pre>\n<hr />\n<p>Options that are specified <strong>across an entire Airflow setup</strong>:</p>\n<ul>\n<li><code>core.parallelism</code>: maximum number of tasks running across an entire Airflow installation</li>\n<li><code>core.dag_concurrency</code>: max number of tasks that can be running per DAG (across multiple <em>DAG runs</em>)</li>\n<li><code>core.non_pooled_task_slot_count</code>: number of task slots allocated to tasks not running in a pool</li>\n<li><code>core.max_active_runs_per_dag</code>: maximum number of active DAG <em>runs</em>, per DAG</li>\n<li><code>scheduler.max_threads</code>: how many threads the scheduler process should use to use to schedule DAGs</li>\n<li><code>celery.worker_concurrency</code>: max number of task instances that a worker will process at a time <em>if using CeleryExecutor</em></li>\n<li><code>celery.sync_parallelism</code>: number of processes CeleryExecutor should use to sync task state</li>\n</ul>\n"}
{"title":"Writing to Airflow Logs","question":"<p>One way to write to the logs in Airflow is to return a string from a PythonOperator like on line 44 <a href=\"https://github.com/apache/incubator-airflow/blob/ff45d8f2218a8da9328161aa66d004c3db3b367e/airflow/example_dags/example_python_operator.py\" rel=\"noreferrer\">here</a>.</p>\n\n<p>Are there other ways that allow me to write to the airflow log files? I've found that print statements are not saved to the logs.</p>\n","answer":"<p>You can import the logging module into your code and write to logs that way</p>\n\n<pre><code>import logging\n\nlogging.info('Hello')\n</code></pre>\n\n<p>Here are some more options</p>\n\n<pre><code>import logging    \n\nlogging.debug('This is a debug message')\nlogging.info('This is an info message')\nlogging.warning('This is a warning message')\nlogging.error('This is an error message')\nlogging.critical('This is a critical message')\n</code></pre>\n"}
{"title":"Airflow - How to pass xcom variable into Python function","question":"<p>I need to reference a variable that's returned by a <code>BashOperator</code>. In my <code>task_archive_s3_file</code>, I need to get the filename from <code>get_s3_file</code>. The task simply prints <code>{{ ti.xcom_pull(task_ids=submit_file_to_spark) }}</code> as a string instead of the value.</p>\n<p>If I use the <code>bash_command</code>, the value prints correctly.</p>\n<pre class=\"lang-py prettyprint-override\"><code>get_s3_file = PythonOperator(\n    task_id='get_s3_file',\n    python_callable=obj.func_get_s3_file,\n    trigger_rule=TriggerRule.ALL_SUCCESS,\n    dag=dag)\n\nsubmit_file_to_spark = BashOperator(\n    task_id='submit_file_to_spark',\n    bash_command=&quot;echo 'hello world'&quot;,\n    trigger_rule=&quot;all_done&quot;,\n    xcom_push=True,\n    dag=dag)\n\ntask_archive_s3_file = PythonOperator(\n    task_id='archive_s3_file',\n#    bash_command=&quot;echo {{ ti.xcom_pull(task_ids='submit_file_to_spark') }}&quot;,\n    python_callable=obj.func_archive_s3_file,\n    params={'s3_path_filename': &quot;{{ ti.xcom_pull(task_ids=submit_file_to_spark) }}&quot; },\n    dag=dag)\n\nget_s3_file &gt;&gt; submit_file_to_spark &gt;&gt; task_archive_s3_file\n</code></pre>\n","answer":"<p>Templates like <code>{{ ti.xcom_pull(...) }}</code> can only be used inside of parameters that support templates or they won't be rendered prior to execution. See the <code>template_fields</code>, <code>template_fields_renderers</code> and <code>template_ext</code> attributes of the <a href=\"https://github.com/apache/airflow/blob/main/airflow/operators/python.py#L124-L125\" rel=\"noreferrer\">PythonOperator</a> and <a href=\"https://github.com/apache/airflow/blob/main/airflow/operators/bash.py#L125-L130\" rel=\"noreferrer\">BashOperator</a>.</p>\n<p>So <code>templates_dict</code> is what you use to pass templates to your python operator:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def func_archive_s3_file(**context):\n    archive(context['templates_dict']['s3_path_filename'])\n\ntask_archive_s3_file = PythonOperator(\n    task_id='archive_s3_file',\n    dag=dag,\n    python_callable=obj.func_archive_s3_file,\n    provide_context=True,  # must pass this because templates_dict gets passed via context\n    templates_dict={'s3_path_filename': &quot;{{ ti.xcom_pull(task_ids='submit_file_to_spark') }}&quot; })\n</code></pre>\n<p>However in the case of fetching an XCom value, another alternative is just using the <code>TaskInstance</code> object made available to you via context:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def func_archive_s3_file(**context):\n    archive(context['ti'].xcom_pull(task_ids='submit_file_to_spark'))\n\ntask_archive_s3_file = PythonOperator(\n    task_id='archive_s3_file',\n    dag=dag,\n    python_callable=obj.func_archive_s3_file,\n    provide_context=True,\n</code></pre>\n"}
{"title":"Airflow s3 connection using UI","question":"<p>I've been trying to use Airflow to schedule a DAG.\nOne of the DAG includes a task which loads data from s3 bucket.</p>\n\n<p>For the purpose above I need to setup s3 connection. But UI provided by airflow isn't that intutive (<a href=\"http://pythonhosted.org/airflow/configuration.html?highlight=connection#connections\" rel=\"noreferrer\">http://pythonhosted.org/airflow/configuration.html?highlight=connection#connections</a>). Any one succeeded setting up the s3 connection if so are there any best practices you folks follow?</p>\n\n<p>Thanks.</p>\n","answer":"<p>EDIT: This answer stores your secret key in <em>plain text</em> which can be a <strong>security risk</strong> and is not recommended. The best way is to put access key and secret key in the login/password fields, as mentioned in other answers below.\nEND EDIT</p>\n<p>It's hard to find references, but after digging a bit  I was able to make it work.</p>\n<h3>TLDR</h3>\n<p>Create a new connection with the following attributes:</p>\n<p><strong>Conn Id:</strong> my_conn_S3</p>\n<p><strong>Conn Type:</strong> S3</p>\n<p><strong>Extra:</strong></p>\n<pre><code>{&quot;aws_access_key_id&quot;:&quot;_your_aws_access_key_id_&quot;, &quot;aws_secret_access_key&quot;: &quot;_your_aws_secret_access_key_&quot;}\n</code></pre>\n<hr />\n<h3>Long version, setting up UI connection:</h3>\n<ul>\n<li>On Airflow UI, go to Admin &gt; Connections</li>\n<li>Create a new connection with the following attributes:</li>\n<li>Conn Id: <code>my_conn_S3</code></li>\n<li>Conn Type: <code>S3</code></li>\n<li>Extra: <code>{&quot;aws_access_key_id&quot;:&quot;_your_aws_access_key_id_&quot;, &quot;aws_secret_access_key&quot;: &quot;_your_aws_secret_access_key_&quot;}</code></li>\n<li>Leave all the other fields (Host, Schema, Login) blank.</li>\n</ul>\n<p>To use this connection, below you can find a simple S3 Sensor Test. The idea of this test is to set up a sensor that watches files in S3 (T1 task) and once below condition is satisfied it triggers a bash command (T2 task).</p>\n<h3>Testing</h3>\n<ul>\n<li>Before running the DAG, ensure you've an S3 bucket named 'S3-Bucket-To-Watch'.</li>\n<li>Add below s3_dag_test.py to airflow dags folder (~/airflow/dags)</li>\n<li>Start <code>airflow webserver</code>.</li>\n<li>Go to Airflow UI (http://localhost:8383/)</li>\n<li>Start <code>airflow scheduler</code>.</li>\n<li>Turn on 's3_dag_test' DAG on the main DAGs view.</li>\n<li>Select 's3_dag_test' to show the dag details.</li>\n<li>On the Graph View you should be able to see it's current state.</li>\n<li>'check_s3_for_file_in_s3' task should be active and running.</li>\n<li>Now, add a file named 'file-to-watch-1' to your 'S3-Bucket-To-Watch'.</li>\n<li>First tasks should have been completed, second should be started and finish.</li>\n</ul>\n<p>The schedule_interval in the dag definition is set to '@once', to facilitate debugging.</p>\n<p>To run it again, leave everything as it's, remove files in the bucket and try  again by selecting the first task (in the graph view) and selecting 'Clear' all 'Past','Future','Upstream','Downstream' .... activity.  This should kick off the DAG again.</p>\n<p>Let me know how it went.</p>\n<h3>s3_dag_test.py ;</h3>\n<pre><code>&quot;&quot;&quot;\nS3 Sensor Connection Test\n&quot;&quot;&quot;\n\nfrom airflow import DAG\nfrom airflow.operators import SimpleHttpOperator, HttpSensor,   BashOperator, EmailOperator, S3KeySensor\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2016, 11, 1),\n    'email': ['something@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 5,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG('s3_dag_test', default_args=default_args, schedule_interval= '@once')\n\nt1 = BashOperator(\n    task_id='bash_test',\n    bash_command='echo &quot;hello, it should work&quot; &gt; s3_conn_test.txt',\n    dag=dag)\n\nsensor = S3KeySensor(\n    task_id='check_s3_for_file_in_s3',\n    bucket_key='file-to-watch-*',\n    wildcard_match=True,\n    bucket_name='S3-Bucket-To-Watch',\n    s3_conn_id='my_conn_S3',\n    timeout=18*60*60,\n    poke_interval=120,\n    dag=dag)\n\nt1.set_upstream(sensor)\n</code></pre>\n<hr />\nMain References:\n<ul>\n<li><a href=\"https://gitter.im/apache/incubator-airflow\" rel=\"noreferrer\">https://gitter.im/apache/incubator-airflow</a></li>\n<li><a href=\"https://groups.google.com/forum/#!topic/airbnb_airflow/TXsJNOBBfig\" rel=\"noreferrer\">https://groups.google.com/forum/#!topic/airbnb_airflow/TXsJNOBBfig</a></li>\n<li><a href=\"https://github.com/apache/incubator-airflow\" rel=\"noreferrer\">https://github.com/apache/incubator-airflow</a></li>\n</ul>\n"}
{"title":"Airflow versus AWS Step Functions for workflow","question":"<p>I am working on a project that grabs a set of input data from AWS S3, pre-processes and divvies it up, spins up 10K batch containers to process the divvied data in parallel on AWS Batch, post-aggregates the data, and pushes it to S3.</p>\n<p>I already have software patterns from other projects for Airflow + Batch, but have not dealt with the scaling factors of 10k parallel tasks. Airflow is nice since I can look at which tasks failed and retry a task after debugging. But dealing with that many tasks on one Airflow EC2 instance seems like a barrier. Other option would be to have one task that kicks off the 10k containers and monitors it from there.</p>\n<p>I have no experience with Step Functions, but have heard it's AWS's Airflow. There looks to be plenty of patterns online for Step Functions + Batch. Does Step Functions seem like a good path to check out for my use case? Do you get the same insights on failing jobs / ability to retry tasks as you do with Airflow?</p>\n","answer":"<p>I have worked on both Apache Airflow and AWS Step Functions and here are some insights:</p>\n<ul>\n<li>Step Functions provide out of the box maintenance. It has high availability and scalability that is required for your use-case, for Airflow we'll have to do to it with auto-scaling/load balancing on servers or containers (kubernetes).*</li>\n<li>Both Airflow and Step Functions have user friendly UI's. While Airflow supports multiple representations of the state machine, Step Functions only display state machine as DAG's.</li>\n<li>As of version 2.0, Airflow's Rest API is now <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html\" rel=\"noreferrer\">stable</a>. AWS Step Functions are also supported by a range of production graded cli and <a href=\"https://aws.amazon.com/tools/\" rel=\"noreferrer\">SDK's</a>.</li>\n<li>Airflow has server costs while Step Functions have 4000/month free step executions (free tier) and $0.000025/step after that. e.g. if you use 10K steps for AWS Batch that run once daily, you will be priced $0.25 per day ($7.5 per month). The price for Airflow server (t2.large ec2 1 year reserved instance) is $41.98 per month. We will have to use AWS Batch for either case.**</li>\n<li>AWS Batch can integrate to both <a href=\"https://airflow.apache.org/docs/stable/_api/airflow/contrib/operators/awsbatch_operator/index.html\" rel=\"noreferrer\">Airflow</a> and <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/connect-batch.html\" rel=\"noreferrer\">Step Functions</a>.</li>\n<li>You can clear and rerun a failed task in Apache Airflow, but in Step Functions you will have to create a <a href=\"https://aws.amazon.com/blogs/compute/resume-aws-step-functions-from-any-state/\" rel=\"noreferrer\">custom implementation</a> to handle that. You may handle <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html\" rel=\"noreferrer\">automated retries with back-offs</a> in Step Functions definition as well.</li>\n<li>For failed task in Step Functions you will get a visual representation of failed state and the detailed message when you click it. You may also use aws cli or sdk to get the details.</li>\n<li>Step Functions use easy to use JSON as state machine definition, while Airflow uses Python script.</li>\n<li>Step Functions support <a href=\"https://aws.amazon.com/about-aws/whats-new/2019/05/aws-step-functions-support-callback-patterns/\" rel=\"noreferrer\">async callbacks</a>, i.e. state machine pauses until an external source notifies it to resume. While Airflow has <a href=\"https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-28%3A+Add+AsyncExecutor+option\" rel=\"noreferrer\">yet to add</a> this feature.</li>\n</ul>\n<p>Overall, I see more advantages of using AWS Step Functions. You will have to consider maintenance cost and development cost for both services as per your use case.</p>\n<p><strong>UPDATES</strong> (AWS Managed Workflows for Apache Airflow Service):</p>\n<ul>\n<li>*With AWS Managed Workflows for Apache Airflow service, you can offload deployment, maintenance, autoscaling/load balancing and security of your Airflow Service to AWS. But please consider the version number you're willing to settle for, as AWS managed services are mostly behind the latest version. (e.g. As of March 08, 2021, the latest version of open source airflow is 2.01, while MWAA allows version 1.10.12)</li>\n<li>**MWAA costs on environment, instance and storage. <a href=\"https://aws.amazon.com/managed-workflows-for-apache-airflow/pricing/\" rel=\"noreferrer\">More details here</a>.</li>\n</ul>\n"}
{"title":"Error while install airflow: By default one of Airflow&#39;s dependencies installs a GPL","question":"<p>Getting the following error after running <code>pip install airflow[postgres]</code> command:</p>\n<pre><code>&gt; raise RuntimeError(&quot;By default one of Airflow's dependencies installs\n&gt; a GPL &quot;\n&gt; \n&gt; RuntimeError: By default one of Airflow's dependencies  installs a GPL\n&gt; dependency (unidecode). To avoid this dependency set\n&gt; SLUGIFY_USES_TEXT_UNIDECODE=yes in your environment when you install\n&gt; or upgrade Airflow. To force installing the GPL version set\n&gt; AIRFLOW_GPL_UNIDECODE\n</code></pre>\n<p>I am trying to install in Debian 9</p>\n","answer":"<p>Try the following:</p>\n\n<pre><code>export AIRFLOW_GPL_UNIDECODE=yes\n</code></pre>\n\n<p>OR</p>\n\n<pre><code>export SLUGIFY_USES_TEXT_UNIDECODE=yes\n</code></pre>\n\n<p>Using <code>export</code> makes the environment variable available to all the subprocesses.</p>\n\n<p>Also, make sure you are using <code>pip install apache-airflow[postgres]</code> and not <code>pip install airflow[postgres]</code></p>\n\n<p>Which should you use: if using AIRFLOW_GPL_UNIDECODE, airflow will install a dependency that is under GPL license, which means you won't be able to distribute your resulting application commercially. If that's a problem for you, go for SLUGIFY_USES_TEXT_UNIDECODE.</p>\n"}
{"title":"How to restart a failed task on Airflow","question":"<p>I am using a <strong>LocalExecutor</strong> and my dag has <strong>3 tasks</strong> where task(C) is dependant on task(A). Task(B) and task(A) can run in parallel something like below</p>\n\n<p>A-->C</p>\n\n<p>B</p>\n\n<p><strong>So task(A) has failed</strong> and but <strong>task(B) ran fine</strong>. Task(C) is yet to run as task(A) has failed.</p>\n\n<p>My question is <strong>how do i re run Task(A) alone so Task(C) runs</strong> once Task(A) completes and Airflow UI marks them as success.</p>\n","answer":"<p>In the UI:</p>\n\n<ol>\n<li>Go to the dag, and dag run of the run you want to change</li>\n<li>Click on GraphView </li>\n<li>Click on task A</li>\n<li>Click \"Clear\"</li>\n</ol>\n\n<p>This will let task A run again, and if it succeeds, task C should run.\nThis works because when you clear a task's status, the scheduler will treat it as if it hadn't run before for this dag run.</p>\n"}
{"title":"Airflow parallelism","question":"<p>the Local Executor spawns new processes while scheduling tasks. Is there a limit to the number of processes it creates. I needed to change it. I need to know what is the difference between scheduler's \"max_threads\" and \n\"parallelism\" in airflow.cfg ?</p>\n","answer":"<p><strong>parallelism:</strong> not a very descriptive name. The description says it sets the maximum task instances for the airflow installation, which is a bit ambiguous â€” if I have two hosts running airflow workers, I'd have airflow installed on two hosts, so that should be two installations, but based on context 'per installation' here means 'per Airflow state database'. I'd name this max_active_tasks.</p>\n\n<p><strong>dag_concurrency:</strong> Despite the name based on the comment this is actually the task concurrency, and it's per worker. I'd name this max_active_tasks_for_worker (per_worker would suggest that it's a global setting for workers, but I think you can have workers with different values set for this).</p>\n\n<p><strong>max_active_runs_per_dag</strong>: This one's kinda alright, but since it seems to be just a default value for the matching DAG kwarg, it might be nice to reflect that in the name, something like default_max_active_runs_for_dags\nSo let's move on to the DAG kwargs:</p>\n\n<p><strong>concurrency</strong>: Again, having a general name like this, coupled with the fact that concurrency is used for something different elsewhere makes this pretty confusing. I'd call this max_active_tasks.</p>\n\n<p><strong>max_active_runs</strong>: This one sounds alright to me.</p>\n\n<p>source: <a href=\"https://issues.apache.org/jira/browse/AIRFLOW-57\" rel=\"noreferrer\">https://issues.apache.org/jira/browse/AIRFLOW-57</a></p>\n\n<hr>\n\n<p><strong>max_threads</strong> gives the user some control over cpu usage. It specifies scheduler parallelism.</p>\n"}
{"title":"For Apache Airflow, How can I pass the parameters when manually trigger DAG via CLI?","question":"<p>I use Airflow to manage ETL tasks execution and schedule. A DAG has been created and it works fine. But is it possible to pass parameters when manually trigger the dag via cli.</p>\n\n<p>For example:\nMy DAG runs every day at 01:30, and processes data for yesterday(time range from 01:30 yesterday to 01:30 today). There might be some issues with the data source. I need to re-process those data (manually specify the time range).</p>\n\n<p>So can I create such an airflow DAG, when it's scheduled, that the default time range is from 01:30 yesterday to 01:30 today. Then if anything wrong with the data source, I need to manually trigger the DAG and manually pass the time range as parameters.</p>\n\n<p>As I know <code>airflow test</code> has <code>-tp</code> that can pass params to the task. But this is only for testing a specific task. and <code>airflow trigger_dag</code> doesn't have <code>-tp</code> option. So is there any way to tigger_dag and pass parameters to the DAG, and then the Operator can read these parameters?</p>\n\n<p>Thanks!</p>\n","answer":"<p>You can pass parameters from the CLI using <code>--conf '{&quot;key&quot;:&quot;value&quot;}'</code> and then use it in the DAG file as <code>&quot;{{ dag_run.conf[&quot;key&quot;] }}&quot;</code> in templated field.</p>\n<p><strong>CLI</strong>:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>airflow trigger_dag 'example_dag_conf' -r 'run_id' --conf '{&quot;message&quot;:&quot;value&quot;}'\n</code></pre>\n<p><strong>DAG File</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>args = {\n    'start_date': datetime.utcnow(),\n    'owner': 'airflow',\n}\n\ndag = DAG(\n    dag_id='example_dag_conf',\n    default_args=args,\n    schedule_interval=None,\n)\n\ndef run_this_func(ds, **kwargs):\n    print(&quot;Remotely received value of {} for key=message&quot;.\n          format(kwargs['dag_run'].conf['message']))\n\n\nrun_this = PythonOperator(\n    task_id='run_this',\n    provide_context=True,\n    python_callable=run_this_func,\n    dag=dag,\n)\n\n# You can also access the DagRun object in templates\nbash_task = BashOperator(\n    task_id=&quot;bash_task&quot;,\n    bash_command='echo &quot;Here is the message: '\n                 '{{ dag_run.conf[&quot;message&quot;] if dag_run else &quot;&quot; }}&quot; ',\n    dag=dag,\n)\n</code></pre>\n<hr />\n<p>PSA for new airflow users: it's worth making the switch to <code>--conf</code>. Params are a nice pattern, but they are an architectural dead end because they don't appear widely supported for programmatic use. Meanwhile, --conf is exposed in services like google Cloud Composer's trigger dag run.</p>\n"}
{"title":"Efficient way to deploy dag files on airflow","question":"<p>Are there any best practices that are followed for deploying new dags to airflow? </p>\n\n<p>I saw a couple of comments on the google forum stating that the dags are saved inside a GIT repository and the same is synced periodically to the local location in the airflow cluster.<br>    Regarding this approach, I had a  couple of questions\n<li>\n  Do we maintain separate dag files for separate environments? (testing. production)\n  <li>How to handle rollback of an ETL to an older version in case the new version has a bug?</p>\n\n<p>Any help here is highly appreciated. Let me know in case you need any further details?</p>\n","answer":"<p>Here is how we manage it for our team.</p>\n\n<p>First in terms of naming convention, each of our <strong>DAG file name</strong> matches the <strong>DAG Id</strong> from the content of the DAG itself (including the DAG version). This is useful because ultimately it's the DAG Id that you see in the Airflow UI so you will know exactly which file has been used behind each DAG.</p>\n\n<p>Example for a DAG like this:</p>\n\n<pre><code>from airflow import DAG\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n  'owner': 'airflow',\n  'depends_on_past': False,\n  'start_date': datetime(2017,12,05,23,59),\n  'email': ['me@mail.com'],\n  'email_on_failure': True\n}\n\ndag = DAG(\n  'my_nice_dag-v1.0.9', #update version whenever you change something\n  default_args=default_args,\n  schedule_interval=\"0,15,30,45 * * * *\",\n  dagrun_timeout=timedelta(hours=24),\n  max_active_runs=1)\n  [...]\n</code></pre>\n\n<p>The name of the <strong>DAG file</strong> would be: <strong>my_nice_dag-v1.0.9.py</strong></p>\n\n<ul>\n<li>All our DAG files are stored in a Git repository (among other things)</li>\n<li>Everytime a merge request is done in our master branch, our Continuous Integration pipeline starts a new build and packages our DAG files into a zip (we use Atlassian Bamboo but there's other solutions like Jenkins, Circle CI, Travis...)</li>\n<li>In Bamboo we configured a deployment script (shell) which unzips the package and places the DAG files on the Airflow server in the <strong>/dags</strong> folder.</li>\n<li>We usually deploy the DAGs in DEV for testing, then to UAT and finally PROD. The deployment is done with the click of a button in Bamboo UI thanks to the shell script mentioned above.</li>\n</ul>\n\n<p><strong>Benefits</strong></p>\n\n<ol>\n<li>Because you have included the DAG version in your file name, the previous version of your DAG file is not overwritten in the DAG folder so you can easily come back to it</li>\n<li>When your new DAG file is loaded in Airflow you can recognize it in the UI thanks to the version number.</li>\n<li>Because your DAG file name = DAG Id you could even improve the deployment script by adding some Airflow command line to automatically switch ON your new DAGs once they are deployed.</li>\n<li>Because every version of the DAGs is historicized in Git, we can always comeback to previous versions if needed.</li>\n</ol>\n"}
{"title":"Airflow backfill clarification","question":"<p>I'm just getting started with Airbnb's <a href=\"http://nerds.airbnb.com/airflow/\" rel=\"noreferrer\">airflow</a>, and I'm still not clear on how/when backfilling is done.  </p>\n\n<p>Specifically, there are 2 use-cases that confuse me:</p>\n\n<ol>\n<li><p>If I run <code>airflow scheduler</code> for a few minutes, stop it for a minute, then restart it again, my DAG seems to run extra tasks for the first 30 seconds or so, then it continues as normal (runs every 10 sec).  Are these extra tasks \"backfilled\" tasks that weren't able to complete in an earlier run?  If so, how would I tell airflow not to backfill those tasks? </p></li>\n<li><p>If I run <code>airflow scheduler</code> for a few minutes, then run <code>airflow clear MY_tutorial</code>, then restart <code>airflow scheduler</code>, it seems to run a TON of extra tasks.  Are these tasks also somehow \"backfilled\" tasks?  Or am I missing something.</p></li>\n</ol>\n\n<p>Currently, I have a very simple dag: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>default_args = {\n    'owner': 'me',\n    'depends_on_past': False,\n    'start_date': datetime(2016, 10, 4),\n    'email': ['airflow@airflow.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n    # 'queue': 'bash_queue',\n    # 'pool': 'backfill',\n    # 'priority_weight': 10,\n    # 'end_date': datetime(2016, 1, 1),\n}\n\ndag = DAG(\n    'MY_tutorial', default_args=default_args, schedule_interval=timedelta(seconds=10))\n\n# t1, t2 and t3 are examples of tasks created by instantiating operators\nt1 = BashOperator(\n    task_id='print_date',\n    bash_command='date',\n    dag=dag)\n\nt2 = BashOperator(\n    task_id='sleep',\n    bash_command='sleep 5',\n    retries=3,\n    dag=dag)\n\ntemplated_command = \"\"\"\n    {% for i in range(5) %}\n        echo \"{{ ds }}\"\n        echo \"{{ macros.ds_add(ds, 8)}}\"\n        echo \"{{ params.my_param }}\"\n    {% endfor %}\n\"\"\"\n\nt3 = BashOperator(\n    task_id='templated',\n    bash_command=templated_command,\n    params={'my_param': 'Parameter I passed in'},\n    dag=dag)\n\nsecond_template = \"\"\"\n    touch ~/airflow/logs/test\n    echo $(date) &gt;&gt; ~/airflow/logs/test\n\"\"\"\n\nt4 = BashOperator(\n    task_id='write_test',\n    bash_command=second_template,\n    dag=dag)\n\nt1.set_upstream(t4)\nt2.set_upstream(t1)\nt3.set_upstream(t1)\n</code></pre>\n\n<p>The only two things I've changed in my airflow config are</p>\n\n<ol>\n<li>I changed from using a sqlite db to using a postgres db</li>\n<li>I'm using a <code>CeleryExecutor</code> instead of a <code>SequentialExecutor</code></li>\n</ol>\n\n<p>Thanks so much for you help!</p>\n","answer":"<p>When you change the scheduler toggle to &quot;on&quot; for a DAG, the scheduler will trigger a backfill of all dag run instances for which it has no status recorded, starting with the start_date you specify in your &quot;default_args&quot;.</p>\n<p>For example: If the start date was &quot;2017-01-21&quot; and you turned on the scheduling toggle at &quot;2017-01-22T00:00:00&quot; and your dag was configured to run hourly, then the scheduler will backfill 24 dag runs and then start running on the scheduled interval.</p>\n<p>This is essentially what is happening in both of your question. In #1, it is filling in the 3 missing runs from the 30 seconds which you turned off the scheduler. In #2, it is filling in all of the DAG runs from start_date until &quot;now&quot;.</p>\n<p>There are 2 ways around this:</p>\n<ol>\n<li>Set the start_date to a date in the future so that it will only start scheduling dag runs once that date is reached. Note that if you change the start_date of a DAG, you must change the name of the DAG as well due to the way the start date is stored in airflow's DB.</li>\n<li>Manually run backfill <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#backfill\" rel=\"noreferrer\">from the command line</a> with the &quot;-m&quot; (--mark-success) flag which tells airflow not to actually run the DAG, rather just mark it as successful in the DB.</li>\n</ol>\n<p>e.g.</p>\n<pre><code>airflow backfill MY_tutorial -m -s 2016-10-04 -e 2017-01-22T14:28:30\n</code></pre>\n"}
{"title":"TemplateNotFound error when running simple Airflow BashOperator","question":"<p>I'm trying to write our first Airflow DAG, and I'm getting the following error when I try to list the tasks using command <code>airflow list_tasks orderwarehouse</code>:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/airflow/models.py\", line 2038, in resolve_template_files\n    setattr(self, attr, env.loader.get_source(env, content)[0])\n  File \"/usr/local/lib/python2.7/site-packages/jinja2/loaders.py\", line 187, in get_source\n    raise TemplateNotFound(template)\nTemplateNotFound: ./home/deploy/airflow-server/task_scripts/orderwarehouse/load_warehouse_tables.sh\n</code></pre>\n\n<p>This DAG is not supposed to use a template. I'm only trying to run the shell script in the specified location per the instructions in <a href=\"https://airflow.incubator.apache.org/_modules/bash_operator.html#BashOperator\" rel=\"noreferrer\">the docs</a>. The shell script does exist in that location and is spelled correctly. My DAG looks like this:</p>\n\n<pre><code>from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2015, 6, 1),\n    'email': ['airflow@airflow.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n    # 'queue': 'bash_queue',\n    # 'pool': 'backfill',\n    # 'priority_weight': 10,\n    # 'end_date': datetime(2016, 1, 1),\n}\n\norderwarehouse = DAG('orderwarehouse', default_args=default_args)\n\nload_mysql = BashOperator(\n    task_id='load_warehouse_mysql',\n    bash_command='./home/deploy/airflow-server/task_scripts/orderwarehouse/load_warehouse_tables.sh',\n    dag=orderwarehouse)\n</code></pre>\n\n<p>Not sure why it thinks it needs to look for a Jinja template. Running out of ideas on this one, would appreciate if anyone can point me to where I'm going astray. Thanks.</p>\n","answer":"<p>This is a pitfall of airflow. Add a space at the end of your bash_command and it should run fine</p>\n<p>Source:\n<a href=\"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=62694614\" rel=\"noreferrer\">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=62694614</a></p>\n"}
{"title":"How to pass parameter to PythonOperator in Airflow","question":"<p>I just started using <strong>Airflow</strong>, can anyone enlighten me how to pass a parameter into <strong>PythonOperator</strong> like below:</p>\n\n<pre><code>t5_send_notification = PythonOperator(\n    task_id='t5_send_notification',\n    provide_context=True,\n    python_callable=SendEmail,\n    op_kwargs=None,\n    #op_kwargs=(key1='value1', key2='value2'),\n    dag=dag,\n)\n\ndef SendEmail(**kwargs):\n    msg = MIMEText(\"The pipeline for client1 is completed, please check.\")\n    msg['Subject'] = \"xxxx\"\n    msg['From'] = \"xxxx\"\n    ......\n    s = smtplib.SMTP('localhost')\n    s.send_message(msg)\n    s.quit()\n</code></pre>\n\n<p>I would like to be able to pass some parameters into the <code>t5_send_notification</code>'s callable which is <code>SendEmail</code>, ideally I want to attach the full log and/or part of the log (which is essentially from the kwargs) to the email to be sent out, guessing the <code>t5_send_notification</code> is the place to gather those information.</p>\n\n<p>Thank you very much.</p>\n","answer":"<ol>\n<li>Pass a dict object to <em>op_kwargs</em></li>\n<li><p>Use the keys to access their value from <em>kwargs</em> dict in your python callable</p>\n\n<pre><code>def SendEmail(**kwargs):\n    print(kwargs['key1'])\n    print(kwargs['key2'])\n    msg = MIMEText(\"The pipeline for client1 is completed, please check.\")\n    msg['Subject'] = \"xxxx\"\n    msg['From'] = \"xxxx\"\n    ......\n    s = smtplib.SMTP('localhost')\n    s.send_message(msg)\n    s.quit()\n\n\nt5_send_notification = PythonOperator(\n    task_id='t5_send_notification',\n    provide_context=True,\n    python_callable=SendEmail,\n    op_kwargs={'key1': 'value1', 'key2': 'value2'},\n    dag=dag,\n)\n</code></pre></li>\n</ol>\n"}
{"title":"How to set up Airflow Send Email?","question":"<p>I followed online tutorial to set up Email SMTP server in airflow.cfg as below:</p>\n\n<pre><code>[email]\nemail_backend = airflow.utils.email.send_email_smtp\n\n\n[smtp]\n# If you want airflow to send emails on retries, failure, and you want to use\n# the airflow.utils.email.send_email_smtp function, you have to configure an\n# smtp server here\nsmtp_host = smtp.gmail.com\nsmtp_starttls = True\nsmtp_ssl = False\n# Uncomment and set the user/pass settings if you want to use SMTP AUTH \n# smtp_user =                       \n# smtp_password =  \nsmtp_port = 587\nsmtp_mail_from = myemail@gmail.com\n</code></pre>\n\n<p>And my DAG is as below:</p>\n\n<pre><code>from datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.email_operator import EmailOperator\n\ndef print_hello():\n    return 'Hello world!'\n\ndefault_args = {\n        'owner': 'peter',\n        'start_date':datetime(2018,8,11),\n}\n\ndag = DAG('hello_world', description='Simple tutorial DAG',\n          schedule_interval='* * * * *',\n          default_args = default_args, catchup=False)\n\ndummy_operator = DummyOperator(task_id='dummy_task', retries=3, dag=dag)\n\nhello_operator = PythonOperator(task_id='hello_task', python_callable=print_hello, dag=dag)\n\nemail = EmailOperator(\n        task_id='send_email',\n        to='to@gmail.com',\n        subject='Airflow Alert',\n        html_content=\"\"\" &lt;h3&gt;Email Test&lt;/h3&gt; \"\"\",\n        dag=dag\n)\n\nemail &gt;&gt; dummy_operator &gt;&gt; hello_operator\n</code></pre>\n\n<p>I assumed the email operator will run after the other two operators and then send me an email.\nBut email was not sent to me.\nI really appreciate your help. Thank you very much.</p>\n\n<p>Best</p>\n","answer":"<p><strong>Setting up SMTP Server for Airflow Email alerts using Gmail</strong>:</p>\n<p>Create an email id from which you want to send alerts about DAG failure or if you want to use <strong>EmailOperator</strong>. Edit <code>airflow.cfg</code> file to edit the smtp details for the mail server.</p>\n<p>For demo you can use any gmail account.</p>\n<p>Create a google App Password for your gmail account (<a href=\"https://support.google.com/accounts/answer/185833\" rel=\"nofollow noreferrer\">Instruction here</a>). This is done so that you don't use your original password or 2 Factor authentication.</p>\n<ol>\n<li>Visit your <a href=\"https://security.google.com/settings/security/apppasswords\" rel=\"nofollow noreferrer\">App passwords</a> page. You may be asked to sign in to your Google Account.</li>\n<li>Under &quot;Your app passwords&quot;, click <strong>Select app</strong> and choose &quot;Mail&quot;.</li>\n<li>Click <strong>Select device</strong> and choose &quot;Other (Custom name)&quot; so that you can input &quot;Airflow&quot;.</li>\n<li>Select <strong>Generate</strong>.</li>\n<li>Copy the generated App password (the 16 character code in the yellow bar), for example xxxxyyyyxxxxyyyy.</li>\n<li>Select <strong>Done</strong>.</li>\n</ol>\n<p>Once you are finished, you wonâ€™t see that App password code again. However, you will see a list of apps and devices (which you've created App passwords for) in your Google Account.</p>\n<p>Edit <code>airflow.cfg</code> and edit the <code>[smtp]</code> section as shown below:</p>\n<pre><code>[smtp]\nsmtp_host = smtp.gmail.com\nsmtp_starttls = True\nsmtp_ssl = False\nsmtp_user = YOUR_EMAIL_ADDRESS\nsmtp_password = xxxxyyyyxxxxyyyy\nsmtp_port = 587\nsmtp_mail_from = YOUR_EMAIL_ADDRESS\n</code></pre>\n<p>Edit the below parameters to the corresponding values:</p>\n<p><code>YOUR_EMAIL_ADDRESS</code> = Your Gmail address<br />\n<code>xxxxyyyyxxxxyyyy</code> = The App password generated above</p>\n"}
{"title":"I am getting &quot;bash: airflow: command not found&quot;","question":"<p>I am getting</p>\n<blockquote>\n<p>-bash: airflow: command not found</p>\n</blockquote>\n<p>after installing <a href=\"https://en.wikipedia.org/wiki/Apache_Airflow\" rel=\"noreferrer\">Apache Airflow</a>. I am using Google Cloud Compute Engine and OS is DebianÂ 9 (Stretch).</p>\n<p>I have followed the below steps:</p>\n<pre><code>export AIRFLOW_HOME=~/airflow\n\npip install apache-airflow\n</code></pre>\n","answer":"<p>I have uninstalled Apache Airflow and installed it with the <code>sudo</code> command (i.e., <code>sudo pip install apache-airflow</code>). After that it worked fine.</p>\n"}
